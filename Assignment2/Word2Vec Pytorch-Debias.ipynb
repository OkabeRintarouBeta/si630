{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SI 630: Homework 2: Word2Vec\n",
    "\n",
    "This homework will have you implementing word2vec using PyTorch and let you familiarize yourself with building more complex neural networks and the larger PyTorch development infrastructure.\n",
    "\n",
    "Broadly, this homework consists of a few major parts:\n",
    "1. Implement a `Corpus` class that will load the dataset and convert it to a sequence of token ids\n",
    "2. Implement negative sampling to select tokens to be used as negative examples of words in the context\n",
    "3. Create your dataset of positive and negative examples per context and load it into PyTorch's `DataLoader` to use for sampling\n",
    "4. Implement a `Word2Vec` class that is a PyTorch neural network\n",
    "5. Implement a training loop that samples a _batch_ of target words and their respective positive/negative context words\n",
    "6. Implement rare word removal and frequent word subsampling\n",
    "7. Run your model on the full dataset for at least one epoch\n",
    "8. Do the exploratory parts of the homework\n",
    "9. Make a copy of this notebook and change your implementation so it learns word vectors with less bias\n",
    "\n",
    "After Step 5, you should be able to run your word2vec implementation on a small dataset and verify that it's learning correctly. Once you can verify everything is working, proceed with steps 6 and beyond. **Please note that this list is a general sketch and the homework PDF has the full list/description of to-dos and all your deliverables.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ff898f941f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "from tqdm.auto import tqdm, trange\n",
    "from collections import Counter\n",
    "import random\n",
    "from torch import optim\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Helpful for computing cosine similarity--Note that this is NOT a similarity!\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Handy command-line argument parsing\n",
    "import argparse\n",
    "\n",
    "# Sort of smart tokenization\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import MWETokenizer\n",
    "\n",
    "# We'll use this to save our models\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "#\n",
    "# IMPORTANT NOTE: Always set your random seeds when dealing with stochastic\n",
    "# algorithms as it lets your bugs be reproducible and (more importantly) it lets\n",
    "# your results be reproducible by others.\n",
    "#\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a class to hold the data\n",
    "\n",
    "Before we get to training word2vec, we'll need to process the corpus into some representation. The `Corpus` class will handle much of the functionality for corpus reading and keeping track of which word types belong to which ids. The `Corpus` class will also handle the crucial functionality of generating negative samples for training (i.e., randomly-sampled words that were not in the target word's context).\n",
    "\n",
    "Some parts of this class can be completed after you've gotten word2vec up and running, so see the notes below and the details in the homework PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        self.tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        \n",
    "        # These state variables become populated with function calls\n",
    "        #\n",
    "        # 1. load_data()\n",
    "        # 2. generate_negative_sampling_table()\n",
    "        #\n",
    "        # See those functions for how the various values get filled in\n",
    "\n",
    "        self.word_to_index = {} # word to unique-id\n",
    "        self.index_to_word = {} # unique-id to word\n",
    "\n",
    "        # How many times each word occurs in our data after filtering\n",
    "        self.word_counts = Counter()\n",
    "\n",
    "        # A utility data structure that lets us quickly sample \"negative\"\n",
    "        # instances in a context. This table contains unique-ids\n",
    "        self.negative_sampling_table = []\n",
    "        \n",
    "        # The dataset we'll use for training, as a sequence of unqiue word\n",
    "        # ids. This is the sequence across all documents after tokens have been\n",
    "        # randomly subsampled by the word2vec preprocessing step\n",
    "        self.full_token_sequence_as_ids = None\n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        '''\n",
    "        Tokenize the document and returns a list of the tokens\n",
    "        '''\n",
    "        \n",
    "        return self.tokenizer.tokenize(text)        \n",
    "\n",
    "    def load_data(self, file_name, min_token_freq):\n",
    "        '''\n",
    "        Reads the data from the specified file as long long sequence of text\n",
    "        (ignoring line breaks) and populates the data structures of this\n",
    "        word2vec object.\n",
    "        '''\n",
    "\n",
    "        # Step 1: Read in the file and create a long sequence of tokens for\n",
    "        # all tokens in the file\n",
    "        tokenizer2 = MWETokenizer()\n",
    "        f=open('bio-mwes.txt','r')\n",
    "        lines=f.readlines()\n",
    "        for line in lines:\n",
    "            arr=line.split(' ')\n",
    "            arr[-1]=arr[-1][:-1]\n",
    "#             print(arr)\n",
    "            tokenizer2.add_mwe(tuple(arr))\n",
    "        f.close()\n",
    "        \n",
    "        all_tokens = []\n",
    "        print('Reading data and tokenizing')\n",
    "        \n",
    "        tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+')\n",
    "        fp = open(file_name, 'r')\n",
    "        lines=fp.readlines()\n",
    "        for line in lines:\n",
    "            tokens=tokenizer.tokenize(line)\n",
    "            tokens=tokenizer2.tokenize(tokens)\n",
    "#             print(tokens)\n",
    "            tokens=[i.lower() for i in tokens]\n",
    "            all_tokens+=tokens\n",
    "#         print(all_tokens)\n",
    "    \n",
    "        # Step 2: Count how many tokens we have of each type\n",
    "        print('Counting token frequencies')\n",
    "        \n",
    "        all_tokens_dict=dict(Counter(all_tokens))\n",
    "        \n",
    "\n",
    "        # Step 3: Replace all tokens below the specified frequency with an <UNK>\n",
    "        # token. \n",
    "        #\n",
    "        # NOTE: You can do this step later if needed\n",
    "        print(\"Performing minimum thresholding\")\n",
    "        \n",
    "        delete_keys1=set()\n",
    "        all_tokens_dict['<UNK>']=0\n",
    "        for k,v in all_tokens_dict.items():\n",
    "            if v<min_token_freq and k!='<UNK>':\n",
    "                all_tokens_dict['<UNK>']+=1\n",
    "                delete_keys1.add(k)\n",
    "        \n",
    "        for k in delete_keys1:\n",
    "            del all_tokens_dict[k]\n",
    "                \n",
    "        \n",
    "        \n",
    "\n",
    "        # Step 4: update self.word_counts to be the number of times each word\n",
    "        # occurs (including <UNK>)\n",
    "        self.word_counts=all_tokens_dict\n",
    "        \n",
    "        \n",
    "        # Step 5: Create the mappings from word to unique integer ID and the\n",
    "        # reverse mapping.\n",
    "        idx=0\n",
    "        for k in all_tokens_dict.keys():\n",
    "            self.word_to_index[k]=idx\n",
    "            self.index_to_word[idx]=k\n",
    "            idx+=1\n",
    "        \n",
    "        \n",
    "        # Step 6: Compute the probability of keeping any particular *token* of a\n",
    "        # word in the training sequence, which we'll use to subsample. This subsampling\n",
    "        # avoids having the training data be filled with many overly common words\n",
    "        # as positive examples in the context\n",
    "        length=len(all_tokens)\n",
    "        token_set=set()\n",
    "        delete_keys=set()\n",
    "        for k,v in all_tokens_dict.items():\n",
    "            p=v/length\n",
    "            keep=(np.sqrt(p/0.001)+1)*0.001/(np.sqrt(p))\n",
    "#             if k=='the' or k=='there' or k=='monday':\n",
    "#                 print(k,keep)\n",
    "#             if keep>=0.0358:\n",
    "            if keep>=random.random()/50:\n",
    "                token_set.add(k)\n",
    "                \n",
    "            \n",
    "#         for k in delete_keys:\n",
    "#             print(k)\n",
    "#             if k in all_tokens_dict.keys():\n",
    "#                 del all_tokens_dict[k]\n",
    "#                 del self.index_to_word[self.word_to_index[k]]\n",
    "#                 del self.word_to_index[k]\n",
    "            \n",
    "            \n",
    "        print(\"============\")\n",
    "                        \n",
    "        # Step 7: process the list of tokens (after min-freq filtering) to fill\n",
    "        # a new list self.full_token_sequence_as_ids where \n",
    "        #\n",
    "        # (1) we probabilistically choose whether to keep each *token* based on the\n",
    "        # subsampling probabilities (note that this does not mean we drop\n",
    "        # an entire word!) and \n",
    "        #\n",
    "        # (2) all tokens are convered to their unique ids for faster training.\n",
    "        #\n",
    "        # NOTE: You can skip the subsampling part and just do step 2 to get\n",
    "        # your model up and running.\n",
    "            \n",
    "        # NOTE 2: You will perform token-based subsampling based on the probabilities in\n",
    "        # word_to_sample_prob. When subsampling, you are modifying the sequence itself \n",
    "        # (like deleting an item in a list). This action effectively makes the context\n",
    "        # window  larger for some target words by removing context words that are common\n",
    "        # from a particular context before the training occurs (which then would now include\n",
    "        # other words that were previously just outside the window).\n",
    "\n",
    "        \n",
    "        unk_id=self.word_to_index['<UNK>']\n",
    "        self.full_token_sequence_as_ids=[]\n",
    "        for token in all_tokens:\n",
    "            if token in token_set:\n",
    "                self.full_token_sequence_as_ids.append(self.word_to_index[token])\n",
    "            else:\n",
    "                self.full_token_sequence_as_ids.append(unk_id)\n",
    "            \n",
    "        \n",
    "\n",
    "        # Helpful print statement to verify what you've loaded\n",
    "        print('Loaded all data from %s; saw %d tokens (%d unique)' \\\n",
    "              % (file_name, len(self.full_token_sequence_as_ids),\n",
    "                 len(self.word_to_index)))\n",
    "\n",
    "        \n",
    "    def generate_negative_sampling_table(self, exp_power=0.75, table_size=1e6):\n",
    "        '''\n",
    "        Generates a big list data structure that we can quickly randomly index into\n",
    "        in order to select a negative training example (i.e., a word that was\n",
    "        *not* present in the context). \n",
    "        '''       \n",
    "        \n",
    "        # Step 1: Figure out how many instances of each word need to go into the\n",
    "        # negative sampling table. \n",
    "        #\n",
    "        # HINT: np.power and np.fill might be useful here        \n",
    "        print(\"Generating sampling table\")\n",
    "\n",
    "        keep_token_dict={}\n",
    "        total_p=0\n",
    "        for k in self.index_to_word.keys():\n",
    "            keep_token_dict[k]=np.power(self.word_counts[self.index_to_word[k]],exp_power)\n",
    "            total_p+=keep_token_dict[k]\n",
    "            \n",
    "        \n",
    "        for k in self.index_to_word.keys():\n",
    "            keep_token_dict[k]=keep_token_dict[k]/total_p\n",
    "        \n",
    "        \n",
    "        ct=0\n",
    "        for idx, val in keep_token_dict.items():\n",
    "            i=0\n",
    "            max_val = round(val*table_size)\n",
    "\n",
    "            while ct < table_size and i <= max_val:\n",
    "#                 print(idx)\n",
    "                self.negative_sampling_table.append(idx)\n",
    "                ct+=1\n",
    "                i+=1\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Step 2: Create the table to the correct size. You'll want this to be a\n",
    "        # numpy array of type int\n",
    "\n",
    "\n",
    "        # Step 3: Fill the table so that each word has a number of IDs\n",
    "        # proportionate to its probability of being sampled.\n",
    "        #\n",
    "        # Example: if we have 3 words \"a\" \"b\" and \"c\" with probabilites 0.5,\n",
    "        # 0.33, 0.16 and a table size of 6 then our table would look like this\n",
    "        # (before converting the words to IDs):\n",
    "        #\n",
    "        # [ \"a\", \"a\", \"a\", \"b\", \"b\", \"c\" ]\n",
    "        #\n",
    "        \n",
    "\n",
    "\n",
    "    def generate_negative_samples(self, cur_context_word_id, num_samples):\n",
    "        '''\n",
    "        Randomly samples the specified number of negative samples from the lookup\n",
    "        table and returns this list of IDs as a numpy array. As a performance\n",
    "        improvement, avoid sampling a negative example that has the same ID as\n",
    "        the current positive context word.\n",
    "        '''\n",
    "\n",
    "        results = []\n",
    "\n",
    "        # Create a list and sample from the negative_sampling_table to\n",
    "        # grow the list to num_samples, avoiding adding a negative example that\n",
    "        # has the same ID as the current context_word\n",
    "        \n",
    "\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            while len(results)!=num_samples:\n",
    "                idx=random.randint(0,len(self.negative_sampling_table)-1)\n",
    "                if self.negative_sampling_table[idx] not in cur_context_word_id:\n",
    "                    results.append(self.negative_sampling_table[idx])\n",
    "        \n",
    "#         print(self.index_to_word[cur_context_word_id])\n",
    "#         for i in results:\n",
    "#             print(self.index_to_word[i])\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the corpus\n",
    "\n",
    "Now that we have code to turn the text into training data, let's do so. We've provided several files for you to help:\n",
    "\n",
    "* `wiki-bios.DEBUG.txt` -- use this to debug your corpus reader\n",
    "* `wiki-bios.10k.txt` -- use this to debug/verify the whole word2vec works\n",
    "* `wiki-bios.med.txt` -- use this when everything works to generate your vectors for later parts\n",
    "* `wiki-bios.HUGE.txt.gz` -- _do not use this_ unless (1) everything works and (2) you really want to test/explore. This file is not needed at all to do your homework.\n",
    "\n",
    "We recommend startin to debug with the first file, as it is small and fast to load (quicker to find bugs). When debugging, we recommend setting the `min_token_freq` argument to 2 so that you can verify that part of the code is working but you still have enough word types left to test the rest.\n",
    "\n",
    "You'll use the remaining files later, where they're described.\n",
    "\n",
    "In the next cell, create your `Corpus`, read in the data, and generate the negative sampling table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data and tokenizing\n",
      "Counting token frequencies\n",
      "Performing minimum thresholding\n",
      "============\n",
      "Loaded all data from wiki-bios.med.txt; saw 22220944 tokens (110331 unique)\n",
      "Generating sampling table\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus()\n",
    "corpus.load_data('wiki-bios.med.txt', 5)\n",
    "corpus.generate_negative_sampling_table()\n",
    "# corpus.generate_negative_samples(325,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the training data\n",
    "\n",
    "Once we have the corpus ready, we need to generate our training dataset. Each instance in the dataset is a target word and positive and negative examples of contexts words. Given the target word as input, we'll want to predict (or not predict) these positive and negative context words as outputs using our network. Your task here is to create a python `list` of instances. \n",
    "\n",
    "Your final training data should be a list of tuples in the format ([target_word_id], [word_id_1, ...], [predicted_labels]), where each item in the list is a list:\n",
    "1. The first item is a list consisting only of the target word's ID.\n",
    "2. The second item is a list of word ids for both context words and negative samples \n",
    "3. The third item is a list of labels to predicted for each of the word ids in the second list (i.e., `1` for context words and `0` for negative samples). \n",
    "\n",
    "You will feed these tuples into the PyTorch `DatasetLoader` later that will do the converstion to `Tensor` objects. You will need to make sure that all of the lists in each tuple are `np.array` instances and are not plain python lists for this `Tensor` converstion to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0796a7da76ae4063a41c155fd9f245b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22220944 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "window_size = 2\n",
    "num_negative_samples_per_target = 2\n",
    "\n",
    "training_data = []\n",
    "    \n",
    "# Loop through each token in the corpus and generate an instance for each, \n",
    "# adding it to training_data\n",
    "ct=0\n",
    "unk_id=corpus.word_to_index['<UNK>']\n",
    "\n",
    "for i in tqdm(range(len(corpus.full_token_sequence_as_ids))):\n",
    "#     print(i)\n",
    "    idx=corpus.full_token_sequence_as_ids[i]\n",
    "    \n",
    "    if idx==unk_id:\n",
    "        continue\n",
    "    elif i==0:\n",
    "        positive_samples_list=corpus.full_token_sequence_as_ids[1:1+window_size]\n",
    "        negative_samples_num=2*window_size*num_negative_samples_per_target+2\n",
    "    elif i==1:\n",
    "        positive_samples_list=corpus.full_token_sequence_as_ids[:1]+corpus.full_token_sequence_as_ids[2:4]\n",
    "        negative_samples_num=2*window_size*num_negative_samples_per_target+1\n",
    "    elif i==len(corpus.full_token_sequence_as_ids)-2:\n",
    "        positive_samples_list=corpus.full_token_sequence_as_ids[i-2:i]+corpus.full_token_sequence_as_ids[i+1:]\n",
    "        negative_samples_num=2*window_size*num_negative_samples_per_target+1\n",
    "    elif i==len(corpus.full_token_sequence_as_ids)-1:\n",
    "        positive_samples_list=corpus.full_token_sequence_as_ids[i-2:i]\n",
    "        negative_samples_num=2*window_size*num_negative_samples_per_target+2\n",
    "    else:\n",
    "        positive_samples_list=[corpus.full_token_sequence_as_ids[i-2],corpus.full_token_sequence_as_ids[i-1],corpus.full_token_sequence_as_ids[i+1],corpus.full_token_sequence_as_ids[i+2]]\n",
    "        negative_samples_num=2*window_size*num_negative_samples_per_target\n",
    "    \n",
    "    \n",
    "    negative_samples_list=corpus.generate_negative_samples(positive_samples_list, negative_samples_num)\n",
    "    predicted_labels=[1]*len(positive_samples_list)+[0]*negative_samples_num\n",
    "    training_data.append((np.array([idx]),np.array(positive_samples_list+negative_samples_list),np.array(predicted_labels)))\n",
    "    # For exach target word in our dataset, select context words \n",
    "    # within +/- the window size in the token sequence\n",
    "    \n",
    "    # For each positive target, we need to select negative examples of\n",
    "    # words that were not in the context. Use the num_negative_samples_per_target\n",
    "    # hyperparameter to generate these, using the generate_negative_samples()\n",
    "    # method from the Corpus class\n",
    "\n",
    "    # NOTE: this part might not make sense until later when you do the training \n",
    "    # so feel free to revisit it to see why it happens.\n",
    "    #\n",
    "    # Our training will use batches of instances together (compare that \n",
    "    # with HW1's SGD that used one item at a time). PyTorch will require\n",
    "    # that all instances in a batches have the same size, which creates an issue\n",
    "    # for us here since the target wordss at the very beginning or end of the corpus\n",
    "    # have shorter contexts. \n",
    "    # \n",
    "    # To work around these edge-cases, we need to ensure that each instance has\n",
    "    # the same size, which means it needs to have the same number of positive\n",
    "    # and negative examples. Since we are short on positive examples here (due\n",
    "    # to the edge of the corpus), we can just add more negative samples.\n",
    "    #\n",
    "    # YOUR TASK: determine what is the maximum number of context words (positive\n",
    "    # and negative) for any instance and then, for instances that have fewer than\n",
    "    # this number of context words, add in negative examples.\n",
    "    #\n",
    "    # NOTE: The maximum is fixed, so you can precompute this outside the loop\n",
    "    # ahead of time.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the network\n",
    "\n",
    "We'll create a new neural network as a subclass of `nn.Module` like we did in Homework 1. However, _unlike_ the network you built in Homework 1, we do not need to used linear layers to implement word2vec. Instead, we will use PyTorch's `Emedding` class, which maps an index (e.g., a word id in this case) to an embedding. \n",
    "\n",
    "Roughly speaking, word2vec's network makes a prediction by computing the dot product of the target word's embedding and a context word's embedding and then passing this dot product through the sigmoid function ($\\sigma$) to predict the probability that the context word was actually in the context. The homework write-up has lots of details on how this works. Your `forward()` function will have to implement this computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        \n",
    "        # Save what state you want and create the embeddings for your\n",
    "        # target and context words\n",
    "        self.target_embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.context_embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        \n",
    "        # Once created, let's fill the embeddings with non-zero random\n",
    "        # numbers. We need to do this to get the training started. \n",
    "        #\n",
    "        # NOTE: Why do this? Think about what happens if all the embeddings\n",
    "        # are all zeros initially. What would the predictions look like for\n",
    "        # word2vec with these embeddings and how would the updated work?\n",
    "        \n",
    "        self.init_emb(init_range=0.5/vocab_size)\n",
    "        \n",
    "    def init_emb(self, init_range):\n",
    "        \n",
    "        # Fill your two embeddings with random numbers uniformly sampled\n",
    "        # between +/- init_range\n",
    "        \n",
    "        self.target_embeddings.weight.data.uniform_(-init_range, init_range)\n",
    "        self.context_embeddings.weight.data.uniform_(-init_range,init_range)\n",
    "        \n",
    "    def forward(self, target_word_id, context_word_ids):\n",
    "        ''' \n",
    "        Predicts whether each context word was actually in the context of the target word.\n",
    "        The input is a tensor with a single target word's id and a tensor containing each\n",
    "        of the context words' ids (this includes both positive and negative examples).\n",
    "        '''\n",
    "        \n",
    "        v_t=self.target_embeddings(target_word_id)\n",
    "        v_c=self.context_embeddings(context_word_ids)\n",
    "\n",
    "        hid=torch.matmul(v_t,torch.transpose(v_c,2,1))\n",
    "        out=torch.sigmoid(hid)\n",
    "        return out\n",
    "        \n",
    "        # NOTE 1: This is probably the hardest part of the homework, so you'll\n",
    "        # need to figure out how to do the dot-product between embeddings and return\n",
    "        # the sigmoid. Be prepared for lots of debugging. For some reference,\n",
    "        # our implementation is three lines and really the hard part is just\n",
    "        # the last line. However, it's usually a matter of figuring out what\n",
    "        # that one line looks like that ends up being the hard part.\n",
    "        \n",
    "        # NOTE 2: In this homework you'll be dealing with *batches* of instances\n",
    "        # rather than a single instance at once. PyTorch mostly handles this\n",
    "        # seamlessly under the hood for you (which is very nice) but batching\n",
    "        # can show in weird ways and create challenges in debugging initially.\n",
    "        # For one, your inputs will get an extra dimension. So, for example,\n",
    "        # if you have a batch size of 4, your input for target_word_id will\n",
    "        # really be 4 x 1. If you get the embeddings of those targets,\n",
    "        # it then becomes 4x50! The same applies to the context_word_ids, except\n",
    "        # that was alreayd a list so now you have things with shape \n",
    "        #\n",
    "        #    (batch x context_words x embedding_size)\n",
    "        #\n",
    "        # One of your tasks will be to figure out how to get things lined up\n",
    "        # so everything \"just works\". When it does, the code looks surprisingly\n",
    "        # simple, but it might take a lot of debugging (or not!) to get there.\n",
    "        \n",
    "        # NOTE 3: We *strongly* discourage you from looking for existing \n",
    "        # implementations of word2vec online. Sadly, having reviewed most of the\n",
    "        # highly-visible ones, they are actually wrong (wow!) or are doing\n",
    "        # inefficient things like computing the full softmax instead of doing\n",
    "        # the negative sampling. Looking at these will likely leave you more\n",
    "        # confused than if you just tried to figure it out yourself.\n",
    "        \n",
    "        # NOTE 4: There many ways to implement this, some more efficient\n",
    "        # than others. You will want to get it working first and then\n",
    "        # test the timing to see how long it takes. As long as the\n",
    "        # code works (vector comparisons look good) you'll receive full\n",
    "        # credit. However, very slow implementations may take hours(!)\n",
    "        # to converge so plan ahead.\n",
    "        \n",
    "        \n",
    "        # Hint 1: You may want to review the mathematical operations on how\n",
    "        # to compute the dot product to see how to do these\n",
    "        \n",
    "        # Hint 2: the \"dim\" argument for some operations may come in handy,\n",
    "        # depending on your implementation\n",
    "        \n",
    "        # Hint 3: printing the shape of the tensors can come in very handy when\n",
    "        # debugging to see where things aren't lining up\n",
    "           \n",
    "        # TODO: Implement the forward pass of word2vec\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network!\n",
    "\n",
    "Now that you have data in the right format and a neural network designed, it's time to train the network and see if it's all working. The trainin code will look surprisingly similar at times to your pytorch code from Homework 1 since all networks share the same base training setup. However, we'll add a few new elements to get you familiar with more common training techniques. \n",
    "\n",
    "For all steps, be sure to use the hyperparameters values described in the write-up.\n",
    "\n",
    "1. Initialize your optimizer and loss function \n",
    "2. Create your network\n",
    "3. Load your dataset into PyTorch's `DataLoader` class, which will take care of batching and shuffling for us (yay!)\n",
    "4. Create a new `SummaryWriter` to periodically write our running-sum of the loss to a tensorboard\n",
    "5. Train your model \n",
    "\n",
    "Two new elements show up. First, we'll be using `DataLoader` which is going to sample data for us and put it in a batch (and also convert the data to `Tensor` objects. You can iterate over the batches and each iteration will return all the items eventually, one batch at a time (a full epoch's worth).\n",
    "\n",
    "The second new part is using `tensorboard`. As you might have noticed in Homework 1, training neural models can take some time. [TensorBoard](https://www.tensorflow.org/tensorboard/) is a handy web-based view that you can check during training to see how the model is doing. We'll use it here and periodically log a running sum of the loss after a set number of steps. The Homework write up has a plot of what this looks like. We'll be doing something simple here with tensorboard but it will come in handy later as you train larger models (for longer) and may want to visually check if your model is converging. TensorBoard was initially written for another deep learning framework, TensorFlow, but proved so useful it was ported to work in PyTorch too and is [easy to integrate](https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html).\n",
    "\n",
    "To start training, we recommend training on the `wiki-bios.10k.txt` dataset. This data is small enough you can get through an epoch in a few minutes (or less) while still being large enough you can test whether the model is learning anything by examining common words. Below this cell we've added a few helper functions that you can use to debug and query your model. In particular, the `get_neighbors()` function is a great way to test: if your model has learned anything, the nearest neighbors for common words should seem reasonable (without having to jump through mental hoops). An easy word to test on the `10k` data is \"january\" which should return month-related words as being most similar.\n",
    "\n",
    "**NOTE**: Since we're training biographies, the text itself will be skewed towards words likely to show up biographices--which isn't necessary like \"regular\" text. You may find that your model has few instances of words you think are common, or that the model learns poor or unusual neighbors for these. When querying the neighbors, it can help to think of which words you think are likely to show up in biographies on Wikipedia and use those as probes to see what the model has learned.\n",
    "\n",
    "Once you're convinced the model is learning, switch to the `med` data and train your model as specified in the PDF. Once trained, save your model using the `save()` function at the end of the notebook. This function records your data in a common format for word2vec vectors and lets you load the vectors into other libraries that have more advanced functionality. In particular, you can use the [gensim](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html) code in other notebook included to explore the vectors and do simple vector analogies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbors(model, word_to_index, target_word):\n",
    "    \"\"\" \n",
    "    Finds the top 10 most similar words to a target word\n",
    "    \"\"\"\n",
    "    outputs = []\n",
    "    for word, index in tqdm(word_to_index.items(), total=len(word_to_index)):\n",
    "        similarity = compute_cosine_similarity(model, word_to_index, target_word, word)\n",
    "        result = {\"word\": word, \"score\": similarity}\n",
    "        outputs.append(result)\n",
    "\n",
    "    # Sort by highest scores\n",
    "    neighbors = sorted(outputs, key=lambda o: o['score'], reverse=True)\n",
    "    return neighbors[1:1000]\n",
    "\n",
    "def compute_cosine_similarity(model, word_to_index, word_one, word_two):\n",
    "    '''\n",
    "    Computes the cosine similarity between the two words\n",
    "    '''\n",
    "    try:\n",
    "        word_one_index = word_to_index[word_one]\n",
    "        word_two_index = word_to_index[word_two]\n",
    "    except KeyError:\n",
    "#         print(\"============\")\n",
    "#         print(word_one,word_two)\n",
    "        return 0\n",
    "\n",
    "#     print(word_one_index,word_two_index)\n",
    "    embedding_one = model.target_embeddings(torch.LongTensor([word_one_index]))\n",
    "    embedding_two = model.target_embeddings(torch.LongTensor([word_two_index]))\n",
    "#     print(embedding_one,embedding_two,word_one_index,word_two_index)\n",
    "    similarity = 1 - abs(float(cosine(embedding_one.detach().numpy(),\n",
    "                                      embedding_two.detach().numpy())))\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_evaluation(model):\n",
    "    man_woman_sim = (compute_cosine_similarity(model, corpus.word_to_index,\"man\", \"woman\")\\\n",
    "        +compute_cosine_similarity(model, corpus.word_to_index,\"he\", \"she\"))/2\n",
    "    er_ress_sim = compute_cosine_similarity(model, corpus.word_to_index,\"actor\", \"actress\")\\\n",
    "                +compute_cosine_similarity(model, corpus.word_to_index,\"actor\", \"actress\")\\\n",
    "        +compute_cosine_similarity(model, corpus.word_to_index,\"waiter\", \"waitress\")\\\n",
    "        +compute_cosine_similarity(model, corpus.word_to_index,\"actor\", \"actress\")\\\n",
    "        +compute_cosine_similarity(model, corpus.word_to_index,\"steward\", \"stewardess\")\n",
    "    er_ress_sim=er_ress_sim/5\n",
    "    return 1 - max(max(man_woman_sim, 0),er_ress_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "225c2015cb924d1c8b1ce818be2129f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/680814 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1.0, loss: 998.1401433944702\n",
      "step: 2.0, loss: 995.9329824447632\n",
      "step: 3.0, loss: 994.2385625839233\n",
      "step: 4.0, loss: 994.141167640686\n",
      "step: 5.0, loss: 994.3312759399414\n",
      "step: 6.0, loss: 994.2990665435791\n",
      "step: 7.0, loss: 994.0350112915039\n",
      "step: 8.0, loss: 993.7798357009888\n",
      "step: 9.0, loss: 993.7109670639038\n",
      "step: 10.0, loss: 993.6447715759277\n",
      "step: 11.0, loss: 993.6056575775146\n",
      "step: 12.0, loss: 993.4813098907471\n",
      "step: 13.0, loss: 993.3919477462769\n",
      "step: 14.0, loss: 993.2496223449707\n",
      "step: 15.0, loss: 993.1263790130615\n",
      "step: 16.0, loss: 993.0352048873901\n",
      "step: 17.0, loss: 992.9101400375366\n",
      "step: 18.0, loss: 992.8140020370483\n",
      "step: 19.0, loss: 992.6486930847168\n",
      "step: 20.0, loss: 992.4874820709229\n",
      "step: 21.0, loss: 992.3602924346924\n",
      "step: 22.0, loss: 992.2697677612305\n",
      "step: 23.0, loss: 991.9124612808228\n",
      "step: 24.0, loss: 991.766923904419\n",
      "step: 25.0, loss: 991.7572374343872\n",
      "step: 26.0, loss: 991.5710077285767\n",
      "step: 27.0, loss: 991.5125408172607\n",
      "step: 28.0, loss: 991.2476806640625\n",
      "step: 29.0, loss: 990.9935855865479\n",
      "step: 30.0, loss: 990.9377031326294\n",
      "step: 31.0, loss: 990.7129287719727\n",
      "step: 32.0, loss: 990.4498529434204\n",
      "step: 33.0, loss: 990.3773822784424\n",
      "step: 34.0, loss: 990.1693229675293\n",
      "step: 35.0, loss: 990.4032926559448\n",
      "step: 36.0, loss: 990.0566101074219\n",
      "step: 37.0, loss: 989.6262159347534\n",
      "step: 38.0, loss: 989.8443193435669\n",
      "step: 39.0, loss: 989.3371057510376\n",
      "step: 40.0, loss: 989.2853021621704\n",
      "step: 41.0, loss: 989.4196577072144\n",
      "step: 42.0, loss: 988.8440780639648\n",
      "step: 43.0, loss: 988.5356960296631\n",
      "step: 44.0, loss: 988.7209768295288\n",
      "step: 45.0, loss: 988.4623365402222\n",
      "step: 46.0, loss: 987.9924325942993\n",
      "step: 47.0, loss: 988.0596628189087\n",
      "step: 48.0, loss: 987.9282598495483\n",
      "step: 49.0, loss: 987.8912906646729\n",
      "step: 50.0, loss: 987.8575010299683\n",
      "step: 51.0, loss: 987.3449096679688\n",
      "step: 52.0, loss: 987.6087045669556\n",
      "step: 53.0, loss: 987.4197359085083\n",
      "step: 54.0, loss: 986.6912574768066\n",
      "step: 55.0, loss: 986.7230167388916\n",
      "step: 56.0, loss: 987.2802734375\n",
      "step: 57.0, loss: 986.8737964630127\n",
      "step: 58.0, loss: 986.4281234741211\n",
      "step: 59.0, loss: 986.0222434997559\n",
      "step: 60.0, loss: 986.3397569656372\n",
      "step: 61.0, loss: 986.2884788513184\n",
      "step: 62.0, loss: 985.1812400817871\n",
      "step: 63.0, loss: 985.9138803482056\n",
      "step: 64.0, loss: 985.4860572814941\n",
      "step: 65.0, loss: 985.257682800293\n",
      "step: 66.0, loss: 985.2585897445679\n",
      "step: 67.0, loss: 985.1098012924194\n",
      "step: 68.0, loss: 985.3056783676147\n",
      "step: 69.0, loss: 984.7977485656738\n",
      "step: 70.0, loss: 984.7254619598389\n",
      "step: 71.0, loss: 984.6969337463379\n",
      "step: 72.0, loss: 984.1915817260742\n",
      "step: 73.0, loss: 984.0200242996216\n",
      "step: 74.0, loss: 984.3448534011841\n",
      "step: 75.0, loss: 983.9548063278198\n",
      "step: 76.0, loss: 983.8993768692017\n",
      "step: 77.0, loss: 983.620922088623\n",
      "step: 78.0, loss: 983.4562540054321\n",
      "step: 79.0, loss: 983.8536434173584\n",
      "step: 80.0, loss: 983.9179449081421\n",
      "step: 81.0, loss: 983.5765609741211\n",
      "step: 82.0, loss: 983.685845375061\n",
      "step: 83.0, loss: 983.5630235671997\n",
      "step: 84.0, loss: 982.9639377593994\n",
      "step: 85.0, loss: 982.6116514205933\n",
      "step: 86.0, loss: 982.6266965866089\n",
      "step: 87.0, loss: 982.3830451965332\n",
      "step: 88.0, loss: 982.2898292541504\n",
      "step: 89.0, loss: 981.5632638931274\n",
      "step: 90.0, loss: 982.5726337432861\n",
      "step: 91.0, loss: 982.5047807693481\n",
      "step: 92.0, loss: 981.5113229751587\n",
      "step: 93.0, loss: 981.7067632675171\n",
      "step: 94.0, loss: 981.1091985702515\n",
      "step: 95.0, loss: 982.2569408416748\n",
      "step: 96.0, loss: 981.8702669143677\n",
      "step: 97.0, loss: 981.611912727356\n",
      "step: 98.0, loss: 980.8384981155396\n",
      "step: 99.0, loss: 980.865647315979\n",
      "step: 100.0, loss: 981.3595027923584\n",
      "step: 101.0, loss: 981.1105947494507\n",
      "step: 102.0, loss: 980.1767978668213\n",
      "step: 103.0, loss: 980.3637447357178\n",
      "step: 104.0, loss: 980.6250104904175\n",
      "step: 105.0, loss: 979.6774816513062\n",
      "step: 106.0, loss: 980.5035171508789\n",
      "step: 107.0, loss: 981.002893447876\n",
      "step: 108.0, loss: 980.6766777038574\n",
      "step: 109.0, loss: 979.8181190490723\n",
      "step: 110.0, loss: 979.9645338058472\n",
      "step: 111.0, loss: 980.4714860916138\n",
      "step: 112.0, loss: 979.4945049285889\n",
      "step: 113.0, loss: 979.5994462966919\n",
      "step: 114.0, loss: 979.7355785369873\n",
      "step: 115.0, loss: 979.8275842666626\n",
      "step: 116.0, loss: 979.5128135681152\n",
      "step: 117.0, loss: 979.4666757583618\n",
      "step: 118.0, loss: 978.9772262573242\n",
      "step: 119.0, loss: 978.8725433349609\n",
      "step: 120.0, loss: 978.6291179656982\n",
      "step: 121.0, loss: 978.9242715835571\n",
      "step: 122.0, loss: 978.6725959777832\n",
      "step: 123.0, loss: 978.7349605560303\n",
      "step: 124.0, loss: 979.0377130508423\n",
      "step: 125.0, loss: 977.9736318588257\n",
      "step: 126.0, loss: 978.1544895172119\n",
      "step: 127.0, loss: 977.2875871658325\n",
      "step: 128.0, loss: 978.5991811752319\n",
      "step: 129.0, loss: 977.7082929611206\n",
      "step: 130.0, loss: 977.7871932983398\n",
      "step: 131.0, loss: 977.143388748169\n",
      "step: 132.0, loss: 977.6731605529785\n",
      "step: 133.0, loss: 977.0383787155151\n",
      "step: 134.0, loss: 976.7670154571533\n",
      "step: 135.0, loss: 977.2163887023926\n",
      "step: 136.0, loss: 976.8712577819824\n",
      "step: 137.0, loss: 977.0175199508667\n",
      "step: 138.0, loss: 976.8213958740234\n",
      "step: 139.0, loss: 976.823371887207\n",
      "step: 140.0, loss: 976.7837810516357\n",
      "step: 141.0, loss: 976.3152093887329\n",
      "step: 142.0, loss: 977.1075353622437\n",
      "step: 143.0, loss: 976.2542505264282\n",
      "step: 144.0, loss: 976.6979694366455\n",
      "step: 145.0, loss: 975.8720607757568\n",
      "step: 146.0, loss: 975.9576568603516\n",
      "step: 147.0, loss: 976.1450262069702\n",
      "step: 148.0, loss: 976.1612710952759\n",
      "step: 149.0, loss: 975.9284687042236\n",
      "step: 150.0, loss: 976.0798416137695\n",
      "step: 151.0, loss: 976.0529136657715\n",
      "step: 152.0, loss: 976.3105268478394\n",
      "step: 153.0, loss: 976.0831813812256\n",
      "step: 154.0, loss: 975.628963470459\n",
      "step: 155.0, loss: 975.2529611587524\n",
      "step: 156.0, loss: 975.6599159240723\n",
      "step: 157.0, loss: 974.8900671005249\n",
      "step: 158.0, loss: 974.5355186462402\n",
      "step: 159.0, loss: 975.3958883285522\n",
      "step: 160.0, loss: 974.347692489624\n",
      "step: 161.0, loss: 975.3988008499146\n",
      "step: 162.0, loss: 974.0770473480225\n",
      "step: 163.0, loss: 974.2765655517578\n",
      "step: 164.0, loss: 974.5948810577393\n",
      "step: 165.0, loss: 974.3378057479858\n",
      "step: 166.0, loss: 974.4959230422974\n",
      "step: 167.0, loss: 974.0450973510742\n",
      "step: 168.0, loss: 974.2783193588257\n",
      "step: 169.0, loss: 973.6004467010498\n",
      "step: 170.0, loss: 974.1088199615479\n",
      "step: 171.0, loss: 973.3029365539551\n",
      "step: 172.0, loss: 974.4069356918335\n",
      "step: 173.0, loss: 973.4742774963379\n",
      "step: 174.0, loss: 972.8181867599487\n",
      "step: 175.0, loss: 972.9291677474976\n",
      "step: 176.0, loss: 973.9699239730835\n",
      "step: 177.0, loss: 973.9207630157471\n",
      "step: 178.0, loss: 972.2822933197021\n",
      "step: 179.0, loss: 973.1510591506958\n",
      "step: 180.0, loss: 972.4216003417969\n",
      "step: 181.0, loss: 973.5734882354736\n",
      "step: 182.0, loss: 973.1246566772461\n",
      "step: 183.0, loss: 972.3113613128662\n",
      "step: 184.0, loss: 972.5048503875732\n",
      "step: 185.0, loss: 973.071455001831\n",
      "step: 186.0, loss: 973.1107692718506\n",
      "step: 187.0, loss: 972.2217235565186\n",
      "step: 188.0, loss: 972.7636585235596\n",
      "step: 189.0, loss: 972.2542543411255\n",
      "step: 190.0, loss: 972.4663152694702\n",
      "step: 191.0, loss: 972.4119787216187\n",
      "step: 192.0, loss: 972.0550336837769\n",
      "step: 193.0, loss: 971.6758127212524\n",
      "step: 194.0, loss: 972.3807582855225\n",
      "step: 195.0, loss: 972.5078802108765\n",
      "step: 196.0, loss: 971.0661182403564\n",
      "step: 197.0, loss: 972.4265956878662\n",
      "step: 198.0, loss: 971.0201005935669\n",
      "step: 199.0, loss: 970.7236156463623\n",
      "step: 200.0, loss: 972.1971635818481\n",
      "step: 201.0, loss: 971.016191482544\n",
      "step: 202.0, loss: 971.4100761413574\n",
      "step: 203.0, loss: 971.815185546875\n",
      "step: 204.0, loss: 970.5991840362549\n",
      "step: 205.0, loss: 971.3121128082275\n",
      "step: 206.0, loss: 970.6017045974731\n",
      "step: 207.0, loss: 971.2477836608887\n",
      "step: 208.0, loss: 971.215612411499\n",
      "step: 209.0, loss: 970.5732135772705\n",
      "step: 210.0, loss: 971.1924228668213\n",
      "step: 211.0, loss: 970.9609518051147\n",
      "step: 212.0, loss: 970.920690536499\n",
      "step: 213.0, loss: 971.002631187439\n",
      "step: 214.0, loss: 970.611081123352\n",
      "step: 215.0, loss: 970.3357601165771\n",
      "step: 216.0, loss: 970.9935293197632\n",
      "step: 217.0, loss: 970.4590530395508\n",
      "step: 218.0, loss: 969.6146774291992\n",
      "step: 219.0, loss: 970.4618835449219\n",
      "step: 220.0, loss: 969.4790830612183\n",
      "step: 221.0, loss: 970.219274520874\n",
      "step: 222.0, loss: 969.8668327331543\n",
      "step: 223.0, loss: 969.5693140029907\n",
      "step: 224.0, loss: 969.895770072937\n",
      "step: 225.0, loss: 970.1924772262573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 226.0, loss: 969.3612031936646\n",
      "step: 227.0, loss: 969.143964767456\n",
      "step: 228.0, loss: 970.1375675201416\n",
      "step: 229.0, loss: 968.9046154022217\n",
      "step: 230.0, loss: 969.6152925491333\n",
      "step: 231.0, loss: 969.399938583374\n",
      "step: 232.0, loss: 970.3990602493286\n",
      "step: 233.0, loss: 969.4069957733154\n",
      "step: 234.0, loss: 969.5409822463989\n",
      "step: 235.0, loss: 969.4139881134033\n",
      "step: 236.0, loss: 968.5234308242798\n",
      "step: 237.0, loss: 968.6493577957153\n",
      "step: 238.0, loss: 969.040020942688\n",
      "step: 239.0, loss: 969.0174369812012\n",
      "step: 240.0, loss: 967.5755271911621\n",
      "step: 241.0, loss: 969.2416944503784\n",
      "step: 242.0, loss: 969.0648279190063\n",
      "step: 243.0, loss: 969.2906551361084\n",
      "step: 244.0, loss: 968.1310768127441\n",
      "step: 245.0, loss: 968.3900203704834\n",
      "step: 246.0, loss: 969.4932661056519\n",
      "step: 247.0, loss: 968.4427671432495\n",
      "step: 248.0, loss: 968.566596031189\n",
      "step: 249.0, loss: 969.6232147216797\n",
      "step: 250.0, loss: 967.8563852310181\n",
      "step: 251.0, loss: 968.6176233291626\n",
      "step: 252.0, loss: 968.3653335571289\n",
      "step: 253.0, loss: 968.4499521255493\n",
      "step: 254.0, loss: 968.2229061126709\n",
      "step: 255.0, loss: 968.2468156814575\n",
      "step: 256.0, loss: 967.8018493652344\n",
      "step: 257.0, loss: 967.6558809280396\n",
      "step: 258.0, loss: 967.6510744094849\n",
      "step: 259.0, loss: 967.608567237854\n",
      "step: 260.0, loss: 967.7230358123779\n",
      "step: 261.0, loss: 967.2032499313354\n",
      "step: 262.0, loss: 967.3491363525391\n",
      "step: 263.0, loss: 967.6513042449951\n",
      "step: 264.0, loss: 967.0632734298706\n",
      "step: 265.0, loss: 966.8803796768188\n",
      "step: 266.0, loss: 966.8626937866211\n",
      "step: 267.0, loss: 966.7247648239136\n",
      "step: 268.0, loss: 966.9324264526367\n",
      "step: 269.0, loss: 966.9068946838379\n",
      "step: 270.0, loss: 967.1498212814331\n",
      "step: 271.0, loss: 965.9278573989868\n",
      "step: 272.0, loss: 966.6056747436523\n",
      "step: 273.0, loss: 966.6824378967285\n",
      "step: 274.0, loss: 966.5112504959106\n",
      "step: 275.0, loss: 966.5470018386841\n",
      "step: 276.0, loss: 966.8553161621094\n",
      "step: 277.0, loss: 966.7292919158936\n",
      "step: 278.0, loss: 966.2437429428101\n",
      "step: 279.0, loss: 966.3572816848755\n",
      "step: 280.0, loss: 965.4147081375122\n",
      "step: 281.0, loss: 966.9238471984863\n",
      "step: 282.0, loss: 965.9061145782471\n",
      "step: 283.0, loss: 966.133731842041\n",
      "step: 284.0, loss: 964.5060129165649\n",
      "step: 285.0, loss: 966.0871963500977\n",
      "step: 286.0, loss: 965.9288034439087\n",
      "step: 287.0, loss: 966.4514808654785\n",
      "step: 288.0, loss: 966.7554550170898\n",
      "step: 289.0, loss: 965.4999208450317\n",
      "step: 290.0, loss: 966.0069427490234\n",
      "step: 291.0, loss: 965.2367649078369\n",
      "step: 292.0, loss: 966.3099641799927\n",
      "step: 293.0, loss: 965.5399923324585\n",
      "step: 294.0, loss: 965.4672622680664\n",
      "step: 295.0, loss: 965.4729442596436\n",
      "step: 296.0, loss: 965.2736730575562\n",
      "step: 297.0, loss: 965.0837297439575\n",
      "step: 298.0, loss: 964.9946165084839\n",
      "step: 299.0, loss: 965.3429794311523\n",
      "step: 300.0, loss: 963.912091255188\n",
      "step: 301.0, loss: 965.382622718811\n",
      "step: 302.0, loss: 964.1914129257202\n",
      "step: 303.0, loss: 965.4114580154419\n",
      "step: 304.0, loss: 964.2233581542969\n",
      "step: 305.0, loss: 964.9782819747925\n",
      "step: 306.0, loss: 965.7276945114136\n",
      "step: 307.0, loss: 965.1069946289062\n",
      "step: 308.0, loss: 964.7133846282959\n",
      "step: 309.0, loss: 966.0823287963867\n",
      "step: 310.0, loss: 965.4639348983765\n",
      "step: 311.0, loss: 964.7700815200806\n",
      "step: 312.0, loss: 965.2262201309204\n",
      "step: 313.0, loss: 964.0366287231445\n",
      "step: 314.0, loss: 964.5956830978394\n",
      "step: 315.0, loss: 964.9702110290527\n",
      "step: 316.0, loss: 964.1649703979492\n",
      "step: 317.0, loss: 964.6514129638672\n",
      "step: 318.0, loss: 964.1847114562988\n",
      "step: 319.0, loss: 964.8033828735352\n",
      "step: 320.0, loss: 964.4284172058105\n",
      "step: 321.0, loss: 964.7884769439697\n",
      "step: 322.0, loss: 964.1246919631958\n",
      "step: 323.0, loss: 964.4984483718872\n",
      "step: 324.0, loss: 964.4177227020264\n",
      "step: 325.0, loss: 963.4603729248047\n",
      "step: 326.0, loss: 964.4946327209473\n",
      "step: 327.0, loss: 964.5828237533569\n",
      "step: 328.0, loss: 962.8070240020752\n",
      "step: 329.0, loss: 965.3149852752686\n",
      "step: 330.0, loss: 964.0942440032959\n",
      "step: 331.0, loss: 963.7567548751831\n",
      "step: 332.0, loss: 965.2812490463257\n",
      "step: 333.0, loss: 963.7258644104004\n",
      "step: 334.0, loss: 965.0105962753296\n",
      "step: 335.0, loss: 963.9211978912354\n",
      "step: 336.0, loss: 963.649227142334\n",
      "step: 337.0, loss: 963.67347240448\n",
      "step: 338.0, loss: 964.6787567138672\n",
      "step: 339.0, loss: 962.6568603515625\n",
      "step: 340.0, loss: 964.2052812576294\n",
      "step: 341.0, loss: 962.8920955657959\n",
      "step: 342.0, loss: 963.3904666900635\n",
      "step: 343.0, loss: 963.8633337020874\n",
      "step: 344.0, loss: 962.9855766296387\n",
      "step: 345.0, loss: 963.2722320556641\n",
      "step: 346.0, loss: 963.0652103424072\n",
      "step: 347.0, loss: 962.749382019043\n",
      "step: 348.0, loss: 962.6948833465576\n",
      "step: 349.0, loss: 962.614049911499\n",
      "step: 350.0, loss: 963.5906934738159\n",
      "step: 351.0, loss: 963.811764717102\n",
      "step: 352.0, loss: 963.1192398071289\n",
      "step: 353.0, loss: 962.1994276046753\n",
      "step: 354.0, loss: 963.1372289657593\n",
      "step: 355.0, loss: 962.8441896438599\n",
      "step: 356.0, loss: 962.4632406234741\n",
      "step: 357.0, loss: 962.4355773925781\n",
      "step: 358.0, loss: 963.4250059127808\n",
      "step: 359.0, loss: 962.3404407501221\n",
      "step: 360.0, loss: 962.8999519348145\n",
      "step: 361.0, loss: 962.6412410736084\n",
      "step: 362.0, loss: 962.42209815979\n",
      "step: 363.0, loss: 963.2029180526733\n",
      "step: 364.0, loss: 962.794264793396\n",
      "step: 365.0, loss: 962.3110904693604\n",
      "step: 366.0, loss: 961.7524433135986\n",
      "step: 367.0, loss: 962.9471740722656\n",
      "step: 368.0, loss: 962.0966510772705\n",
      "step: 369.0, loss: 961.9729309082031\n",
      "step: 370.0, loss: 960.3505811691284\n",
      "step: 371.0, loss: 962.2035484313965\n",
      "step: 372.0, loss: 962.0837554931641\n",
      "step: 373.0, loss: 961.9140539169312\n",
      "step: 374.0, loss: 962.0291767120361\n",
      "step: 375.0, loss: 962.7177610397339\n",
      "step: 376.0, loss: 962.8579044342041\n",
      "step: 377.0, loss: 961.1845178604126\n",
      "step: 378.0, loss: 962.5255832672119\n",
      "step: 379.0, loss: 962.2836198806763\n",
      "step: 380.0, loss: 963.068844795227\n",
      "step: 381.0, loss: 961.9356784820557\n",
      "step: 382.0, loss: 961.6009407043457\n",
      "step: 383.0, loss: 962.1915798187256\n",
      "step: 384.0, loss: 962.2935285568237\n",
      "step: 385.0, loss: 962.4975357055664\n",
      "step: 386.0, loss: 960.7203521728516\n",
      "step: 387.0, loss: 960.7667217254639\n",
      "step: 388.0, loss: 962.3687391281128\n",
      "step: 389.0, loss: 961.6558513641357\n",
      "step: 390.0, loss: 960.6784048080444\n",
      "step: 391.0, loss: 962.0363121032715\n",
      "step: 392.0, loss: 960.8359298706055\n",
      "step: 393.0, loss: 961.4521884918213\n",
      "step: 394.0, loss: 960.9532098770142\n",
      "step: 395.0, loss: 961.3494634628296\n",
      "step: 396.0, loss: 962.4203081130981\n",
      "step: 397.0, loss: 961.3591756820679\n",
      "step: 398.0, loss: 961.9837589263916\n",
      "step: 399.0, loss: 962.1672639846802\n",
      "step: 400.0, loss: 961.3018550872803\n",
      "step: 401.0, loss: 961.1134223937988\n",
      "step: 402.0, loss: 961.2813892364502\n",
      "step: 403.0, loss: 961.1027641296387\n",
      "step: 404.0, loss: 961.4382629394531\n",
      "step: 405.0, loss: 961.323561668396\n",
      "step: 406.0, loss: 961.4027080535889\n",
      "step: 407.0, loss: 960.9330816268921\n",
      "step: 408.0, loss: 959.3514995574951\n",
      "step: 409.0, loss: 960.6222248077393\n",
      "step: 410.0, loss: 960.4009246826172\n",
      "step: 411.0, loss: 961.2897682189941\n",
      "step: 412.0, loss: 961.3295526504517\n",
      "step: 413.0, loss: 960.5050573348999\n",
      "step: 414.0, loss: 960.8325729370117\n",
      "step: 415.0, loss: 959.7175512313843\n",
      "step: 416.0, loss: 959.9206008911133\n",
      "step: 417.0, loss: 961.3958177566528\n",
      "step: 418.0, loss: 960.0364360809326\n",
      "step: 419.0, loss: 960.7062158584595\n",
      "step: 420.0, loss: 960.013126373291\n",
      "step: 421.0, loss: 960.4178609848022\n",
      "step: 422.0, loss: 960.5533180236816\n",
      "step: 423.0, loss: 961.1749153137207\n",
      "step: 424.0, loss: 959.0845012664795\n",
      "step: 425.0, loss: 958.77956199646\n",
      "step: 426.0, loss: 961.1310262680054\n",
      "step: 427.0, loss: 960.7382164001465\n",
      "step: 428.0, loss: 961.0076837539673\n",
      "step: 429.0, loss: 961.8940687179565\n",
      "step: 430.0, loss: 961.0813226699829\n",
      "step: 431.0, loss: 960.5646200180054\n",
      "step: 432.0, loss: 959.5422525405884\n",
      "step: 433.0, loss: 960.0454864501953\n",
      "step: 434.0, loss: 960.3064422607422\n",
      "step: 435.0, loss: 959.1028356552124\n",
      "step: 436.0, loss: 960.3121061325073\n",
      "step: 437.0, loss: 959.6862192153931\n",
      "step: 438.0, loss: 960.1341714859009\n",
      "step: 439.0, loss: 960.2412824630737\n",
      "step: 440.0, loss: 960.1097478866577\n",
      "step: 441.0, loss: 959.0635242462158\n",
      "step: 442.0, loss: 959.7715034484863\n",
      "step: 443.0, loss: 959.4305553436279\n",
      "step: 444.0, loss: 959.1718664169312\n",
      "step: 445.0, loss: 960.2127389907837\n",
      "step: 446.0, loss: 960.1746320724487\n",
      "step: 447.0, loss: 958.8470344543457\n",
      "step: 448.0, loss: 959.7311162948608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 449.0, loss: 960.0613126754761\n",
      "step: 450.0, loss: 959.0369472503662\n",
      "step: 451.0, loss: 960.4486694335938\n",
      "step: 452.0, loss: 960.1271924972534\n",
      "step: 453.0, loss: 959.3364210128784\n",
      "step: 454.0, loss: 959.1240272521973\n",
      "step: 455.0, loss: 960.0269594192505\n",
      "step: 456.0, loss: 958.7825269699097\n",
      "step: 457.0, loss: 959.3619508743286\n",
      "step: 458.0, loss: 959.5500965118408\n",
      "step: 459.0, loss: 958.859787940979\n",
      "step: 460.0, loss: 959.4976625442505\n",
      "step: 461.0, loss: 959.4323167800903\n",
      "step: 462.0, loss: 958.0067157745361\n",
      "step: 463.0, loss: 959.1380434036255\n",
      "step: 464.0, loss: 958.8501987457275\n",
      "step: 465.0, loss: 958.3387250900269\n",
      "step: 466.0, loss: 957.5407495498657\n",
      "step: 467.0, loss: 957.7642126083374\n",
      "step: 468.0, loss: 958.4733276367188\n",
      "step: 469.0, loss: 959.3065214157104\n",
      "step: 470.0, loss: 958.5182466506958\n",
      "step: 471.0, loss: 959.113317489624\n",
      "step: 472.0, loss: 958.7238903045654\n",
      "step: 473.0, loss: 957.6771326065063\n",
      "step: 474.0, loss: 958.5432615280151\n",
      "step: 475.0, loss: 959.1093521118164\n",
      "step: 476.0, loss: 957.8202962875366\n",
      "step: 477.0, loss: 957.1572341918945\n",
      "step: 478.0, loss: 957.710391998291\n",
      "step: 479.0, loss: 958.4800596237183\n",
      "step: 480.0, loss: 958.167986869812\n",
      "step: 481.0, loss: 958.9168462753296\n",
      "step: 482.0, loss: 957.7679042816162\n",
      "step: 483.0, loss: 959.1926136016846\n",
      "step: 484.0, loss: 959.4985313415527\n",
      "step: 485.0, loss: 958.181881904602\n",
      "step: 486.0, loss: 957.7805395126343\n",
      "step: 487.0, loss: 958.5397930145264\n",
      "step: 488.0, loss: 958.6613864898682\n",
      "step: 489.0, loss: 957.3638801574707\n",
      "step: 490.0, loss: 958.0743627548218\n",
      "step: 491.0, loss: 957.2316932678223\n",
      "step: 492.0, loss: 957.6111068725586\n",
      "step: 493.0, loss: 957.5115776062012\n",
      "step: 494.0, loss: 957.53147315979\n",
      "step: 495.0, loss: 958.6018142700195\n",
      "step: 496.0, loss: 958.5643444061279\n",
      "step: 497.0, loss: 958.6209011077881\n",
      "step: 498.0, loss: 958.088475227356\n",
      "step: 499.0, loss: 958.0314283370972\n",
      "step: 500.0, loss: 957.960825920105\n",
      "step: 501.0, loss: 958.1190547943115\n",
      "step: 502.0, loss: 956.6420850753784\n",
      "step: 503.0, loss: 956.2979164123535\n",
      "step: 504.0, loss: 958.7972726821899\n",
      "step: 505.0, loss: 958.1112461090088\n",
      "step: 506.0, loss: 957.0894861221313\n",
      "step: 507.0, loss: 957.6637687683105\n",
      "step: 508.0, loss: 957.2054252624512\n",
      "step: 509.0, loss: 958.093430519104\n",
      "step: 510.0, loss: 958.1046266555786\n",
      "step: 511.0, loss: 957.4484739303589\n",
      "step: 512.0, loss: 956.2344703674316\n",
      "step: 513.0, loss: 957.3477153778076\n",
      "step: 514.0, loss: 958.4258165359497\n",
      "step: 515.0, loss: 958.6968936920166\n",
      "step: 516.0, loss: 958.2520685195923\n",
      "step: 517.0, loss: 956.8618516921997\n",
      "step: 518.0, loss: 957.1220598220825\n",
      "step: 519.0, loss: 956.5360174179077\n",
      "step: 520.0, loss: 957.1857681274414\n",
      "step: 521.0, loss: 957.7071485519409\n",
      "step: 522.0, loss: 956.9409914016724\n",
      "step: 523.0, loss: 956.048150062561\n",
      "step: 524.0, loss: 957.3707809448242\n",
      "step: 525.0, loss: 957.2343482971191\n",
      "step: 526.0, loss: 957.141942024231\n",
      "step: 527.0, loss: 956.2210683822632\n",
      "step: 528.0, loss: 957.8124122619629\n",
      "step: 529.0, loss: 956.7474241256714\n",
      "step: 530.0, loss: 958.0058088302612\n",
      "step: 531.0, loss: 956.9046115875244\n",
      "step: 532.0, loss: 956.6030826568604\n",
      "step: 533.0, loss: 957.2336883544922\n",
      "step: 534.0, loss: 955.970380783081\n",
      "step: 535.0, loss: 956.343297958374\n",
      "step: 536.0, loss: 957.6152153015137\n",
      "step: 537.0, loss: 955.8673601150513\n",
      "step: 538.0, loss: 956.0048980712891\n",
      "step: 539.0, loss: 957.2647390365601\n",
      "step: 540.0, loss: 957.2943267822266\n",
      "step: 541.0, loss: 955.2633571624756\n",
      "step: 542.0, loss: 957.5503664016724\n",
      "step: 543.0, loss: 956.3608541488647\n",
      "step: 544.0, loss: 955.2374172210693\n",
      "step: 545.0, loss: 957.3188638687134\n",
      "step: 546.0, loss: 956.4322032928467\n",
      "step: 547.0, loss: 956.3564186096191\n",
      "step: 548.0, loss: 957.2237157821655\n",
      "step: 549.0, loss: 958.7681894302368\n",
      "step: 550.0, loss: 956.4823904037476\n",
      "step: 551.0, loss: 957.1601047515869\n",
      "step: 552.0, loss: 955.5590314865112\n",
      "step: 553.0, loss: 956.4404010772705\n",
      "step: 554.0, loss: 956.9926071166992\n",
      "step: 555.0, loss: 956.3163585662842\n",
      "step: 556.0, loss: 955.9646291732788\n",
      "step: 557.0, loss: 956.4575090408325\n",
      "step: 558.0, loss: 956.464189529419\n",
      "step: 559.0, loss: 957.4716653823853\n",
      "step: 560.0, loss: 957.0788669586182\n",
      "step: 561.0, loss: 956.3235721588135\n",
      "step: 562.0, loss: 955.9726934432983\n",
      "step: 563.0, loss: 955.987795829773\n",
      "step: 564.0, loss: 955.9639253616333\n",
      "step: 565.0, loss: 955.9549074172974\n",
      "step: 566.0, loss: 956.8680400848389\n",
      "step: 567.0, loss: 956.0730199813843\n",
      "step: 568.0, loss: 955.4479560852051\n",
      "step: 569.0, loss: 956.1545009613037\n",
      "step: 570.0, loss: 955.7567663192749\n",
      "step: 571.0, loss: 955.665433883667\n",
      "step: 572.0, loss: 956.3487253189087\n",
      "step: 573.0, loss: 955.9894828796387\n",
      "step: 574.0, loss: 955.6488399505615\n",
      "step: 575.0, loss: 955.0740127563477\n",
      "step: 576.0, loss: 956.8065128326416\n",
      "step: 577.0, loss: 955.5013818740845\n",
      "step: 578.0, loss: 954.8251466751099\n",
      "step: 579.0, loss: 955.3690614700317\n",
      "step: 580.0, loss: 955.8383512496948\n",
      "step: 581.0, loss: 955.4997596740723\n",
      "step: 582.0, loss: 955.4806060791016\n",
      "step: 583.0, loss: 954.6139669418335\n",
      "step: 584.0, loss: 955.295747756958\n",
      "step: 585.0, loss: 955.0776205062866\n",
      "step: 586.0, loss: 956.7962484359741\n",
      "step: 587.0, loss: 954.9066438674927\n",
      "step: 588.0, loss: 955.2615728378296\n",
      "step: 589.0, loss: 956.2097082138062\n",
      "step: 590.0, loss: 955.3159246444702\n",
      "step: 591.0, loss: 955.3186159133911\n",
      "step: 592.0, loss: 955.5689678192139\n",
      "step: 593.0, loss: 955.1853065490723\n",
      "step: 594.0, loss: 955.4535675048828\n",
      "step: 595.0, loss: 954.3900003433228\n",
      "step: 596.0, loss: 955.4115266799927\n",
      "step: 597.0, loss: 954.0837697982788\n",
      "step: 598.0, loss: 955.5579214096069\n",
      "step: 599.0, loss: 955.967038154602\n",
      "step: 600.0, loss: 954.3723554611206\n",
      "step: 601.0, loss: 955.54212474823\n",
      "step: 602.0, loss: 955.1096343994141\n",
      "step: 603.0, loss: 954.709231376648\n",
      "step: 604.0, loss: 955.8703517913818\n",
      "step: 605.0, loss: 954.8101892471313\n",
      "step: 606.0, loss: 955.8992643356323\n",
      "step: 607.0, loss: 954.4891605377197\n",
      "step: 608.0, loss: 954.244460105896\n",
      "step: 609.0, loss: 955.2283201217651\n",
      "step: 610.0, loss: 954.3122653961182\n",
      "step: 611.0, loss: 955.2371215820312\n",
      "step: 612.0, loss: 953.6857109069824\n",
      "step: 613.0, loss: 954.416181564331\n",
      "step: 614.0, loss: 954.1704235076904\n",
      "step: 615.0, loss: 954.2995100021362\n",
      "step: 616.0, loss: 955.7474927902222\n",
      "step: 617.0, loss: 955.2375841140747\n",
      "step: 618.0, loss: 954.6519260406494\n",
      "step: 619.0, loss: 954.239089012146\n",
      "step: 620.0, loss: 954.9298191070557\n",
      "step: 621.0, loss: 954.1222505569458\n",
      "step: 622.0, loss: 954.4465103149414\n",
      "step: 623.0, loss: 954.3200740814209\n",
      "step: 624.0, loss: 954.2779693603516\n",
      "step: 625.0, loss: 954.4075870513916\n",
      "step: 626.0, loss: 954.5308666229248\n",
      "step: 627.0, loss: 955.5449867248535\n",
      "step: 628.0, loss: 954.9655857086182\n",
      "step: 629.0, loss: 955.0942897796631\n",
      "step: 630.0, loss: 953.1329736709595\n",
      "step: 631.0, loss: 954.8793668746948\n",
      "step: 632.0, loss: 953.699538230896\n",
      "step: 633.0, loss: 953.8062973022461\n",
      "step: 634.0, loss: 955.0463972091675\n",
      "step: 635.0, loss: 954.7342233657837\n",
      "step: 636.0, loss: 954.0318384170532\n",
      "step: 637.0, loss: 955.0907745361328\n",
      "step: 638.0, loss: 954.399206161499\n",
      "step: 639.0, loss: 953.293571472168\n",
      "step: 640.0, loss: 954.8363599777222\n",
      "step: 641.0, loss: 954.776104927063\n",
      "step: 642.0, loss: 954.5941143035889\n",
      "step: 643.0, loss: 954.2114763259888\n",
      "step: 644.0, loss: 954.7095222473145\n",
      "step: 645.0, loss: 955.0281496047974\n",
      "step: 646.0, loss: 953.1830959320068\n",
      "step: 647.0, loss: 953.5984544754028\n",
      "step: 648.0, loss: 954.3932638168335\n",
      "step: 649.0, loss: 953.8157329559326\n",
      "step: 650.0, loss: 954.7332105636597\n",
      "step: 651.0, loss: 955.2089319229126\n",
      "step: 652.0, loss: 955.3946371078491\n",
      "step: 653.0, loss: 953.8096866607666\n",
      "step: 654.0, loss: 955.1686868667603\n",
      "step: 655.0, loss: 954.0246829986572\n",
      "step: 656.0, loss: 955.0714321136475\n",
      "step: 657.0, loss: 954.1921224594116\n",
      "step: 658.0, loss: 953.1398944854736\n",
      "step: 659.0, loss: 953.721116065979\n",
      "step: 660.0, loss: 953.6093797683716\n",
      "step: 661.0, loss: 954.9141807556152\n",
      "step: 662.0, loss: 952.4911270141602\n",
      "step: 663.0, loss: 954.9289751052856\n",
      "step: 664.0, loss: 954.0472373962402\n",
      "step: 665.0, loss: 953.6160926818848\n",
      "step: 666.0, loss: 954.3443059921265\n",
      "step: 667.0, loss: 953.3744297027588\n",
      "step: 668.0, loss: 953.4954280853271\n",
      "step: 669.0, loss: 953.3986301422119\n",
      "step: 670.0, loss: 952.6589460372925\n",
      "step: 671.0, loss: 952.8481130599976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 672.0, loss: 953.6053810119629\n",
      "step: 673.0, loss: 953.3173770904541\n",
      "step: 674.0, loss: 952.8842344284058\n",
      "step: 675.0, loss: 952.2291851043701\n",
      "step: 676.0, loss: 954.2591648101807\n",
      "step: 677.0, loss: 952.596396446228\n",
      "step: 678.0, loss: 954.2597513198853\n",
      "step: 679.0, loss: 954.2100057601929\n",
      "step: 680.0, loss: 953.2261009216309\n",
      "step: 681.0, loss: 953.0515537261963\n",
      "step: 682.0, loss: 953.4767055511475\n",
      "step: 683.0, loss: 954.3754634857178\n",
      "step: 684.0, loss: 952.8189611434937\n",
      "step: 685.0, loss: 951.5963134765625\n",
      "step: 686.0, loss: 953.8276510238647\n",
      "step: 687.0, loss: 953.513671875\n",
      "step: 688.0, loss: 953.254225730896\n",
      "step: 689.0, loss: 954.1397876739502\n",
      "step: 690.0, loss: 953.2795391082764\n",
      "step: 691.0, loss: 952.9120216369629\n",
      "step: 692.0, loss: 952.6092767715454\n",
      "step: 693.0, loss: 952.8358097076416\n",
      "step: 694.0, loss: 954.0168190002441\n",
      "step: 695.0, loss: 952.851803779602\n",
      "step: 696.0, loss: 953.1328439712524\n",
      "step: 697.0, loss: 953.9074544906616\n",
      "step: 698.0, loss: 953.5862712860107\n",
      "step: 699.0, loss: 951.6693124771118\n",
      "step: 700.0, loss: 952.9895133972168\n",
      "step: 701.0, loss: 952.3411502838135\n",
      "step: 702.0, loss: 953.1349716186523\n",
      "step: 703.0, loss: 953.3489465713501\n",
      "step: 704.0, loss: 951.1507596969604\n",
      "step: 705.0, loss: 952.8711681365967\n",
      "step: 706.0, loss: 952.5624446868896\n",
      "step: 707.0, loss: 953.5586261749268\n",
      "step: 708.0, loss: 951.743221282959\n",
      "step: 709.0, loss: 953.3014602661133\n",
      "step: 710.0, loss: 951.8561954498291\n",
      "step: 711.0, loss: 953.4928941726685\n",
      "step: 712.0, loss: 951.2221422195435\n",
      "step: 713.0, loss: 953.2073154449463\n",
      "step: 714.0, loss: 952.7515163421631\n",
      "step: 715.0, loss: 952.1002960205078\n",
      "step: 716.0, loss: 952.666464805603\n",
      "step: 717.0, loss: 953.1305723190308\n",
      "step: 718.0, loss: 953.4225378036499\n",
      "step: 719.0, loss: 952.1143712997437\n",
      "step: 720.0, loss: 953.298077583313\n",
      "step: 721.0, loss: 951.6289978027344\n",
      "step: 722.0, loss: 951.1320486068726\n",
      "step: 723.0, loss: 953.8202085494995\n",
      "step: 724.0, loss: 952.114010810852\n",
      "step: 725.0, loss: 952.1186609268188\n",
      "step: 726.0, loss: 952.5894451141357\n",
      "step: 727.0, loss: 951.5575637817383\n",
      "step: 728.0, loss: 952.9209852218628\n",
      "step: 729.0, loss: 952.4480180740356\n",
      "step: 730.0, loss: 952.6519060134888\n",
      "step: 731.0, loss: 951.5310354232788\n",
      "step: 732.0, loss: 952.5821743011475\n",
      "step: 733.0, loss: 951.7827005386353\n",
      "step: 734.0, loss: 952.3854284286499\n",
      "step: 735.0, loss: 953.0966634750366\n",
      "step: 736.0, loss: 952.8410863876343\n",
      "step: 737.0, loss: 951.9669799804688\n",
      "step: 738.0, loss: 952.5516395568848\n",
      "step: 739.0, loss: 952.0459461212158\n",
      "step: 740.0, loss: 951.2106847763062\n",
      "step: 741.0, loss: 951.5441102981567\n",
      "step: 742.0, loss: 952.3093738555908\n",
      "step: 743.0, loss: 952.3966522216797\n",
      "step: 744.0, loss: 952.3022623062134\n",
      "step: 745.0, loss: 951.0447998046875\n",
      "step: 746.0, loss: 951.8817682266235\n",
      "step: 747.0, loss: 952.9460649490356\n",
      "step: 748.0, loss: 953.2994441986084\n",
      "step: 749.0, loss: 950.8501472473145\n",
      "step: 750.0, loss: 951.3312788009644\n",
      "step: 751.0, loss: 952.5178098678589\n",
      "step: 752.0, loss: 952.0192451477051\n",
      "step: 753.0, loss: 951.1012010574341\n",
      "step: 754.0, loss: 951.7754182815552\n",
      "step: 755.0, loss: 951.9726114273071\n",
      "step: 756.0, loss: 950.5715761184692\n",
      "step: 757.0, loss: 951.369909286499\n",
      "step: 758.0, loss: 951.052882194519\n",
      "step: 759.0, loss: 951.6394309997559\n",
      "step: 760.0, loss: 951.0370721817017\n",
      "step: 761.0, loss: 952.2208833694458\n",
      "step: 762.0, loss: 952.2159194946289\n",
      "step: 763.0, loss: 951.3760719299316\n",
      "step: 764.0, loss: 951.6680374145508\n",
      "step: 765.0, loss: 951.3266696929932\n",
      "step: 766.0, loss: 952.4685392379761\n",
      "step: 767.0, loss: 952.3058795928955\n",
      "step: 768.0, loss: 952.8489818572998\n",
      "step: 769.0, loss: 950.5961771011353\n",
      "step: 770.0, loss: 951.9735488891602\n",
      "step: 771.0, loss: 951.859450340271\n",
      "step: 772.0, loss: 951.571460723877\n",
      "step: 773.0, loss: 951.9227418899536\n",
      "step: 774.0, loss: 952.5942649841309\n",
      "step: 775.0, loss: 952.5185279846191\n",
      "step: 776.0, loss: 951.3200569152832\n",
      "step: 777.0, loss: 950.1214408874512\n",
      "step: 778.0, loss: 950.5471925735474\n",
      "step: 779.0, loss: 952.2305898666382\n",
      "step: 780.0, loss: 950.9550352096558\n",
      "step: 781.0, loss: 951.4672174453735\n",
      "step: 782.0, loss: 952.4595766067505\n",
      "step: 783.0, loss: 951.4073181152344\n",
      "step: 784.0, loss: 952.2632217407227\n",
      "step: 785.0, loss: 951.0584402084351\n",
      "step: 786.0, loss: 950.6746444702148\n",
      "step: 787.0, loss: 950.5841398239136\n",
      "step: 788.0, loss: 951.0806016921997\n",
      "step: 789.0, loss: 950.6270875930786\n",
      "step: 790.0, loss: 950.247480392456\n",
      "step: 791.0, loss: 951.49684715271\n",
      "step: 792.0, loss: 951.8613710403442\n",
      "step: 793.0, loss: 951.129373550415\n",
      "step: 794.0, loss: 952.2234907150269\n",
      "step: 795.0, loss: 950.3854446411133\n",
      "step: 796.0, loss: 952.4912919998169\n",
      "step: 797.0, loss: 951.9993257522583\n",
      "step: 798.0, loss: 950.8433713912964\n",
      "step: 799.0, loss: 950.3709449768066\n",
      "step: 800.0, loss: 952.0549077987671\n",
      "step: 801.0, loss: 953.6675052642822\n",
      "step: 802.0, loss: 949.8315382003784\n",
      "step: 803.0, loss: 950.9882287979126\n",
      "step: 804.0, loss: 948.9917402267456\n",
      "step: 805.0, loss: 951.4009943008423\n",
      "step: 806.0, loss: 952.1193675994873\n",
      "step: 807.0, loss: 951.5406913757324\n",
      "step: 808.0, loss: 950.5818290710449\n",
      "step: 809.0, loss: 949.861102104187\n",
      "step: 810.0, loss: 950.300238609314\n",
      "step: 811.0, loss: 950.311619758606\n",
      "step: 812.0, loss: 949.9801511764526\n",
      "step: 813.0, loss: 950.4302463531494\n",
      "step: 814.0, loss: 949.7310237884521\n",
      "step: 815.0, loss: 949.9860200881958\n",
      "step: 816.0, loss: 951.4578266143799\n",
      "step: 817.0, loss: 950.3816394805908\n",
      "step: 818.0, loss: 951.6252393722534\n",
      "step: 819.0, loss: 951.2476263046265\n",
      "step: 820.0, loss: 951.616865158081\n",
      "step: 821.0, loss: 951.6380462646484\n",
      "step: 822.0, loss: 949.8674488067627\n",
      "step: 823.0, loss: 953.3610410690308\n",
      "step: 824.0, loss: 950.3233194351196\n",
      "step: 825.0, loss: 951.5734052658081\n",
      "step: 826.0, loss: 949.2933855056763\n",
      "step: 827.0, loss: 950.9182558059692\n",
      "step: 828.0, loss: 951.7982015609741\n",
      "step: 829.0, loss: 951.2614641189575\n",
      "step: 830.0, loss: 951.0907354354858\n",
      "step: 831.0, loss: 950.9136390686035\n",
      "step: 832.0, loss: 949.6426858901978\n",
      "step: 833.0, loss: 949.3563051223755\n",
      "step: 834.0, loss: 948.2928123474121\n",
      "step: 835.0, loss: 950.5669641494751\n",
      "step: 836.0, loss: 950.3258829116821\n",
      "step: 837.0, loss: 950.1057395935059\n",
      "step: 838.0, loss: 949.9333648681641\n",
      "step: 839.0, loss: 949.9984760284424\n",
      "step: 840.0, loss: 949.3881483078003\n",
      "step: 841.0, loss: 951.7245283126831\n",
      "step: 842.0, loss: 950.2299432754517\n",
      "step: 843.0, loss: 949.9702081680298\n",
      "step: 844.0, loss: 951.1976099014282\n",
      "step: 845.0, loss: 950.3599624633789\n",
      "step: 846.0, loss: 950.4871377944946\n",
      "step: 847.0, loss: 950.0190286636353\n",
      "step: 848.0, loss: 949.3315658569336\n",
      "step: 849.0, loss: 948.1055889129639\n",
      "step: 850.0, loss: 951.4523763656616\n",
      "step: 851.0, loss: 950.0524806976318\n",
      "step: 852.0, loss: 950.3865728378296\n",
      "step: 853.0, loss: 950.3502359390259\n",
      "step: 854.0, loss: 950.0392913818359\n",
      "step: 855.0, loss: 949.8394708633423\n",
      "step: 856.0, loss: 950.3182258605957\n",
      "step: 857.0, loss: 950.7481727600098\n",
      "step: 858.0, loss: 949.8526182174683\n",
      "step: 859.0, loss: 949.5801315307617\n",
      "step: 860.0, loss: 950.8916158676147\n",
      "step: 861.0, loss: 949.9682741165161\n",
      "step: 862.0, loss: 950.1401329040527\n",
      "step: 863.0, loss: 950.6013469696045\n",
      "step: 864.0, loss: 948.6727991104126\n",
      "step: 865.0, loss: 950.4231386184692\n",
      "step: 866.0, loss: 950.0152788162231\n",
      "step: 867.0, loss: 949.4812088012695\n",
      "step: 868.0, loss: 950.4282312393188\n",
      "step: 869.0, loss: 949.8571338653564\n",
      "step: 870.0, loss: 951.2359800338745\n",
      "step: 871.0, loss: 950.1857452392578\n",
      "step: 872.0, loss: 950.6904754638672\n",
      "step: 873.0, loss: 949.8589735031128\n",
      "step: 874.0, loss: 949.7555990219116\n",
      "step: 875.0, loss: 950.4366540908813\n",
      "step: 876.0, loss: 950.9421072006226\n",
      "step: 877.0, loss: 950.9011516571045\n",
      "step: 878.0, loss: 950.6436948776245\n",
      "step: 879.0, loss: 949.5256032943726\n",
      "step: 880.0, loss: 948.5004301071167\n",
      "step: 881.0, loss: 948.3275022506714\n",
      "step: 882.0, loss: 948.5511999130249\n",
      "step: 883.0, loss: 948.8661012649536\n",
      "step: 884.0, loss: 949.1844263076782\n",
      "step: 885.0, loss: 949.5643577575684\n",
      "step: 886.0, loss: 949.619029045105\n",
      "step: 887.0, loss: 948.4260349273682\n",
      "step: 888.0, loss: 950.6140995025635\n",
      "step: 889.0, loss: 949.3958053588867\n",
      "step: 890.0, loss: 949.7448434829712\n",
      "step: 891.0, loss: 950.3509035110474\n",
      "step: 892.0, loss: 949.3244380950928\n",
      "step: 893.0, loss: 949.7001447677612\n",
      "step: 894.0, loss: 948.6969633102417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 895.0, loss: 948.9782810211182\n",
      "step: 896.0, loss: 949.3774757385254\n",
      "step: 897.0, loss: 949.0826120376587\n",
      "step: 898.0, loss: 949.0274076461792\n",
      "step: 899.0, loss: 949.6390171051025\n",
      "step: 900.0, loss: 949.3440885543823\n",
      "step: 901.0, loss: 949.0588464736938\n",
      "step: 902.0, loss: 949.6975650787354\n",
      "step: 903.0, loss: 947.9730939865112\n",
      "step: 904.0, loss: 949.8659744262695\n",
      "step: 905.0, loss: 950.133282661438\n",
      "step: 906.0, loss: 949.3290948867798\n",
      "step: 907.0, loss: 949.088963508606\n",
      "step: 908.0, loss: 948.5068368911743\n",
      "step: 909.0, loss: 949.4069423675537\n",
      "step: 910.0, loss: 947.788200378418\n",
      "step: 911.0, loss: 949.1089220046997\n",
      "step: 912.0, loss: 948.8480815887451\n",
      "step: 913.0, loss: 948.3013257980347\n",
      "step: 914.0, loss: 950.5277729034424\n",
      "step: 915.0, loss: 948.2202377319336\n",
      "step: 916.0, loss: 949.2486515045166\n",
      "step: 917.0, loss: 949.2757930755615\n",
      "step: 918.0, loss: 948.1622190475464\n",
      "step: 919.0, loss: 947.757342338562\n",
      "step: 920.0, loss: 948.1527719497681\n",
      "step: 921.0, loss: 948.3944463729858\n",
      "step: 922.0, loss: 948.8794355392456\n",
      "step: 923.0, loss: 950.2999715805054\n",
      "step: 924.0, loss: 947.0477333068848\n",
      "step: 925.0, loss: 948.5874862670898\n",
      "step: 926.0, loss: 948.6503572463989\n",
      "step: 927.0, loss: 947.7672271728516\n",
      "step: 928.0, loss: 948.8311233520508\n",
      "step: 929.0, loss: 950.0910148620605\n",
      "step: 930.0, loss: 949.6152048110962\n",
      "step: 931.0, loss: 948.3330354690552\n",
      "step: 932.0, loss: 948.0645666122437\n",
      "step: 933.0, loss: 948.3360786437988\n",
      "step: 934.0, loss: 947.2132577896118\n",
      "step: 935.0, loss: 947.8766555786133\n",
      "step: 936.0, loss: 949.9582386016846\n",
      "step: 937.0, loss: 949.820164680481\n",
      "step: 938.0, loss: 949.4065103530884\n",
      "step: 939.0, loss: 948.4445180892944\n",
      "step: 940.0, loss: 947.8534345626831\n",
      "step: 941.0, loss: 946.2717418670654\n",
      "step: 942.0, loss: 948.4118633270264\n",
      "step: 943.0, loss: 947.1090803146362\n",
      "step: 944.0, loss: 949.535870552063\n",
      "step: 945.0, loss: 948.0938882827759\n",
      "step: 946.0, loss: 949.0498580932617\n",
      "step: 947.0, loss: 948.9042682647705\n",
      "step: 948.0, loss: 950.7824354171753\n",
      "step: 949.0, loss: 947.7416257858276\n",
      "step: 950.0, loss: 948.1124935150146\n",
      "step: 951.0, loss: 950.1367654800415\n",
      "step: 952.0, loss: 947.6891193389893\n",
      "step: 953.0, loss: 947.1420392990112\n",
      "step: 954.0, loss: 948.3072595596313\n",
      "step: 955.0, loss: 948.0286531448364\n",
      "step: 956.0, loss: 950.6174011230469\n",
      "step: 957.0, loss: 948.8157300949097\n",
      "step: 958.0, loss: 949.243447303772\n",
      "step: 959.0, loss: 946.4658737182617\n",
      "step: 960.0, loss: 948.3693103790283\n",
      "step: 961.0, loss: 949.4146156311035\n",
      "step: 962.0, loss: 949.5300798416138\n",
      "step: 963.0, loss: 948.3502941131592\n",
      "step: 964.0, loss: 946.7729940414429\n",
      "step: 965.0, loss: 947.4291486740112\n",
      "step: 966.0, loss: 948.9059114456177\n",
      "step: 967.0, loss: 948.885383605957\n",
      "step: 968.0, loss: 948.6331157684326\n",
      "step: 969.0, loss: 946.9603223800659\n",
      "step: 970.0, loss: 946.5022802352905\n",
      "step: 971.0, loss: 949.0767993927002\n",
      "step: 972.0, loss: 947.193187713623\n",
      "step: 973.0, loss: 948.8155889511108\n",
      "step: 974.0, loss: 949.1213521957397\n",
      "step: 975.0, loss: 948.3491449356079\n",
      "step: 976.0, loss: 949.0556211471558\n",
      "step: 977.0, loss: 947.5279150009155\n",
      "step: 978.0, loss: 948.6883478164673\n",
      "step: 979.0, loss: 948.1645650863647\n",
      "step: 980.0, loss: 948.9927196502686\n",
      "step: 981.0, loss: 947.303505897522\n",
      "step: 982.0, loss: 947.7096815109253\n",
      "step: 983.0, loss: 947.9074039459229\n",
      "step: 984.0, loss: 949.0030221939087\n",
      "step: 985.0, loss: 948.2754926681519\n",
      "step: 986.0, loss: 948.2311496734619\n",
      "step: 987.0, loss: 948.2397165298462\n",
      "step: 988.0, loss: 947.0879964828491\n",
      "step: 989.0, loss: 948.7930717468262\n",
      "step: 990.0, loss: 947.9092216491699\n",
      "step: 991.0, loss: 947.561466217041\n",
      "step: 992.0, loss: 949.6768827438354\n",
      "step: 993.0, loss: 947.9818019866943\n",
      "step: 994.0, loss: 948.4073314666748\n",
      "step: 995.0, loss: 948.7749443054199\n",
      "step: 996.0, loss: 949.3862438201904\n",
      "step: 997.0, loss: 949.6684770584106\n",
      "step: 998.0, loss: 948.8187761306763\n",
      "step: 999.0, loss: 948.0913143157959\n",
      "step: 1000.0, loss: 947.273588180542\n",
      "step: 1001.0, loss: 947.9106750488281\n",
      "step: 1002.0, loss: 946.4392642974854\n",
      "step: 1003.0, loss: 948.4647741317749\n",
      "step: 1004.0, loss: 948.0013666152954\n",
      "step: 1005.0, loss: 946.4241313934326\n",
      "step: 1006.0, loss: 949.5391263961792\n",
      "step: 1007.0, loss: 948.0099191665649\n",
      "step: 1008.0, loss: 948.6483678817749\n",
      "step: 1009.0, loss: 947.0064249038696\n",
      "step: 1010.0, loss: 946.5578718185425\n",
      "step: 1011.0, loss: 945.865029335022\n",
      "step: 1012.0, loss: 948.3716974258423\n",
      "step: 1013.0, loss: 949.22083568573\n",
      "step: 1014.0, loss: 947.7729434967041\n",
      "step: 1015.0, loss: 946.4591035842896\n",
      "step: 1016.0, loss: 948.5139598846436\n",
      "step: 1017.0, loss: 946.1564950942993\n",
      "step: 1018.0, loss: 946.6976518630981\n",
      "step: 1019.0, loss: 948.8058223724365\n",
      "step: 1020.0, loss: 947.8283290863037\n",
      "step: 1021.0, loss: 946.3972253799438\n",
      "step: 1022.0, loss: 946.1978387832642\n",
      "step: 1023.0, loss: 947.459939956665\n",
      "step: 1024.0, loss: 948.4317760467529\n",
      "step: 1025.0, loss: 948.3205375671387\n",
      "step: 1026.0, loss: 948.1206407546997\n",
      "step: 1027.0, loss: 948.1476917266846\n",
      "step: 1028.0, loss: 947.0740947723389\n",
      "step: 1029.0, loss: 947.7718696594238\n",
      "step: 1030.0, loss: 948.2545642852783\n",
      "step: 1031.0, loss: 946.9393377304077\n",
      "step: 1032.0, loss: 947.0333633422852\n",
      "step: 1033.0, loss: 947.9408102035522\n",
      "step: 1034.0, loss: 947.9708824157715\n",
      "step: 1035.0, loss: 946.8058481216431\n",
      "step: 1036.0, loss: 947.700722694397\n",
      "step: 1037.0, loss: 947.8410778045654\n",
      "step: 1038.0, loss: 948.9640703201294\n",
      "step: 1039.0, loss: 947.385181427002\n",
      "step: 1040.0, loss: 947.0847301483154\n",
      "step: 1041.0, loss: 947.9230871200562\n",
      "step: 1042.0, loss: 946.7252187728882\n",
      "step: 1043.0, loss: 948.431248664856\n",
      "step: 1044.0, loss: 948.4958667755127\n",
      "step: 1045.0, loss: 948.141116142273\n",
      "step: 1046.0, loss: 946.3948154449463\n",
      "step: 1047.0, loss: 947.8015270233154\n",
      "step: 1048.0, loss: 947.1406373977661\n",
      "step: 1049.0, loss: 947.0236558914185\n",
      "step: 1050.0, loss: 948.4209852218628\n",
      "step: 1051.0, loss: 947.1899213790894\n",
      "step: 1052.0, loss: 946.9052486419678\n",
      "step: 1053.0, loss: 947.139612197876\n",
      "step: 1054.0, loss: 947.2505445480347\n",
      "step: 1055.0, loss: 946.5945672988892\n",
      "step: 1056.0, loss: 946.7120323181152\n",
      "step: 1057.0, loss: 947.4348955154419\n",
      "step: 1058.0, loss: 947.310227394104\n",
      "step: 1059.0, loss: 946.8149824142456\n",
      "step: 1060.0, loss: 948.0741863250732\n",
      "step: 1061.0, loss: 947.6212358474731\n",
      "step: 1062.0, loss: 947.059476852417\n",
      "step: 1063.0, loss: 946.8129072189331\n",
      "step: 1064.0, loss: 947.6137838363647\n",
      "step: 1065.0, loss: 946.8370780944824\n",
      "step: 1066.0, loss: 947.0996961593628\n",
      "step: 1067.0, loss: 947.3691730499268\n",
      "step: 1068.0, loss: 947.8257150650024\n",
      "step: 1069.0, loss: 946.3959655761719\n",
      "step: 1070.0, loss: 947.9396982192993\n",
      "step: 1071.0, loss: 946.4125709533691\n",
      "step: 1072.0, loss: 946.7094049453735\n",
      "step: 1073.0, loss: 946.8506574630737\n",
      "step: 1074.0, loss: 947.8793468475342\n",
      "step: 1075.0, loss: 947.5889377593994\n",
      "step: 1076.0, loss: 945.769211769104\n",
      "step: 1077.0, loss: 946.4610767364502\n",
      "step: 1078.0, loss: 947.2494411468506\n",
      "step: 1079.0, loss: 947.7952871322632\n",
      "step: 1080.0, loss: 946.6956033706665\n",
      "step: 1081.0, loss: 946.7073364257812\n",
      "step: 1082.0, loss: 946.2584085464478\n",
      "step: 1083.0, loss: 946.7666025161743\n",
      "step: 1084.0, loss: 946.570369720459\n",
      "step: 1085.0, loss: 946.7655620574951\n",
      "step: 1086.0, loss: 946.0749998092651\n",
      "step: 1087.0, loss: 947.2499990463257\n",
      "step: 1088.0, loss: 946.9407758712769\n",
      "step: 1089.0, loss: 947.9421663284302\n",
      "step: 1090.0, loss: 946.9399166107178\n",
      "step: 1091.0, loss: 946.1839122772217\n",
      "step: 1092.0, loss: 945.503475189209\n",
      "step: 1093.0, loss: 945.0628118515015\n",
      "step: 1094.0, loss: 945.4635906219482\n",
      "step: 1095.0, loss: 945.6880569458008\n",
      "step: 1096.0, loss: 946.7857027053833\n",
      "step: 1097.0, loss: 946.1475257873535\n",
      "step: 1098.0, loss: 946.9169063568115\n",
      "step: 1099.0, loss: 947.1076898574829\n",
      "step: 1100.0, loss: 944.9666395187378\n",
      "step: 1101.0, loss: 947.8869609832764\n",
      "step: 1102.0, loss: 948.2283048629761\n",
      "step: 1103.0, loss: 946.2257843017578\n",
      "step: 1104.0, loss: 946.4753141403198\n",
      "step: 1105.0, loss: 945.9506874084473\n",
      "step: 1106.0, loss: 944.9824705123901\n",
      "step: 1107.0, loss: 947.8452043533325\n",
      "step: 1108.0, loss: 945.526083946228\n",
      "step: 1109.0, loss: 947.6943597793579\n",
      "step: 1110.0, loss: 946.717098236084\n",
      "step: 1111.0, loss: 945.6518363952637\n",
      "step: 1112.0, loss: 946.7509765625\n",
      "step: 1113.0, loss: 947.3591756820679\n",
      "step: 1114.0, loss: 946.2540578842163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1115.0, loss: 946.3772783279419\n",
      "step: 1116.0, loss: 945.6233987808228\n",
      "step: 1117.0, loss: 945.9465351104736\n",
      "step: 1118.0, loss: 945.0832185745239\n",
      "step: 1119.0, loss: 947.3339710235596\n",
      "step: 1120.0, loss: 945.8384923934937\n",
      "step: 1121.0, loss: 945.7051830291748\n",
      "step: 1122.0, loss: 948.03249168396\n",
      "step: 1123.0, loss: 944.1587791442871\n",
      "step: 1124.0, loss: 945.9944677352905\n",
      "step: 1125.0, loss: 945.6400232315063\n",
      "step: 1126.0, loss: 946.303032875061\n",
      "step: 1127.0, loss: 947.5517997741699\n",
      "step: 1128.0, loss: 945.0510091781616\n",
      "step: 1129.0, loss: 946.7101202011108\n",
      "step: 1130.0, loss: 945.2761220932007\n",
      "step: 1131.0, loss: 946.5724840164185\n",
      "step: 1132.0, loss: 944.3688020706177\n",
      "step: 1133.0, loss: 946.4098825454712\n",
      "step: 1134.0, loss: 944.7318115234375\n",
      "step: 1135.0, loss: 946.1574230194092\n",
      "step: 1136.0, loss: 945.2323188781738\n",
      "step: 1137.0, loss: 945.6682081222534\n",
      "step: 1138.0, loss: 945.1878461837769\n",
      "step: 1139.0, loss: 945.8007802963257\n",
      "step: 1140.0, loss: 946.5837640762329\n",
      "step: 1141.0, loss: 945.9231271743774\n",
      "step: 1142.0, loss: 945.6947479248047\n",
      "step: 1143.0, loss: 943.9361410140991\n",
      "step: 1144.0, loss: 947.1999578475952\n",
      "step: 1145.0, loss: 945.2588386535645\n",
      "step: 1146.0, loss: 947.3725786209106\n",
      "step: 1147.0, loss: 943.5567207336426\n",
      "step: 1148.0, loss: 943.4235038757324\n",
      "step: 1149.0, loss: 945.6806812286377\n",
      "step: 1150.0, loss: 946.6363849639893\n",
      "step: 1151.0, loss: 945.1004695892334\n",
      "step: 1152.0, loss: 945.8891105651855\n",
      "step: 1153.0, loss: 947.0428972244263\n",
      "step: 1154.0, loss: 945.2302379608154\n",
      "step: 1155.0, loss: 945.5863208770752\n",
      "step: 1156.0, loss: 944.6248540878296\n",
      "step: 1157.0, loss: 943.9437398910522\n",
      "step: 1158.0, loss: 944.42005443573\n",
      "step: 1159.0, loss: 945.7471990585327\n",
      "step: 1160.0, loss: 944.0690660476685\n",
      "step: 1161.0, loss: 946.2732095718384\n",
      "step: 1162.0, loss: 945.4819202423096\n",
      "step: 1163.0, loss: 945.6787824630737\n",
      "step: 1164.0, loss: 946.6014957427979\n",
      "step: 1165.0, loss: 948.3707523345947\n",
      "step: 1166.0, loss: 945.079080581665\n",
      "step: 1167.0, loss: 945.1990280151367\n",
      "step: 1168.0, loss: 945.6161975860596\n",
      "step: 1169.0, loss: 944.7358379364014\n",
      "step: 1170.0, loss: 945.5958414077759\n",
      "step: 1171.0, loss: 943.7052907943726\n",
      "step: 1172.0, loss: 945.3849439620972\n",
      "step: 1173.0, loss: 944.5969104766846\n",
      "step: 1174.0, loss: 946.4016618728638\n",
      "step: 1175.0, loss: 945.3678178787231\n",
      "step: 1176.0, loss: 945.5653305053711\n",
      "step: 1177.0, loss: 946.0797681808472\n",
      "step: 1178.0, loss: 946.0608167648315\n",
      "step: 1179.0, loss: 945.0364255905151\n",
      "step: 1180.0, loss: 946.5701293945312\n",
      "step: 1181.0, loss: 945.1325197219849\n",
      "step: 1182.0, loss: 945.3898458480835\n",
      "step: 1183.0, loss: 945.3853626251221\n",
      "step: 1184.0, loss: 946.2785196304321\n",
      "step: 1185.0, loss: 945.6211576461792\n",
      "step: 1186.0, loss: 944.6279315948486\n",
      "step: 1187.0, loss: 945.2459211349487\n",
      "step: 1188.0, loss: 945.8293628692627\n",
      "step: 1189.0, loss: 944.6974096298218\n",
      "step: 1190.0, loss: 944.6148643493652\n",
      "step: 1191.0, loss: 945.3928117752075\n",
      "step: 1192.0, loss: 946.2483062744141\n",
      "step: 1193.0, loss: 946.1001358032227\n",
      "step: 1194.0, loss: 945.2223825454712\n",
      "step: 1195.0, loss: 946.4395427703857\n",
      "step: 1196.0, loss: 945.6819152832031\n",
      "step: 1197.0, loss: 945.7467813491821\n",
      "step: 1198.0, loss: 945.0087575912476\n",
      "step: 1199.0, loss: 945.5064964294434\n",
      "step: 1200.0, loss: 945.4279642105103\n",
      "step: 1201.0, loss: 944.8870506286621\n",
      "step: 1202.0, loss: 945.8379564285278\n",
      "step: 1203.0, loss: 944.5271701812744\n",
      "step: 1204.0, loss: 944.147177696228\n",
      "step: 1205.0, loss: 944.4726228713989\n",
      "step: 1206.0, loss: 944.4695930480957\n",
      "step: 1207.0, loss: 944.7789239883423\n",
      "step: 1208.0, loss: 945.4384994506836\n",
      "step: 1209.0, loss: 943.1301765441895\n",
      "step: 1210.0, loss: 944.7704267501831\n",
      "step: 1211.0, loss: 944.7783031463623\n",
      "step: 1212.0, loss: 945.5217247009277\n",
      "step: 1213.0, loss: 945.0548114776611\n",
      "step: 1214.0, loss: 943.8443002700806\n",
      "step: 1215.0, loss: 945.6007232666016\n",
      "step: 1216.0, loss: 943.7450733184814\n",
      "step: 1217.0, loss: 943.9991579055786\n",
      "step: 1218.0, loss: 944.6782855987549\n",
      "step: 1219.0, loss: 944.0625534057617\n",
      "step: 1220.0, loss: 946.5360927581787\n",
      "step: 1221.0, loss: 945.7383108139038\n",
      "step: 1222.0, loss: 944.7701625823975\n",
      "step: 1223.0, loss: 944.1981010437012\n",
      "step: 1224.0, loss: 943.9495639801025\n",
      "step: 1225.0, loss: 944.3611421585083\n",
      "step: 1226.0, loss: 944.379319190979\n",
      "step: 1227.0, loss: 944.6163921356201\n",
      "step: 1228.0, loss: 944.9573831558228\n",
      "step: 1229.0, loss: 945.8950643539429\n",
      "step: 1230.0, loss: 943.4305458068848\n",
      "step: 1231.0, loss: 945.5379457473755\n",
      "step: 1232.0, loss: 944.3704147338867\n",
      "step: 1233.0, loss: 944.6249465942383\n",
      "step: 1234.0, loss: 943.8463077545166\n",
      "step: 1235.0, loss: 944.6483278274536\n",
      "step: 1236.0, loss: 943.453631401062\n",
      "step: 1237.0, loss: 945.2537298202515\n",
      "step: 1238.0, loss: 945.1928081512451\n",
      "step: 1239.0, loss: 945.2392997741699\n",
      "step: 1240.0, loss: 944.3437833786011\n",
      "step: 1241.0, loss: 944.3469486236572\n",
      "step: 1242.0, loss: 944.3365526199341\n",
      "step: 1243.0, loss: 947.0259408950806\n",
      "step: 1244.0, loss: 945.0413236618042\n",
      "step: 1245.0, loss: 944.7984628677368\n",
      "step: 1246.0, loss: 944.1329212188721\n",
      "step: 1247.0, loss: 945.1026048660278\n",
      "step: 1248.0, loss: 944.8963460922241\n",
      "step: 1249.0, loss: 942.8377475738525\n",
      "step: 1250.0, loss: 944.71497631073\n",
      "step: 1251.0, loss: 943.5464706420898\n",
      "step: 1252.0, loss: 944.8247137069702\n",
      "step: 1253.0, loss: 944.3310623168945\n",
      "step: 1254.0, loss: 944.3470249176025\n",
      "step: 1255.0, loss: 944.2781066894531\n",
      "step: 1256.0, loss: 945.1279811859131\n",
      "step: 1257.0, loss: 943.7573385238647\n",
      "step: 1258.0, loss: 945.317138671875\n",
      "step: 1259.0, loss: 943.0494031906128\n",
      "step: 1260.0, loss: 942.242919921875\n",
      "step: 1261.0, loss: 943.5525102615356\n",
      "step: 1262.0, loss: 944.1283912658691\n",
      "step: 1263.0, loss: 944.8050127029419\n",
      "step: 1264.0, loss: 943.790693283081\n",
      "step: 1265.0, loss: 942.784257888794\n",
      "step: 1266.0, loss: 943.1119022369385\n",
      "step: 1267.0, loss: 943.873236656189\n",
      "step: 1268.0, loss: 945.0755100250244\n",
      "step: 1269.0, loss: 944.4644193649292\n",
      "step: 1270.0, loss: 943.495943069458\n",
      "step: 1271.0, loss: 944.683515548706\n",
      "step: 1272.0, loss: 943.9577045440674\n",
      "step: 1273.0, loss: 944.0399503707886\n",
      "step: 1274.0, loss: 942.674560546875\n",
      "step: 1275.0, loss: 944.031171798706\n",
      "step: 1276.0, loss: 944.6560106277466\n",
      "step: 1277.0, loss: 943.8852958679199\n",
      "step: 1278.0, loss: 945.0506982803345\n",
      "step: 1279.0, loss: 945.4657897949219\n",
      "step: 1280.0, loss: 941.6522016525269\n",
      "step: 1281.0, loss: 943.1585645675659\n",
      "step: 1282.0, loss: 942.7879552841187\n",
      "step: 1283.0, loss: 943.4787845611572\n",
      "step: 1284.0, loss: 944.1400375366211\n",
      "step: 1285.0, loss: 942.1979875564575\n",
      "step: 1286.0, loss: 943.8532285690308\n",
      "step: 1287.0, loss: 944.4745435714722\n",
      "step: 1288.0, loss: 944.9681930541992\n",
      "step: 1289.0, loss: 944.5213899612427\n",
      "step: 1290.0, loss: 944.5038290023804\n",
      "step: 1291.0, loss: 944.6634511947632\n",
      "step: 1292.0, loss: 944.6119346618652\n",
      "step: 1293.0, loss: 944.4691276550293\n",
      "step: 1294.0, loss: 945.0865983963013\n",
      "step: 1295.0, loss: 944.1526470184326\n",
      "step: 1296.0, loss: 944.3691902160645\n",
      "step: 1297.0, loss: 943.9955081939697\n",
      "step: 1298.0, loss: 942.7999868392944\n",
      "step: 1299.0, loss: 944.1333961486816\n",
      "step: 1300.0, loss: 945.0868883132935\n",
      "step: 1301.0, loss: 944.783444404602\n",
      "step: 1302.0, loss: 945.0548448562622\n",
      "step: 1303.0, loss: 942.8633241653442\n",
      "step: 1304.0, loss: 945.5820894241333\n",
      "step: 1305.0, loss: 944.3914480209351\n",
      "step: 1306.0, loss: 943.7075185775757\n",
      "step: 1307.0, loss: 944.0671043395996\n",
      "step: 1308.0, loss: 945.290186882019\n",
      "step: 1309.0, loss: 942.4866409301758\n",
      "step: 1310.0, loss: 944.0256423950195\n",
      "step: 1311.0, loss: 944.2267160415649\n",
      "step: 1312.0, loss: 943.3448524475098\n",
      "step: 1313.0, loss: 943.230770111084\n",
      "step: 1314.0, loss: 944.3189687728882\n",
      "step: 1315.0, loss: 943.7981424331665\n",
      "step: 1316.0, loss: 944.6998872756958\n",
      "step: 1317.0, loss: 942.6502904891968\n",
      "step: 1318.0, loss: 943.6889877319336\n",
      "step: 1319.0, loss: 943.4374284744263\n",
      "step: 1320.0, loss: 944.688777923584\n",
      "step: 1321.0, loss: 943.1233987808228\n",
      "step: 1322.0, loss: 943.6771936416626\n",
      "step: 1323.0, loss: 943.5779914855957\n",
      "step: 1324.0, loss: 945.1921844482422\n",
      "step: 1325.0, loss: 943.4728746414185\n",
      "step: 1326.0, loss: 945.610969543457\n",
      "step: 1327.0, loss: 943.2945432662964\n",
      "step: 1328.0, loss: 943.9125461578369\n",
      "step: 1329.0, loss: 944.7983856201172\n",
      "step: 1330.0, loss: 944.5888624191284\n",
      "step: 1331.0, loss: 944.2087249755859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1332.0, loss: 943.599573135376\n",
      "step: 1333.0, loss: 942.437762260437\n",
      "step: 1334.0, loss: 944.3764896392822\n",
      "step: 1335.0, loss: 943.8201065063477\n",
      "step: 1336.0, loss: 943.2728729248047\n",
      "step: 1337.0, loss: 943.8827142715454\n",
      "step: 1338.0, loss: 942.3527698516846\n",
      "step: 1339.0, loss: 942.5061616897583\n",
      "step: 1340.0, loss: 945.0101737976074\n",
      "step: 1341.0, loss: 943.0335235595703\n",
      "step: 1342.0, loss: 941.3506469726562\n",
      "step: 1343.0, loss: 943.8094329833984\n",
      "step: 1344.0, loss: 942.8995342254639\n",
      "step: 1345.0, loss: 942.6790323257446\n",
      "step: 1346.0, loss: 943.4656715393066\n",
      "step: 1347.0, loss: 944.1192207336426\n",
      "step: 1348.0, loss: 942.7970170974731\n",
      "step: 1349.0, loss: 943.3947048187256\n",
      "step: 1350.0, loss: 945.8371019363403\n",
      "step: 1351.0, loss: 944.2690267562866\n",
      "step: 1352.0, loss: 942.173565864563\n",
      "step: 1353.0, loss: 943.3468046188354\n",
      "step: 1354.0, loss: 943.511420249939\n",
      "step: 1355.0, loss: 942.428258895874\n",
      "step: 1356.0, loss: 942.8732433319092\n",
      "step: 1357.0, loss: 941.7286806106567\n",
      "step: 1358.0, loss: 943.5908279418945\n",
      "step: 1359.0, loss: 943.7762441635132\n",
      "step: 1360.0, loss: 943.3510522842407\n",
      "step: 1361.0, loss: 944.4417524337769\n",
      "step: 1362.0, loss: 943.8535919189453\n",
      "step: 1363.0, loss: 943.229868888855\n",
      "step: 1364.0, loss: 942.2647609710693\n",
      "step: 1365.0, loss: 942.9583034515381\n",
      "step: 1366.0, loss: 944.4604930877686\n",
      "step: 1367.0, loss: 942.721661567688\n",
      "step: 1368.0, loss: 942.0522241592407\n",
      "step: 1369.0, loss: 942.6815223693848\n",
      "step: 1370.0, loss: 943.3226861953735\n",
      "step: 1371.0, loss: 943.6855058670044\n",
      "step: 1372.0, loss: 941.656909942627\n",
      "step: 1373.0, loss: 942.4914379119873\n",
      "step: 1374.0, loss: 943.1477575302124\n",
      "step: 1375.0, loss: 941.4022617340088\n",
      "step: 1376.0, loss: 943.5582847595215\n",
      "step: 1377.0, loss: 942.6074514389038\n",
      "step: 1378.0, loss: 944.3379974365234\n",
      "step: 1379.0, loss: 945.3125200271606\n",
      "step: 1380.0, loss: 942.8396005630493\n",
      "step: 1381.0, loss: 942.2264947891235\n",
      "step: 1382.0, loss: 943.5308055877686\n",
      "step: 1383.0, loss: 943.2726173400879\n",
      "step: 1384.0, loss: 943.1717767715454\n",
      "step: 1385.0, loss: 942.2237319946289\n",
      "step: 1386.0, loss: 943.3012638092041\n",
      "step: 1387.0, loss: 941.9157114028931\n",
      "step: 1388.0, loss: 941.8361024856567\n",
      "step: 1389.0, loss: 942.1449728012085\n",
      "step: 1390.0, loss: 941.6408214569092\n",
      "step: 1391.0, loss: 942.5269956588745\n",
      "step: 1392.0, loss: 943.4139165878296\n",
      "step: 1393.0, loss: 941.8638925552368\n",
      "step: 1394.0, loss: 943.9432783126831\n",
      "step: 1395.0, loss: 941.3863153457642\n",
      "step: 1396.0, loss: 941.0648097991943\n",
      "step: 1397.0, loss: 943.366005897522\n",
      "step: 1398.0, loss: 941.7806262969971\n",
      "step: 1399.0, loss: 942.6875877380371\n",
      "step: 1400.0, loss: 942.7055892944336\n",
      "step: 1401.0, loss: 942.357479095459\n",
      "step: 1402.0, loss: 941.1659507751465\n",
      "step: 1403.0, loss: 942.3540372848511\n",
      "step: 1404.0, loss: 942.8810863494873\n",
      "step: 1405.0, loss: 942.7815427780151\n",
      "step: 1406.0, loss: 940.9469919204712\n",
      "step: 1407.0, loss: 941.4158525466919\n",
      "step: 1408.0, loss: 943.7759113311768\n",
      "step: 1409.0, loss: 941.6584358215332\n",
      "step: 1410.0, loss: 942.2408475875854\n",
      "step: 1411.0, loss: 943.481164932251\n",
      "step: 1412.0, loss: 941.6272792816162\n",
      "step: 1413.0, loss: 943.2128353118896\n",
      "step: 1414.0, loss: 942.8305892944336\n",
      "step: 1415.0, loss: 943.3371429443359\n",
      "step: 1416.0, loss: 942.4565515518188\n",
      "step: 1417.0, loss: 944.8273801803589\n",
      "step: 1418.0, loss: 944.0866107940674\n",
      "step: 1419.0, loss: 943.1045970916748\n",
      "step: 1420.0, loss: 945.2409315109253\n",
      "step: 1421.0, loss: 941.6653156280518\n",
      "step: 1422.0, loss: 942.9158744812012\n",
      "step: 1423.0, loss: 941.7219676971436\n",
      "step: 1424.0, loss: 943.3426494598389\n",
      "step: 1425.0, loss: 943.317626953125\n",
      "step: 1426.0, loss: 944.0922966003418\n",
      "step: 1427.0, loss: 943.3798751831055\n",
      "step: 1428.0, loss: 941.9304332733154\n",
      "step: 1429.0, loss: 941.988169670105\n",
      "step: 1430.0, loss: 941.0632658004761\n",
      "step: 1431.0, loss: 942.1282739639282\n",
      "step: 1432.0, loss: 942.354531288147\n",
      "step: 1433.0, loss: 943.5892143249512\n",
      "step: 1434.0, loss: 943.1673431396484\n",
      "step: 1435.0, loss: 943.5430526733398\n",
      "step: 1436.0, loss: 941.6092329025269\n",
      "step: 1437.0, loss: 943.2399520874023\n",
      "step: 1438.0, loss: 942.3346519470215\n",
      "step: 1439.0, loss: 942.4973526000977\n",
      "step: 1440.0, loss: 944.9840116500854\n",
      "step: 1441.0, loss: 942.4928340911865\n",
      "step: 1442.0, loss: 943.431643486023\n",
      "step: 1443.0, loss: 943.3891792297363\n",
      "step: 1444.0, loss: 941.239200592041\n",
      "step: 1445.0, loss: 942.0970573425293\n",
      "step: 1446.0, loss: 939.4169673919678\n",
      "step: 1447.0, loss: 941.0797624588013\n",
      "step: 1448.0, loss: 940.5030059814453\n",
      "step: 1449.0, loss: 943.0042543411255\n",
      "step: 1450.0, loss: 942.3947649002075\n",
      "step: 1451.0, loss: 942.0985116958618\n",
      "step: 1452.0, loss: 940.1623849868774\n",
      "step: 1453.0, loss: 942.0729789733887\n",
      "step: 1454.0, loss: 942.3485946655273\n",
      "step: 1455.0, loss: 943.6100206375122\n",
      "step: 1456.0, loss: 941.5589361190796\n",
      "step: 1457.0, loss: 942.4251117706299\n",
      "step: 1458.0, loss: 941.3227777481079\n",
      "step: 1459.0, loss: 942.5874719619751\n",
      "step: 1460.0, loss: 944.2182912826538\n",
      "step: 1461.0, loss: 943.6381206512451\n",
      "step: 1462.0, loss: 942.4335079193115\n",
      "step: 1463.0, loss: 941.2850160598755\n",
      "step: 1464.0, loss: 943.0577545166016\n",
      "step: 1465.0, loss: 944.1323986053467\n",
      "step: 1466.0, loss: 941.2419548034668\n",
      "step: 1467.0, loss: 944.2855587005615\n",
      "step: 1468.0, loss: 942.4810962677002\n",
      "step: 1469.0, loss: 942.6811475753784\n",
      "step: 1470.0, loss: 942.6198539733887\n",
      "step: 1471.0, loss: 942.1833658218384\n",
      "step: 1472.0, loss: 942.4072542190552\n",
      "step: 1473.0, loss: 943.7386207580566\n",
      "step: 1474.0, loss: 941.4761924743652\n",
      "step: 1475.0, loss: 943.215579032898\n",
      "step: 1476.0, loss: 941.2045373916626\n",
      "step: 1477.0, loss: 941.3764419555664\n",
      "step: 1478.0, loss: 941.3697566986084\n",
      "step: 1479.0, loss: 941.5914716720581\n",
      "step: 1480.0, loss: 941.200611114502\n",
      "step: 1481.0, loss: 942.3572835922241\n",
      "step: 1482.0, loss: 942.8764247894287\n",
      "step: 1483.0, loss: 940.8008289337158\n",
      "step: 1484.0, loss: 942.3814039230347\n",
      "step: 1485.0, loss: 939.0341758728027\n",
      "step: 1486.0, loss: 941.9558038711548\n",
      "step: 1487.0, loss: 940.9789867401123\n",
      "step: 1488.0, loss: 942.346866607666\n",
      "step: 1489.0, loss: 942.071681022644\n",
      "step: 1490.0, loss: 942.4540042877197\n",
      "step: 1491.0, loss: 942.751745223999\n",
      "step: 1492.0, loss: 942.2335376739502\n",
      "step: 1493.0, loss: 942.629864692688\n",
      "step: 1494.0, loss: 940.9755926132202\n",
      "step: 1495.0, loss: 942.0322608947754\n",
      "step: 1496.0, loss: 942.9496021270752\n",
      "step: 1497.0, loss: 941.838131904602\n",
      "step: 1498.0, loss: 940.7983179092407\n",
      "step: 1499.0, loss: 941.2701721191406\n",
      "step: 1500.0, loss: 941.8304033279419\n",
      "step: 1501.0, loss: 941.5199184417725\n",
      "step: 1502.0, loss: 943.0575494766235\n",
      "step: 1503.0, loss: 941.1522741317749\n",
      "step: 1504.0, loss: 941.3786811828613\n",
      "step: 1505.0, loss: 942.4765281677246\n",
      "step: 1506.0, loss: 944.0400562286377\n",
      "step: 1507.0, loss: 941.6027126312256\n",
      "step: 1508.0, loss: 940.5191679000854\n",
      "step: 1509.0, loss: 943.5137176513672\n",
      "step: 1510.0, loss: 940.690110206604\n",
      "step: 1511.0, loss: 942.1587285995483\n",
      "step: 1512.0, loss: 942.401517868042\n",
      "step: 1513.0, loss: 942.8269510269165\n",
      "step: 1514.0, loss: 941.5499086380005\n",
      "step: 1515.0, loss: 941.0778093338013\n",
      "step: 1516.0, loss: 943.4587383270264\n",
      "step: 1517.0, loss: 941.8253793716431\n",
      "step: 1518.0, loss: 942.2325048446655\n",
      "step: 1519.0, loss: 942.3123302459717\n",
      "step: 1520.0, loss: 942.2549819946289\n",
      "step: 1521.0, loss: 941.5372552871704\n",
      "step: 1522.0, loss: 941.9942960739136\n",
      "step: 1523.0, loss: 941.3648452758789\n",
      "step: 1524.0, loss: 941.1600341796875\n",
      "step: 1525.0, loss: 941.1068801879883\n",
      "step: 1526.0, loss: 942.4256448745728\n",
      "step: 1527.0, loss: 942.5353994369507\n",
      "step: 1528.0, loss: 942.373456954956\n",
      "step: 1529.0, loss: 940.0057210922241\n",
      "step: 1530.0, loss: 941.576621055603\n",
      "step: 1531.0, loss: 941.0654630661011\n",
      "step: 1532.0, loss: 940.8576374053955\n",
      "step: 1533.0, loss: 941.0416917800903\n",
      "step: 1534.0, loss: 942.5195531845093\n",
      "step: 1535.0, loss: 939.9612741470337\n",
      "step: 1536.0, loss: 940.793981552124\n",
      "step: 1537.0, loss: 942.0302019119263\n",
      "step: 1538.0, loss: 942.8464736938477\n",
      "step: 1539.0, loss: 941.8388614654541\n",
      "step: 1540.0, loss: 941.1740627288818\n",
      "step: 1541.0, loss: 940.3901128768921\n",
      "step: 1542.0, loss: 942.0703439712524\n",
      "step: 1543.0, loss: 941.0500936508179\n",
      "step: 1544.0, loss: 941.2342205047607\n",
      "step: 1545.0, loss: 941.8435678482056\n",
      "step: 1546.0, loss: 939.883189201355\n",
      "step: 1547.0, loss: 941.1652126312256\n",
      "step: 1548.0, loss: 940.9064931869507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1549.0, loss: 941.8350839614868\n",
      "step: 1550.0, loss: 941.3098735809326\n",
      "step: 1551.0, loss: 940.552264213562\n",
      "step: 1552.0, loss: 941.0564737319946\n",
      "step: 1553.0, loss: 941.0236959457397\n",
      "step: 1554.0, loss: 940.6225471496582\n",
      "step: 1555.0, loss: 942.2609148025513\n",
      "step: 1556.0, loss: 942.333930015564\n",
      "step: 1557.0, loss: 940.1365871429443\n",
      "step: 1558.0, loss: 939.6776151657104\n",
      "step: 1559.0, loss: 940.6400156021118\n",
      "step: 1560.0, loss: 941.3751544952393\n",
      "step: 1561.0, loss: 941.2048244476318\n",
      "step: 1562.0, loss: 941.1020030975342\n",
      "step: 1563.0, loss: 939.5781707763672\n",
      "step: 1564.0, loss: 941.3828134536743\n",
      "step: 1565.0, loss: 943.4130544662476\n",
      "step: 1566.0, loss: 941.8953485488892\n",
      "step: 1567.0, loss: 941.2317295074463\n",
      "step: 1568.0, loss: 940.0960626602173\n",
      "step: 1569.0, loss: 940.8871841430664\n",
      "step: 1570.0, loss: 941.4820909500122\n",
      "step: 1571.0, loss: 941.4120426177979\n",
      "step: 1572.0, loss: 941.3966865539551\n",
      "step: 1573.0, loss: 942.0874576568604\n",
      "step: 1574.0, loss: 939.3990030288696\n",
      "step: 1575.0, loss: 940.9291496276855\n",
      "step: 1576.0, loss: 940.3209476470947\n",
      "step: 1577.0, loss: 940.556547164917\n",
      "step: 1578.0, loss: 940.4179573059082\n",
      "step: 1579.0, loss: 940.90358543396\n",
      "step: 1580.0, loss: 943.0011348724365\n",
      "step: 1581.0, loss: 944.1883354187012\n",
      "step: 1582.0, loss: 940.9962530136108\n",
      "step: 1583.0, loss: 941.4090957641602\n",
      "step: 1584.0, loss: 940.8093214035034\n",
      "step: 1585.0, loss: 941.2187957763672\n",
      "step: 1586.0, loss: 940.0895395278931\n",
      "step: 1587.0, loss: 941.1401491165161\n",
      "step: 1588.0, loss: 942.6188554763794\n",
      "step: 1589.0, loss: 943.0429821014404\n",
      "step: 1590.0, loss: 941.8547763824463\n",
      "step: 1591.0, loss: 941.737714767456\n",
      "step: 1592.0, loss: 942.3182697296143\n",
      "step: 1593.0, loss: 941.3517580032349\n",
      "step: 1594.0, loss: 941.5703039169312\n",
      "step: 1595.0, loss: 941.2304010391235\n",
      "step: 1596.0, loss: 942.0809326171875\n",
      "step: 1597.0, loss: 942.4735174179077\n",
      "step: 1598.0, loss: 940.4514741897583\n",
      "step: 1599.0, loss: 938.0223340988159\n",
      "step: 1600.0, loss: 941.2611818313599\n",
      "step: 1601.0, loss: 942.5621566772461\n",
      "step: 1602.0, loss: 939.5921154022217\n",
      "step: 1603.0, loss: 940.6194047927856\n",
      "step: 1604.0, loss: 942.1900930404663\n",
      "step: 1605.0, loss: 939.7035455703735\n",
      "step: 1606.0, loss: 941.0965614318848\n",
      "step: 1607.0, loss: 940.5634346008301\n",
      "step: 1608.0, loss: 940.3164138793945\n",
      "step: 1609.0, loss: 939.5815143585205\n",
      "step: 1610.0, loss: 940.8529844284058\n",
      "step: 1611.0, loss: 939.7144899368286\n",
      "step: 1612.0, loss: 938.5671653747559\n",
      "step: 1613.0, loss: 941.9203863143921\n",
      "step: 1614.0, loss: 938.8821239471436\n",
      "step: 1615.0, loss: 941.621283531189\n",
      "step: 1616.0, loss: 940.7975835800171\n",
      "step: 1617.0, loss: 939.2789125442505\n",
      "step: 1618.0, loss: 941.5665016174316\n",
      "step: 1619.0, loss: 941.1266603469849\n",
      "step: 1620.0, loss: 938.2909393310547\n",
      "step: 1621.0, loss: 941.0222091674805\n",
      "step: 1622.0, loss: 939.9168481826782\n",
      "step: 1623.0, loss: 940.344404220581\n",
      "step: 1624.0, loss: 940.2487478256226\n",
      "step: 1625.0, loss: 942.4191875457764\n",
      "step: 1626.0, loss: 939.9791574478149\n",
      "step: 1627.0, loss: 940.5552234649658\n",
      "step: 1628.0, loss: 941.452278137207\n",
      "step: 1629.0, loss: 940.0080699920654\n",
      "step: 1630.0, loss: 941.8925466537476\n",
      "step: 1631.0, loss: 940.6948480606079\n",
      "step: 1632.0, loss: 940.6156063079834\n",
      "step: 1633.0, loss: 939.6868152618408\n",
      "step: 1634.0, loss: 938.5439796447754\n",
      "step: 1635.0, loss: 941.2208528518677\n",
      "step: 1636.0, loss: 939.9824237823486\n",
      "step: 1637.0, loss: 940.8615531921387\n",
      "step: 1638.0, loss: 940.1534776687622\n",
      "step: 1639.0, loss: 940.267596244812\n",
      "step: 1640.0, loss: 937.7821073532104\n",
      "step: 1641.0, loss: 942.4876146316528\n",
      "step: 1642.0, loss: 939.4299154281616\n",
      "step: 1643.0, loss: 939.9867458343506\n",
      "step: 1644.0, loss: 941.5121812820435\n",
      "step: 1645.0, loss: 942.4854021072388\n",
      "step: 1646.0, loss: 941.3291463851929\n",
      "step: 1647.0, loss: 941.4327516555786\n",
      "step: 1648.0, loss: 940.572865486145\n",
      "step: 1649.0, loss: 941.3527526855469\n",
      "step: 1650.0, loss: 942.4333715438843\n",
      "step: 1651.0, loss: 940.1179904937744\n",
      "step: 1652.0, loss: 941.7063369750977\n",
      "step: 1653.0, loss: 940.3805961608887\n",
      "step: 1654.0, loss: 941.168436050415\n",
      "step: 1655.0, loss: 939.9197435379028\n",
      "step: 1656.0, loss: 940.796591758728\n",
      "step: 1657.0, loss: 941.2453575134277\n",
      "step: 1658.0, loss: 940.3424186706543\n",
      "step: 1659.0, loss: 938.0856037139893\n",
      "step: 1660.0, loss: 939.7172594070435\n",
      "step: 1661.0, loss: 940.6903696060181\n",
      "step: 1662.0, loss: 938.9584293365479\n",
      "step: 1663.0, loss: 938.7693710327148\n",
      "step: 1664.0, loss: 940.7744779586792\n",
      "step: 1665.0, loss: 941.6874179840088\n",
      "step: 1666.0, loss: 940.5729322433472\n",
      "step: 1667.0, loss: 942.1015310287476\n",
      "step: 1668.0, loss: 939.8960123062134\n",
      "step: 1669.0, loss: 940.2306413650513\n",
      "step: 1670.0, loss: 940.7538051605225\n",
      "step: 1671.0, loss: 939.7862300872803\n",
      "step: 1672.0, loss: 940.9808235168457\n",
      "step: 1673.0, loss: 941.584397315979\n",
      "step: 1674.0, loss: 940.6200971603394\n",
      "step: 1675.0, loss: 940.2019920349121\n",
      "step: 1676.0, loss: 940.8971080780029\n",
      "step: 1677.0, loss: 940.9946584701538\n",
      "step: 1678.0, loss: 941.127950668335\n",
      "step: 1679.0, loss: 939.3785581588745\n",
      "step: 1680.0, loss: 940.125072479248\n",
      "step: 1681.0, loss: 939.3805637359619\n",
      "step: 1682.0, loss: 942.9127254486084\n",
      "step: 1683.0, loss: 939.9091758728027\n",
      "step: 1684.0, loss: 941.2724561691284\n",
      "step: 1685.0, loss: 939.5642318725586\n",
      "step: 1686.0, loss: 939.8761177062988\n",
      "step: 1687.0, loss: 938.8638582229614\n",
      "step: 1688.0, loss: 940.4858798980713\n",
      "step: 1689.0, loss: 939.1052255630493\n",
      "step: 1690.0, loss: 938.7782545089722\n",
      "step: 1691.0, loss: 939.1147031784058\n",
      "step: 1692.0, loss: 941.1200942993164\n",
      "step: 1693.0, loss: 940.0887765884399\n",
      "step: 1694.0, loss: 940.4896373748779\n",
      "step: 1695.0, loss: 938.8001298904419\n",
      "step: 1696.0, loss: 938.459394454956\n",
      "step: 1697.0, loss: 940.3304595947266\n",
      "step: 1698.0, loss: 939.5372076034546\n",
      "step: 1699.0, loss: 940.4535913467407\n",
      "step: 1700.0, loss: 940.4762439727783\n",
      "step: 1701.0, loss: 940.6688928604126\n",
      "step: 1702.0, loss: 940.4224481582642\n",
      "step: 1703.0, loss: 940.4889068603516\n",
      "step: 1704.0, loss: 938.6480102539062\n",
      "step: 1705.0, loss: 940.18665599823\n",
      "step: 1706.0, loss: 939.8092193603516\n",
      "step: 1707.0, loss: 941.0798673629761\n",
      "step: 1708.0, loss: 940.4435396194458\n",
      "step: 1709.0, loss: 939.8659524917603\n",
      "step: 1710.0, loss: 940.17067527771\n",
      "step: 1711.0, loss: 940.088472366333\n",
      "step: 1712.0, loss: 939.9655895233154\n",
      "step: 1713.0, loss: 940.5615062713623\n",
      "step: 1714.0, loss: 938.7397089004517\n",
      "step: 1715.0, loss: 939.1153450012207\n",
      "step: 1716.0, loss: 940.6045799255371\n",
      "step: 1717.0, loss: 939.6397294998169\n",
      "step: 1718.0, loss: 941.2041025161743\n",
      "step: 1719.0, loss: 939.7362432479858\n",
      "step: 1720.0, loss: 940.5652170181274\n",
      "step: 1721.0, loss: 938.8819484710693\n",
      "step: 1722.0, loss: 939.0774688720703\n",
      "step: 1723.0, loss: 940.1779918670654\n",
      "step: 1724.0, loss: 939.4286003112793\n",
      "step: 1725.0, loss: 939.1629428863525\n",
      "step: 1726.0, loss: 940.2336711883545\n",
      "step: 1727.0, loss: 939.4128189086914\n",
      "step: 1728.0, loss: 939.0895280838013\n",
      "step: 1729.0, loss: 939.5262441635132\n",
      "step: 1730.0, loss: 937.9290676116943\n",
      "step: 1731.0, loss: 940.0134363174438\n",
      "step: 1732.0, loss: 938.7665529251099\n",
      "step: 1733.0, loss: 939.5416793823242\n",
      "step: 1734.0, loss: 940.1465940475464\n",
      "step: 1735.0, loss: 941.2271480560303\n",
      "step: 1736.0, loss: 940.7120923995972\n",
      "step: 1737.0, loss: 941.3421983718872\n",
      "step: 1738.0, loss: 940.3252372741699\n",
      "step: 1739.0, loss: 939.4802579879761\n",
      "step: 1740.0, loss: 939.8453197479248\n",
      "step: 1741.0, loss: 938.0784206390381\n",
      "step: 1742.0, loss: 940.0480794906616\n",
      "step: 1743.0, loss: 940.3232135772705\n",
      "step: 1744.0, loss: 939.5364408493042\n",
      "step: 1745.0, loss: 940.5598611831665\n",
      "step: 1746.0, loss: 939.6210193634033\n",
      "step: 1747.0, loss: 939.3441953659058\n",
      "step: 1748.0, loss: 940.7229080200195\n",
      "step: 1749.0, loss: 939.458194732666\n",
      "step: 1750.0, loss: 941.063666343689\n",
      "step: 1751.0, loss: 939.1267528533936\n",
      "step: 1752.0, loss: 939.8744668960571\n",
      "step: 1753.0, loss: 940.2680330276489\n",
      "step: 1754.0, loss: 937.7828397750854\n",
      "step: 1755.0, loss: 938.7081069946289\n",
      "step: 1756.0, loss: 938.0766706466675\n",
      "step: 1757.0, loss: 939.2498664855957\n",
      "step: 1758.0, loss: 939.2917795181274\n",
      "step: 1759.0, loss: 940.2900705337524\n",
      "step: 1760.0, loss: 940.8540086746216\n",
      "step: 1761.0, loss: 941.2512731552124\n",
      "step: 1762.0, loss: 937.8688430786133\n",
      "step: 1763.0, loss: 940.7987966537476\n",
      "step: 1764.0, loss: 940.7723293304443\n",
      "step: 1765.0, loss: 938.7488193511963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1766.0, loss: 939.7711791992188\n",
      "step: 1767.0, loss: 939.9133911132812\n",
      "step: 1768.0, loss: 940.6118278503418\n",
      "step: 1769.0, loss: 938.4441738128662\n",
      "step: 1770.0, loss: 940.3109045028687\n",
      "step: 1771.0, loss: 938.4687585830688\n",
      "step: 1772.0, loss: 938.5443096160889\n",
      "step: 1773.0, loss: 941.0863199234009\n",
      "step: 1774.0, loss: 939.1661405563354\n",
      "step: 1775.0, loss: 940.5425825119019\n",
      "step: 1776.0, loss: 939.3945941925049\n",
      "step: 1777.0, loss: 940.0381889343262\n",
      "step: 1778.0, loss: 939.4267930984497\n",
      "step: 1779.0, loss: 940.9048452377319\n",
      "step: 1780.0, loss: 938.6580457687378\n",
      "step: 1781.0, loss: 938.166934967041\n",
      "step: 1782.0, loss: 938.0953321456909\n",
      "step: 1783.0, loss: 939.2094964981079\n",
      "step: 1784.0, loss: 939.349365234375\n",
      "step: 1785.0, loss: 939.4165077209473\n",
      "step: 1786.0, loss: 939.9634485244751\n",
      "step: 1787.0, loss: 939.5972127914429\n",
      "step: 1788.0, loss: 938.1565093994141\n",
      "step: 1789.0, loss: 939.5702428817749\n",
      "step: 1790.0, loss: 940.1472082138062\n",
      "step: 1791.0, loss: 939.357419013977\n",
      "step: 1792.0, loss: 939.2671585083008\n",
      "step: 1793.0, loss: 938.9063177108765\n",
      "step: 1794.0, loss: 940.176399230957\n",
      "step: 1795.0, loss: 938.1764631271362\n",
      "step: 1796.0, loss: 937.5670871734619\n",
      "step: 1797.0, loss: 939.7342262268066\n",
      "step: 1798.0, loss: 937.9390325546265\n",
      "step: 1799.0, loss: 938.4588527679443\n",
      "step: 1800.0, loss: 938.2543859481812\n",
      "step: 1801.0, loss: 939.6781053543091\n",
      "step: 1802.0, loss: 939.9968099594116\n",
      "step: 1803.0, loss: 938.8040685653687\n",
      "step: 1804.0, loss: 940.3638916015625\n",
      "step: 1805.0, loss: 937.8616466522217\n",
      "step: 1806.0, loss: 938.1179542541504\n",
      "step: 1807.0, loss: 939.5467414855957\n",
      "step: 1808.0, loss: 938.1708126068115\n",
      "step: 1809.0, loss: 938.9898595809937\n",
      "step: 1810.0, loss: 938.6810102462769\n",
      "step: 1811.0, loss: 939.7426958084106\n",
      "step: 1812.0, loss: 938.7018890380859\n",
      "step: 1813.0, loss: 939.0818996429443\n",
      "step: 1814.0, loss: 939.3499422073364\n",
      "step: 1815.0, loss: 937.2000036239624\n",
      "step: 1816.0, loss: 938.1557168960571\n",
      "step: 1817.0, loss: 938.7725048065186\n",
      "step: 1818.0, loss: 938.2769002914429\n",
      "step: 1819.0, loss: 939.3746871948242\n",
      "step: 1820.0, loss: 939.0681810379028\n",
      "step: 1821.0, loss: 939.6893653869629\n",
      "step: 1822.0, loss: 940.3938140869141\n",
      "step: 1823.0, loss: 938.9296436309814\n",
      "step: 1824.0, loss: 938.5754108428955\n",
      "step: 1825.0, loss: 937.9460201263428\n",
      "step: 1826.0, loss: 937.8275461196899\n",
      "step: 1827.0, loss: 938.3065509796143\n",
      "step: 1828.0, loss: 938.7580976486206\n",
      "step: 1829.0, loss: 939.2494478225708\n",
      "step: 1830.0, loss: 940.2971792221069\n",
      "step: 1831.0, loss: 939.3619031906128\n",
      "step: 1832.0, loss: 939.4734811782837\n",
      "step: 1833.0, loss: 938.1644077301025\n",
      "step: 1834.0, loss: 939.0940103530884\n",
      "step: 1835.0, loss: 937.2788410186768\n",
      "step: 1836.0, loss: 939.5290603637695\n",
      "step: 1837.0, loss: 939.7979669570923\n",
      "step: 1838.0, loss: 937.5138654708862\n",
      "step: 1839.0, loss: 938.8737316131592\n",
      "step: 1840.0, loss: 939.2136116027832\n",
      "step: 1841.0, loss: 940.4738540649414\n",
      "step: 1842.0, loss: 939.5046319961548\n",
      "step: 1843.0, loss: 937.8567180633545\n",
      "step: 1844.0, loss: 939.4041299819946\n",
      "step: 1845.0, loss: 938.976957321167\n",
      "step: 1846.0, loss: 938.4798498153687\n",
      "step: 1847.0, loss: 938.0891437530518\n",
      "step: 1848.0, loss: 939.9328594207764\n",
      "step: 1849.0, loss: 939.6322937011719\n",
      "step: 1850.0, loss: 939.7806196212769\n",
      "step: 1851.0, loss: 937.5823802947998\n",
      "step: 1852.0, loss: 939.4242658615112\n",
      "step: 1853.0, loss: 937.5280523300171\n",
      "step: 1854.0, loss: 937.8269472122192\n",
      "step: 1855.0, loss: 938.6896524429321\n",
      "step: 1856.0, loss: 939.6292924880981\n",
      "step: 1857.0, loss: 939.1035995483398\n",
      "step: 1858.0, loss: 937.8269176483154\n",
      "step: 1859.0, loss: 939.0694580078125\n",
      "step: 1860.0, loss: 937.6629390716553\n",
      "step: 1861.0, loss: 939.6928405761719\n",
      "step: 1862.0, loss: 939.7796659469604\n",
      "step: 1863.0, loss: 937.9277563095093\n",
      "step: 1864.0, loss: 938.7612648010254\n",
      "step: 1865.0, loss: 939.6508235931396\n",
      "step: 1866.0, loss: 938.4130392074585\n",
      "step: 1867.0, loss: 938.2781858444214\n",
      "step: 1868.0, loss: 937.9863996505737\n",
      "step: 1869.0, loss: 938.7645816802979\n",
      "step: 1870.0, loss: 938.3261032104492\n",
      "step: 1871.0, loss: 939.4711751937866\n",
      "step: 1872.0, loss: 938.4672269821167\n",
      "step: 1873.0, loss: 938.7031011581421\n",
      "step: 1874.0, loss: 938.4189939498901\n",
      "step: 1875.0, loss: 940.290020942688\n",
      "step: 1876.0, loss: 937.9539794921875\n",
      "step: 1877.0, loss: 938.5909414291382\n",
      "step: 1878.0, loss: 939.0027570724487\n",
      "step: 1879.0, loss: 938.4982223510742\n",
      "step: 1880.0, loss: 937.8048725128174\n",
      "step: 1881.0, loss: 938.2599306106567\n",
      "step: 1882.0, loss: 938.3052587509155\n",
      "step: 1883.0, loss: 938.3220958709717\n",
      "step: 1884.0, loss: 936.8980445861816\n",
      "step: 1885.0, loss: 939.5584201812744\n",
      "step: 1886.0, loss: 937.6237516403198\n",
      "step: 1887.0, loss: 939.7755012512207\n",
      "step: 1888.0, loss: 940.625599861145\n",
      "step: 1889.0, loss: 938.8842182159424\n",
      "step: 1890.0, loss: 938.5786905288696\n",
      "step: 1891.0, loss: 939.3335657119751\n",
      "step: 1892.0, loss: 937.3412103652954\n",
      "step: 1893.0, loss: 936.1862163543701\n",
      "step: 1894.0, loss: 939.0622854232788\n",
      "step: 1895.0, loss: 938.5792999267578\n",
      "step: 1896.0, loss: 938.3833599090576\n",
      "step: 1897.0, loss: 938.7103996276855\n",
      "step: 1898.0, loss: 938.2403163909912\n",
      "step: 1899.0, loss: 939.5251502990723\n",
      "step: 1900.0, loss: 938.23046875\n",
      "step: 1901.0, loss: 940.5733098983765\n",
      "step: 1902.0, loss: 938.768648147583\n",
      "step: 1903.0, loss: 938.4180202484131\n",
      "step: 1904.0, loss: 940.1499309539795\n",
      "step: 1905.0, loss: 939.440990447998\n",
      "step: 1906.0, loss: 940.0441627502441\n",
      "step: 1907.0, loss: 940.3613414764404\n",
      "step: 1908.0, loss: 936.2472009658813\n",
      "step: 1909.0, loss: 938.5110559463501\n",
      "step: 1910.0, loss: 939.4993600845337\n",
      "step: 1911.0, loss: 938.2047681808472\n",
      "step: 1912.0, loss: 937.0568857192993\n",
      "step: 1913.0, loss: 938.3986215591431\n",
      "step: 1914.0, loss: 937.8771677017212\n",
      "step: 1915.0, loss: 937.3922100067139\n",
      "step: 1916.0, loss: 938.5742673873901\n",
      "step: 1917.0, loss: 937.4868402481079\n",
      "step: 1918.0, loss: 937.5453653335571\n",
      "step: 1919.0, loss: 937.5274858474731\n",
      "step: 1920.0, loss: 937.0742883682251\n",
      "step: 1921.0, loss: 938.8751029968262\n",
      "step: 1922.0, loss: 938.3344831466675\n",
      "step: 1923.0, loss: 937.6470308303833\n",
      "step: 1924.0, loss: 939.567852973938\n",
      "step: 1925.0, loss: 939.8707122802734\n",
      "step: 1926.0, loss: 939.61803150177\n",
      "step: 1927.0, loss: 937.6320648193359\n",
      "step: 1928.0, loss: 938.0911912918091\n",
      "step: 1929.0, loss: 937.156907081604\n",
      "step: 1930.0, loss: 938.8692216873169\n",
      "step: 1931.0, loss: 937.3617649078369\n",
      "step: 1932.0, loss: 938.6987009048462\n",
      "step: 1933.0, loss: 938.4290771484375\n",
      "step: 1934.0, loss: 939.4873371124268\n",
      "step: 1935.0, loss: 938.9858684539795\n",
      "step: 1936.0, loss: 937.5601806640625\n",
      "step: 1937.0, loss: 940.4652252197266\n",
      "step: 1938.0, loss: 937.8177137374878\n",
      "step: 1939.0, loss: 936.9056091308594\n",
      "step: 1940.0, loss: 937.0386619567871\n",
      "step: 1941.0, loss: 937.3949975967407\n",
      "step: 1942.0, loss: 938.7083778381348\n",
      "step: 1943.0, loss: 937.9897680282593\n",
      "step: 1944.0, loss: 937.6332483291626\n",
      "step: 1945.0, loss: 938.6032886505127\n",
      "step: 1946.0, loss: 937.8922624588013\n",
      "step: 1947.0, loss: 939.0035648345947\n",
      "step: 1948.0, loss: 938.6037769317627\n",
      "step: 1949.0, loss: 938.5964345932007\n",
      "step: 1950.0, loss: 938.7927417755127\n",
      "step: 1951.0, loss: 938.7872076034546\n",
      "step: 1952.0, loss: 937.8683891296387\n",
      "step: 1953.0, loss: 939.0033988952637\n",
      "step: 1954.0, loss: 937.8065547943115\n",
      "step: 1955.0, loss: 939.4713249206543\n",
      "step: 1956.0, loss: 936.9016542434692\n",
      "step: 1957.0, loss: 937.4407367706299\n",
      "step: 1958.0, loss: 936.2933139801025\n",
      "step: 1959.0, loss: 939.4266309738159\n",
      "step: 1960.0, loss: 940.0353660583496\n",
      "step: 1961.0, loss: 938.5251407623291\n",
      "step: 1962.0, loss: 937.5710096359253\n",
      "step: 1963.0, loss: 937.8262348175049\n",
      "step: 1964.0, loss: 936.6114873886108\n",
      "step: 1965.0, loss: 937.6521415710449\n",
      "step: 1966.0, loss: 938.79381275177\n",
      "step: 1967.0, loss: 937.4918079376221\n",
      "step: 1968.0, loss: 936.1882333755493\n",
      "step: 1969.0, loss: 938.1904239654541\n",
      "step: 1970.0, loss: 938.752438545227\n",
      "step: 1971.0, loss: 939.1305065155029\n",
      "step: 1972.0, loss: 938.3576231002808\n",
      "step: 1973.0, loss: 937.2678937911987\n",
      "step: 1974.0, loss: 938.173173904419\n",
      "step: 1975.0, loss: 938.1929311752319\n",
      "step: 1976.0, loss: 939.4747304916382\n",
      "step: 1977.0, loss: 938.5453443527222\n",
      "step: 1978.0, loss: 936.5353240966797\n",
      "step: 1979.0, loss: 938.0729312896729\n",
      "step: 1980.0, loss: 938.302493095398\n",
      "step: 1981.0, loss: 937.9780740737915\n",
      "step: 1982.0, loss: 937.1897821426392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1983.0, loss: 939.1442928314209\n",
      "step: 1984.0, loss: 935.4390535354614\n",
      "step: 1985.0, loss: 937.895058631897\n",
      "step: 1986.0, loss: 937.6948614120483\n",
      "step: 1987.0, loss: 937.0303182601929\n",
      "step: 1988.0, loss: 937.5826101303101\n",
      "step: 1989.0, loss: 938.2951030731201\n",
      "step: 1990.0, loss: 937.6707849502563\n",
      "step: 1991.0, loss: 936.8913154602051\n",
      "step: 1992.0, loss: 937.5159435272217\n",
      "step: 1993.0, loss: 937.2296848297119\n",
      "step: 1994.0, loss: 938.6783380508423\n",
      "step: 1995.0, loss: 939.38560962677\n",
      "step: 1996.0, loss: 938.0964336395264\n",
      "step: 1997.0, loss: 939.3913803100586\n",
      "step: 1998.0, loss: 936.7647323608398\n",
      "step: 1999.0, loss: 936.7242374420166\n",
      "step: 2000.0, loss: 939.4375562667847\n",
      "step: 2001.0, loss: 939.3379640579224\n",
      "step: 2002.0, loss: 936.9405546188354\n",
      "step: 2003.0, loss: 936.8815212249756\n",
      "step: 2004.0, loss: 936.8543405532837\n",
      "step: 2005.0, loss: 937.2838687896729\n",
      "step: 2006.0, loss: 934.549563407898\n",
      "step: 2007.0, loss: 937.314866065979\n",
      "step: 2008.0, loss: 936.5820808410645\n",
      "step: 2009.0, loss: 938.0980291366577\n",
      "step: 2010.0, loss: 938.4906244277954\n",
      "step: 2011.0, loss: 938.681960105896\n",
      "step: 2012.0, loss: 939.2316770553589\n",
      "step: 2013.0, loss: 938.359543800354\n",
      "step: 2014.0, loss: 937.2813816070557\n",
      "step: 2015.0, loss: 938.8594799041748\n",
      "step: 2016.0, loss: 936.163164138794\n",
      "step: 2017.0, loss: 938.1703195571899\n",
      "step: 2018.0, loss: 938.6412315368652\n",
      "step: 2019.0, loss: 939.3188495635986\n",
      "step: 2020.0, loss: 937.8688335418701\n",
      "step: 2021.0, loss: 938.9837608337402\n",
      "step: 2022.0, loss: 937.7701292037964\n",
      "step: 2023.0, loss: 936.9155197143555\n",
      "step: 2024.0, loss: 935.9990549087524\n",
      "step: 2025.0, loss: 936.8588609695435\n",
      "step: 2026.0, loss: 939.7589626312256\n",
      "step: 2027.0, loss: 938.0192813873291\n",
      "step: 2028.0, loss: 936.281252861023\n",
      "step: 2029.0, loss: 936.9380979537964\n",
      "step: 2030.0, loss: 937.0136728286743\n",
      "step: 2031.0, loss: 936.5449123382568\n",
      "step: 2032.0, loss: 936.2396659851074\n",
      "step: 2033.0, loss: 936.552264213562\n",
      "step: 2034.0, loss: 938.3473625183105\n",
      "step: 2035.0, loss: 936.5234422683716\n",
      "step: 2036.0, loss: 937.637001991272\n",
      "step: 2037.0, loss: 938.7244653701782\n",
      "step: 2038.0, loss: 936.6988973617554\n",
      "step: 2039.0, loss: 937.4664421081543\n",
      "step: 2040.0, loss: 937.0589656829834\n",
      "step: 2041.0, loss: 938.7890682220459\n",
      "step: 2042.0, loss: 938.4580945968628\n",
      "step: 2043.0, loss: 937.4717493057251\n",
      "step: 2044.0, loss: 938.1274309158325\n",
      "step: 2045.0, loss: 939.6297073364258\n",
      "step: 2046.0, loss: 936.6963901519775\n",
      "step: 2047.0, loss: 938.7944192886353\n",
      "step: 2048.0, loss: 938.3871917724609\n",
      "step: 2049.0, loss: 936.532548904419\n",
      "step: 2050.0, loss: 938.758469581604\n",
      "step: 2051.0, loss: 936.2791528701782\n",
      "step: 2052.0, loss: 937.7509107589722\n",
      "step: 2053.0, loss: 937.4477481842041\n",
      "step: 2054.0, loss: 937.4475240707397\n",
      "step: 2055.0, loss: 936.6457262039185\n",
      "step: 2056.0, loss: 937.6974449157715\n",
      "step: 2057.0, loss: 937.0830688476562\n",
      "step: 2058.0, loss: 937.8214483261108\n",
      "step: 2059.0, loss: 935.5450401306152\n",
      "step: 2060.0, loss: 938.6698389053345\n",
      "step: 2061.0, loss: 935.7998914718628\n",
      "step: 2062.0, loss: 937.7855348587036\n",
      "step: 2063.0, loss: 937.8942241668701\n",
      "step: 2064.0, loss: 937.9900388717651\n",
      "step: 2065.0, loss: 937.8403072357178\n",
      "step: 2066.0, loss: 937.7903814315796\n",
      "step: 2067.0, loss: 936.7081451416016\n",
      "step: 2068.0, loss: 937.1613674163818\n",
      "step: 2069.0, loss: 938.3645000457764\n",
      "step: 2070.0, loss: 937.3044300079346\n",
      "step: 2071.0, loss: 936.323429107666\n",
      "step: 2072.0, loss: 936.5293073654175\n",
      "step: 2073.0, loss: 938.1699275970459\n",
      "step: 2074.0, loss: 936.8608722686768\n",
      "step: 2075.0, loss: 939.0629549026489\n",
      "step: 2076.0, loss: 938.3105802536011\n",
      "step: 2077.0, loss: 938.3283624649048\n",
      "step: 2078.0, loss: 938.0534391403198\n",
      "step: 2079.0, loss: 937.5113706588745\n",
      "step: 2080.0, loss: 935.6777219772339\n",
      "step: 2081.0, loss: 936.085578918457\n",
      "step: 2082.0, loss: 937.7792959213257\n",
      "step: 2083.0, loss: 936.7187881469727\n",
      "step: 2084.0, loss: 935.7338924407959\n",
      "step: 2085.0, loss: 938.0906810760498\n",
      "step: 2086.0, loss: 937.7469053268433\n",
      "step: 2087.0, loss: 936.326060295105\n",
      "step: 2088.0, loss: 936.4751110076904\n",
      "step: 2089.0, loss: 936.3447790145874\n",
      "step: 2090.0, loss: 937.8494262695312\n",
      "step: 2091.0, loss: 938.7954683303833\n",
      "step: 2092.0, loss: 936.8194665908813\n",
      "step: 2093.0, loss: 936.1408967971802\n",
      "step: 2094.0, loss: 938.3713502883911\n",
      "step: 2095.0, loss: 936.4813547134399\n",
      "step: 2096.0, loss: 936.0534391403198\n",
      "step: 2097.0, loss: 938.0316505432129\n",
      "step: 2098.0, loss: 938.1296033859253\n",
      "step: 2099.0, loss: 937.9959669113159\n",
      "step: 2100.0, loss: 936.7145872116089\n",
      "step: 2101.0, loss: 936.838888168335\n",
      "step: 2102.0, loss: 937.9687194824219\n",
      "step: 2103.0, loss: 937.068603515625\n",
      "step: 2104.0, loss: 937.004955291748\n",
      "step: 2105.0, loss: 938.5630989074707\n",
      "step: 2106.0, loss: 936.7494840621948\n",
      "step: 2107.0, loss: 936.9167613983154\n",
      "step: 2108.0, loss: 936.0309247970581\n",
      "step: 2109.0, loss: 936.1097965240479\n",
      "step: 2110.0, loss: 935.3884077072144\n",
      "step: 2111.0, loss: 936.8478412628174\n",
      "step: 2112.0, loss: 937.9445371627808\n",
      "step: 2113.0, loss: 936.024730682373\n",
      "step: 2114.0, loss: 936.1247892379761\n",
      "step: 2115.0, loss: 936.3516502380371\n",
      "step: 2116.0, loss: 935.1698532104492\n",
      "step: 2117.0, loss: 935.1963872909546\n",
      "step: 2118.0, loss: 937.3643760681152\n",
      "step: 2119.0, loss: 938.7187976837158\n",
      "step: 2120.0, loss: 937.8136253356934\n",
      "step: 2121.0, loss: 937.2629890441895\n",
      "step: 2122.0, loss: 936.3893213272095\n",
      "step: 2123.0, loss: 937.3524227142334\n",
      "step: 2124.0, loss: 936.7335090637207\n",
      "step: 2125.0, loss: 935.7088651657104\n",
      "step: 2126.0, loss: 936.5316371917725\n",
      "step: 2127.0, loss: 936.6021280288696\n",
      "step: 2128.0, loss: 936.3758153915405\n",
      "step: 2129.0, loss: 936.6233758926392\n",
      "step: 2130.0, loss: 939.2815465927124\n",
      "step: 2131.0, loss: 938.6836223602295\n",
      "step: 2132.0, loss: 935.1304273605347\n",
      "step: 2133.0, loss: 936.7502498626709\n",
      "step: 2134.0, loss: 937.192946434021\n",
      "step: 2135.0, loss: 936.2165412902832\n",
      "step: 2136.0, loss: 936.3291273117065\n",
      "step: 2137.0, loss: 936.0237340927124\n",
      "step: 2138.0, loss: 935.4235372543335\n",
      "step: 2139.0, loss: 935.2373380661011\n",
      "step: 2140.0, loss: 936.1267147064209\n",
      "step: 2141.0, loss: 935.1600561141968\n",
      "step: 2142.0, loss: 937.4137783050537\n",
      "step: 2143.0, loss: 937.8945741653442\n",
      "step: 2144.0, loss: 936.7772636413574\n",
      "step: 2145.0, loss: 938.2171020507812\n",
      "step: 2146.0, loss: 936.2956209182739\n",
      "step: 2147.0, loss: 939.4278841018677\n",
      "step: 2148.0, loss: 936.3301191329956\n",
      "step: 2149.0, loss: 936.1684303283691\n",
      "step: 2150.0, loss: 936.8875284194946\n",
      "step: 2151.0, loss: 937.4971647262573\n",
      "step: 2152.0, loss: 937.0763311386108\n",
      "step: 2153.0, loss: 935.8651943206787\n",
      "step: 2154.0, loss: 938.010027885437\n",
      "step: 2155.0, loss: 937.0285301208496\n",
      "step: 2156.0, loss: 935.582857131958\n",
      "step: 2157.0, loss: 935.8784103393555\n",
      "step: 2158.0, loss: 935.5664491653442\n",
      "step: 2159.0, loss: 938.4496822357178\n",
      "step: 2160.0, loss: 937.893497467041\n",
      "step: 2161.0, loss: 938.1712970733643\n",
      "step: 2162.0, loss: 936.6019344329834\n",
      "step: 2163.0, loss: 936.081934928894\n",
      "step: 2164.0, loss: 936.6674957275391\n",
      "step: 2165.0, loss: 937.336103439331\n",
      "step: 2166.0, loss: 935.6382875442505\n",
      "step: 2167.0, loss: 935.9175462722778\n",
      "step: 2168.0, loss: 937.8963146209717\n",
      "step: 2169.0, loss: 937.2104721069336\n",
      "step: 2170.0, loss: 937.473482131958\n",
      "step: 2171.0, loss: 936.167646408081\n",
      "step: 2172.0, loss: 938.9884481430054\n",
      "step: 2173.0, loss: 938.7578477859497\n",
      "step: 2174.0, loss: 936.9825859069824\n",
      "step: 2175.0, loss: 937.7866516113281\n",
      "step: 2176.0, loss: 937.3233108520508\n",
      "step: 2177.0, loss: 933.6506128311157\n",
      "step: 2178.0, loss: 935.2556390762329\n",
      "step: 2179.0, loss: 937.4921941757202\n",
      "step: 2180.0, loss: 937.0209875106812\n",
      "step: 2181.0, loss: 935.92799949646\n",
      "step: 2182.0, loss: 934.923261642456\n",
      "step: 2183.0, loss: 936.1272792816162\n",
      "step: 2184.0, loss: 936.7851581573486\n",
      "step: 2185.0, loss: 936.6543169021606\n",
      "step: 2186.0, loss: 936.1297426223755\n",
      "step: 2187.0, loss: 936.9292860031128\n",
      "step: 2188.0, loss: 936.0232124328613\n",
      "step: 2189.0, loss: 935.7711877822876\n",
      "step: 2190.0, loss: 935.8728551864624\n",
      "step: 2191.0, loss: 937.8350839614868\n",
      "step: 2192.0, loss: 937.7668914794922\n",
      "step: 2193.0, loss: 935.3454885482788\n",
      "step: 2194.0, loss: 936.9168643951416\n",
      "step: 2195.0, loss: 936.3111000061035\n",
      "step: 2196.0, loss: 938.4954700469971\n",
      "step: 2197.0, loss: 938.5374593734741\n",
      "step: 2198.0, loss: 936.7069959640503\n",
      "step: 2199.0, loss: 934.7443246841431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2200.0, loss: 936.4956178665161\n",
      "step: 2201.0, loss: 938.2271003723145\n",
      "step: 2202.0, loss: 936.1989297866821\n",
      "step: 2203.0, loss: 937.9594430923462\n",
      "step: 2204.0, loss: 936.3687715530396\n",
      "step: 2205.0, loss: 933.9245719909668\n",
      "step: 2206.0, loss: 936.6122827529907\n",
      "step: 2207.0, loss: 936.736762046814\n",
      "step: 2208.0, loss: 937.145435333252\n",
      "step: 2209.0, loss: 937.1243181228638\n",
      "step: 2210.0, loss: 938.1030082702637\n",
      "step: 2211.0, loss: 936.4945755004883\n",
      "step: 2212.0, loss: 936.5334196090698\n",
      "step: 2213.0, loss: 934.7787590026855\n",
      "step: 2214.0, loss: 935.3615789413452\n",
      "step: 2215.0, loss: 936.8829069137573\n",
      "step: 2216.0, loss: 934.2920322418213\n",
      "step: 2217.0, loss: 937.1644086837769\n",
      "step: 2218.0, loss: 934.9073524475098\n",
      "step: 2219.0, loss: 937.3508567810059\n",
      "step: 2220.0, loss: 935.9512519836426\n",
      "step: 2221.0, loss: 934.456392288208\n",
      "step: 2222.0, loss: 936.1596212387085\n",
      "step: 2223.0, loss: 937.0589761734009\n",
      "step: 2224.0, loss: 936.1974420547485\n",
      "step: 2225.0, loss: 937.7817478179932\n",
      "step: 2226.0, loss: 935.6463022232056\n",
      "step: 2227.0, loss: 939.2684259414673\n",
      "step: 2228.0, loss: 934.7832326889038\n",
      "step: 2229.0, loss: 935.3738460540771\n",
      "step: 2230.0, loss: 937.0604009628296\n",
      "step: 2231.0, loss: 935.1792974472046\n",
      "step: 2232.0, loss: 935.4324493408203\n",
      "step: 2233.0, loss: 936.5208253860474\n",
      "step: 2234.0, loss: 935.878999710083\n",
      "step: 2235.0, loss: 935.7149820327759\n",
      "step: 2236.0, loss: 936.5885925292969\n",
      "step: 2237.0, loss: 935.5672674179077\n",
      "step: 2238.0, loss: 936.5201539993286\n",
      "step: 2239.0, loss: 937.9843826293945\n",
      "step: 2240.0, loss: 935.002444267273\n",
      "step: 2241.0, loss: 935.6174535751343\n",
      "step: 2242.0, loss: 933.9491844177246\n",
      "step: 2243.0, loss: 938.0524768829346\n",
      "step: 2244.0, loss: 937.1723947525024\n",
      "step: 2245.0, loss: 936.9083480834961\n",
      "step: 2246.0, loss: 935.8635311126709\n",
      "step: 2247.0, loss: 936.6026458740234\n",
      "step: 2248.0, loss: 934.6506462097168\n",
      "step: 2249.0, loss: 937.4370679855347\n",
      "step: 2250.0, loss: 936.9856176376343\n",
      "step: 2251.0, loss: 937.6598329544067\n",
      "step: 2252.0, loss: 935.766562461853\n",
      "step: 2253.0, loss: 936.9584054946899\n",
      "step: 2254.0, loss: 935.3202791213989\n",
      "step: 2255.0, loss: 935.6768484115601\n",
      "step: 2256.0, loss: 935.3999633789062\n",
      "step: 2257.0, loss: 934.7131099700928\n",
      "step: 2258.0, loss: 934.1611938476562\n",
      "step: 2259.0, loss: 937.7711477279663\n",
      "step: 2260.0, loss: 936.2481555938721\n",
      "step: 2261.0, loss: 935.5735597610474\n",
      "step: 2262.0, loss: 937.3855113983154\n",
      "step: 2263.0, loss: 936.3795156478882\n",
      "step: 2264.0, loss: 936.8226861953735\n",
      "step: 2265.0, loss: 935.1317358016968\n",
      "step: 2266.0, loss: 934.9114255905151\n",
      "step: 2267.0, loss: 936.0074319839478\n",
      "step: 2268.0, loss: 937.6191864013672\n",
      "step: 2269.0, loss: 935.4475831985474\n",
      "step: 2270.0, loss: 937.1275177001953\n",
      "step: 2271.0, loss: 936.6002435684204\n",
      "step: 2272.0, loss: 936.0934715270996\n",
      "step: 2273.0, loss: 935.9692192077637\n",
      "step: 2274.0, loss: 937.159873008728\n",
      "step: 2275.0, loss: 936.6216506958008\n",
      "step: 2276.0, loss: 936.5434446334839\n",
      "step: 2277.0, loss: 935.5168304443359\n",
      "step: 2278.0, loss: 935.1443643569946\n",
      "step: 2279.0, loss: 937.578046798706\n",
      "step: 2280.0, loss: 938.0234098434448\n",
      "step: 2281.0, loss: 936.0764665603638\n",
      "step: 2282.0, loss: 935.6247406005859\n",
      "step: 2283.0, loss: 937.5760974884033\n",
      "step: 2284.0, loss: 937.2778234481812\n",
      "step: 2285.0, loss: 934.6475276947021\n",
      "step: 2286.0, loss: 935.0079793930054\n",
      "step: 2287.0, loss: 936.1322154998779\n",
      "step: 2288.0, loss: 935.5476837158203\n",
      "step: 2289.0, loss: 937.7691984176636\n",
      "step: 2290.0, loss: 934.700831413269\n",
      "step: 2291.0, loss: 936.6821069717407\n",
      "step: 2292.0, loss: 935.2835130691528\n",
      "step: 2293.0, loss: 935.5486154556274\n",
      "step: 2294.0, loss: 935.7508220672607\n",
      "step: 2295.0, loss: 936.3662929534912\n",
      "step: 2296.0, loss: 937.8031558990479\n",
      "step: 2297.0, loss: 938.3165464401245\n",
      "step: 2298.0, loss: 936.8343143463135\n",
      "step: 2299.0, loss: 937.0885639190674\n",
      "step: 2300.0, loss: 936.3468742370605\n",
      "step: 2301.0, loss: 936.9586315155029\n",
      "step: 2302.0, loss: 935.6390495300293\n",
      "step: 2303.0, loss: 936.4311895370483\n",
      "step: 2304.0, loss: 934.5185289382935\n",
      "step: 2305.0, loss: 935.8764095306396\n",
      "step: 2306.0, loss: 935.6917476654053\n",
      "step: 2307.0, loss: 935.7197904586792\n",
      "step: 2308.0, loss: 936.0081977844238\n",
      "step: 2309.0, loss: 935.5481958389282\n",
      "step: 2310.0, loss: 935.8273973464966\n",
      "step: 2311.0, loss: 935.3211526870728\n",
      "step: 2312.0, loss: 934.1997938156128\n",
      "step: 2313.0, loss: 934.9455623626709\n",
      "step: 2314.0, loss: 935.1406707763672\n",
      "step: 2315.0, loss: 936.1714906692505\n",
      "step: 2316.0, loss: 937.0834951400757\n",
      "step: 2317.0, loss: 935.5982503890991\n",
      "step: 2318.0, loss: 938.1162376403809\n",
      "step: 2319.0, loss: 934.8702268600464\n",
      "step: 2320.0, loss: 935.6035890579224\n",
      "step: 2321.0, loss: 936.9443988800049\n",
      "step: 2322.0, loss: 935.7863082885742\n",
      "step: 2323.0, loss: 935.739896774292\n",
      "step: 2324.0, loss: 935.9001579284668\n",
      "step: 2325.0, loss: 935.169921875\n",
      "step: 2326.0, loss: 934.4646263122559\n",
      "step: 2327.0, loss: 937.0235795974731\n",
      "step: 2328.0, loss: 936.0607376098633\n",
      "step: 2329.0, loss: 936.3578281402588\n",
      "step: 2330.0, loss: 934.6059627532959\n",
      "step: 2331.0, loss: 937.0000457763672\n",
      "step: 2332.0, loss: 937.228853225708\n",
      "step: 2333.0, loss: 935.7333526611328\n",
      "step: 2334.0, loss: 935.71311378479\n",
      "step: 2335.0, loss: 935.2597408294678\n",
      "step: 2336.0, loss: 938.1301250457764\n",
      "step: 2337.0, loss: 935.8249835968018\n",
      "step: 2338.0, loss: 936.4925298690796\n",
      "step: 2339.0, loss: 936.5057983398438\n",
      "step: 2340.0, loss: 935.7449016571045\n",
      "step: 2341.0, loss: 935.871021270752\n",
      "step: 2342.0, loss: 934.4025211334229\n",
      "step: 2343.0, loss: 936.0278034210205\n",
      "step: 2344.0, loss: 936.5823163986206\n",
      "step: 2345.0, loss: 937.8729734420776\n",
      "step: 2346.0, loss: 934.038290977478\n",
      "step: 2347.0, loss: 936.7437229156494\n",
      "step: 2348.0, loss: 934.9803047180176\n",
      "step: 2349.0, loss: 936.373125076294\n",
      "step: 2350.0, loss: 937.1289825439453\n",
      "step: 2351.0, loss: 937.1387481689453\n",
      "step: 2352.0, loss: 935.5967473983765\n",
      "step: 2353.0, loss: 935.3438816070557\n",
      "step: 2354.0, loss: 937.4559726715088\n",
      "step: 2355.0, loss: 939.1893768310547\n",
      "step: 2356.0, loss: 935.7610368728638\n",
      "step: 2357.0, loss: 934.5900354385376\n",
      "step: 2358.0, loss: 935.3295965194702\n",
      "step: 2359.0, loss: 935.7710075378418\n",
      "step: 2360.0, loss: 937.461911201477\n",
      "step: 2361.0, loss: 933.7905340194702\n",
      "step: 2362.0, loss: 935.2480869293213\n",
      "step: 2363.0, loss: 937.0099401473999\n",
      "step: 2364.0, loss: 935.5721044540405\n",
      "step: 2365.0, loss: 935.596471786499\n",
      "step: 2366.0, loss: 935.6237478256226\n",
      "step: 2367.0, loss: 935.1366052627563\n",
      "step: 2368.0, loss: 935.2321310043335\n",
      "step: 2369.0, loss: 936.0401048660278\n",
      "step: 2370.0, loss: 936.5023107528687\n",
      "step: 2371.0, loss: 936.8414058685303\n",
      "step: 2372.0, loss: 935.6959657669067\n",
      "step: 2373.0, loss: 936.367338180542\n",
      "step: 2374.0, loss: 934.9327907562256\n",
      "step: 2375.0, loss: 936.2054090499878\n",
      "step: 2376.0, loss: 935.1185626983643\n",
      "step: 2377.0, loss: 934.7275447845459\n",
      "step: 2378.0, loss: 936.3081378936768\n",
      "step: 2379.0, loss: 933.0076732635498\n",
      "step: 2380.0, loss: 934.8494653701782\n",
      "step: 2381.0, loss: 935.4367456436157\n",
      "step: 2382.0, loss: 936.042085647583\n",
      "step: 2383.0, loss: 936.4090347290039\n",
      "step: 2384.0, loss: 935.694902420044\n",
      "step: 2385.0, loss: 935.3981504440308\n",
      "step: 2386.0, loss: 935.7354183197021\n",
      "step: 2387.0, loss: 936.9096336364746\n",
      "step: 2388.0, loss: 933.9724349975586\n",
      "step: 2389.0, loss: 935.2667980194092\n",
      "step: 2390.0, loss: 936.005784034729\n",
      "step: 2391.0, loss: 935.0909881591797\n",
      "step: 2392.0, loss: 934.5376682281494\n",
      "step: 2393.0, loss: 934.9763431549072\n",
      "step: 2394.0, loss: 935.7276706695557\n",
      "step: 2395.0, loss: 934.2163686752319\n",
      "step: 2396.0, loss: 936.1173601150513\n",
      "step: 2397.0, loss: 933.3154430389404\n",
      "step: 2398.0, loss: 935.520357131958\n",
      "step: 2399.0, loss: 935.9722585678101\n",
      "step: 2400.0, loss: 935.4063129425049\n",
      "step: 2401.0, loss: 935.0117473602295\n",
      "step: 2402.0, loss: 935.531623840332\n",
      "step: 2403.0, loss: 935.9518871307373\n",
      "step: 2404.0, loss: 934.4057369232178\n",
      "step: 2405.0, loss: 934.7932271957397\n",
      "step: 2406.0, loss: 935.9912939071655\n",
      "step: 2407.0, loss: 936.7026100158691\n",
      "step: 2408.0, loss: 935.0423965454102\n",
      "step: 2409.0, loss: 934.6674127578735\n",
      "step: 2410.0, loss: 936.1039943695068\n",
      "step: 2411.0, loss: 935.7170515060425\n",
      "step: 2412.0, loss: 935.0920505523682\n",
      "step: 2413.0, loss: 934.8352108001709\n",
      "step: 2414.0, loss: 935.3721733093262\n",
      "step: 2415.0, loss: 935.9165506362915\n",
      "step: 2416.0, loss: 935.1558313369751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2417.0, loss: 934.4697141647339\n",
      "step: 2418.0, loss: 936.6462383270264\n",
      "step: 2419.0, loss: 934.9831037521362\n",
      "step: 2420.0, loss: 935.1771516799927\n",
      "step: 2421.0, loss: 937.2724905014038\n",
      "step: 2422.0, loss: 935.958794593811\n",
      "step: 2423.0, loss: 935.0491008758545\n",
      "step: 2424.0, loss: 934.0579652786255\n",
      "step: 2425.0, loss: 933.6872463226318\n",
      "step: 2426.0, loss: 934.948127746582\n",
      "step: 2427.0, loss: 935.8576745986938\n",
      "step: 2428.0, loss: 934.3153085708618\n",
      "step: 2429.0, loss: 934.6751794815063\n",
      "step: 2430.0, loss: 936.5122690200806\n",
      "step: 2431.0, loss: 934.472936630249\n",
      "step: 2432.0, loss: 935.2442111968994\n",
      "step: 2433.0, loss: 936.0962238311768\n",
      "step: 2434.0, loss: 935.3274488449097\n",
      "step: 2435.0, loss: 934.3767738342285\n",
      "step: 2436.0, loss: 935.8089332580566\n",
      "step: 2437.0, loss: 933.7266502380371\n",
      "step: 2438.0, loss: 934.9649066925049\n",
      "step: 2439.0, loss: 935.7859945297241\n",
      "step: 2440.0, loss: 936.0210371017456\n",
      "step: 2441.0, loss: 935.9684448242188\n",
      "step: 2442.0, loss: 935.0599384307861\n",
      "step: 2443.0, loss: 935.0659923553467\n",
      "step: 2444.0, loss: 937.0885972976685\n",
      "step: 2445.0, loss: 935.0439653396606\n",
      "step: 2446.0, loss: 933.2834968566895\n",
      "step: 2447.0, loss: 935.4726934432983\n",
      "step: 2448.0, loss: 935.9488611221313\n",
      "step: 2449.0, loss: 934.258786201477\n",
      "step: 2450.0, loss: 933.3052635192871\n",
      "step: 2451.0, loss: 935.9396839141846\n",
      "step: 2452.0, loss: 935.8444204330444\n",
      "step: 2453.0, loss: 935.6728858947754\n",
      "step: 2454.0, loss: 935.54665184021\n",
      "step: 2455.0, loss: 935.6453189849854\n",
      "step: 2456.0, loss: 937.2365274429321\n",
      "step: 2457.0, loss: 935.3048410415649\n",
      "step: 2458.0, loss: 935.4323253631592\n",
      "step: 2459.0, loss: 934.2573728561401\n",
      "step: 2460.0, loss: 936.9285697937012\n",
      "step: 2461.0, loss: 933.4231729507446\n",
      "step: 2462.0, loss: 936.2295446395874\n",
      "step: 2463.0, loss: 936.6363515853882\n",
      "step: 2464.0, loss: 931.8722429275513\n",
      "step: 2465.0, loss: 933.7858238220215\n",
      "step: 2466.0, loss: 936.8582830429077\n",
      "step: 2467.0, loss: 934.0452194213867\n",
      "step: 2468.0, loss: 938.0271854400635\n",
      "step: 2469.0, loss: 935.4194173812866\n",
      "step: 2470.0, loss: 936.6693458557129\n",
      "step: 2471.0, loss: 933.5615043640137\n",
      "step: 2472.0, loss: 934.8232879638672\n",
      "step: 2473.0, loss: 935.177188873291\n",
      "step: 2474.0, loss: 935.9957332611084\n",
      "step: 2475.0, loss: 935.0301876068115\n",
      "step: 2476.0, loss: 933.7745885848999\n",
      "step: 2477.0, loss: 935.6734952926636\n",
      "step: 2478.0, loss: 934.3257942199707\n",
      "step: 2479.0, loss: 934.9292316436768\n",
      "step: 2480.0, loss: 932.992431640625\n",
      "step: 2481.0, loss: 936.7076635360718\n",
      "step: 2482.0, loss: 935.524468421936\n",
      "step: 2483.0, loss: 935.2650079727173\n",
      "step: 2484.0, loss: 935.7339057922363\n",
      "step: 2485.0, loss: 934.6257915496826\n",
      "step: 2486.0, loss: 935.7328491210938\n",
      "step: 2487.0, loss: 934.7167682647705\n",
      "step: 2488.0, loss: 933.4523544311523\n",
      "step: 2489.0, loss: 935.0770206451416\n",
      "step: 2490.0, loss: 933.7413368225098\n",
      "step: 2491.0, loss: 937.3857221603394\n",
      "step: 2492.0, loss: 934.9974231719971\n",
      "step: 2493.0, loss: 934.7839841842651\n",
      "step: 2494.0, loss: 934.8373956680298\n",
      "step: 2495.0, loss: 936.7441091537476\n",
      "step: 2496.0, loss: 934.9860019683838\n",
      "step: 2497.0, loss: 933.4387140274048\n",
      "step: 2498.0, loss: 936.3978338241577\n",
      "step: 2499.0, loss: 935.7610101699829\n",
      "step: 2500.0, loss: 933.4317007064819\n",
      "step: 2501.0, loss: 933.2241821289062\n",
      "step: 2502.0, loss: 936.4011459350586\n",
      "step: 2503.0, loss: 935.4361877441406\n",
      "step: 2504.0, loss: 934.6660585403442\n",
      "step: 2505.0, loss: 934.4991569519043\n",
      "step: 2506.0, loss: 934.4415893554688\n",
      "step: 2507.0, loss: 934.5678882598877\n",
      "step: 2508.0, loss: 933.1695985794067\n",
      "step: 2509.0, loss: 937.1369323730469\n",
      "step: 2510.0, loss: 934.8571348190308\n",
      "step: 2511.0, loss: 935.6449041366577\n",
      "step: 2512.0, loss: 935.0755786895752\n",
      "step: 2513.0, loss: 935.1212482452393\n",
      "step: 2514.0, loss: 935.3166131973267\n",
      "step: 2515.0, loss: 935.780478477478\n",
      "step: 2516.0, loss: 934.8618011474609\n",
      "step: 2517.0, loss: 935.0415420532227\n",
      "step: 2518.0, loss: 934.6639671325684\n",
      "step: 2519.0, loss: 935.974214553833\n",
      "step: 2520.0, loss: 934.5538005828857\n",
      "step: 2521.0, loss: 934.928243637085\n",
      "step: 2522.0, loss: 935.3593196868896\n",
      "step: 2523.0, loss: 935.4903945922852\n",
      "step: 2524.0, loss: 934.6298141479492\n",
      "step: 2525.0, loss: 934.5305185317993\n",
      "step: 2526.0, loss: 933.7704048156738\n",
      "step: 2527.0, loss: 935.5587844848633\n",
      "step: 2528.0, loss: 935.9680681228638\n",
      "step: 2529.0, loss: 934.9936370849609\n",
      "step: 2530.0, loss: 935.9072904586792\n",
      "step: 2531.0, loss: 936.1158618927002\n",
      "step: 2532.0, loss: 935.0441246032715\n",
      "step: 2533.0, loss: 935.0932540893555\n",
      "step: 2534.0, loss: 933.4235048294067\n",
      "step: 2535.0, loss: 934.6306552886963\n",
      "step: 2536.0, loss: 935.0703067779541\n",
      "step: 2537.0, loss: 934.4280128479004\n",
      "step: 2538.0, loss: 931.9914464950562\n",
      "step: 2539.0, loss: 934.4061489105225\n",
      "step: 2540.0, loss: 934.0043869018555\n",
      "step: 2541.0, loss: 936.3697462081909\n",
      "step: 2542.0, loss: 934.9915914535522\n",
      "step: 2543.0, loss: 934.284107208252\n",
      "step: 2544.0, loss: 934.6418304443359\n",
      "step: 2545.0, loss: 933.4731912612915\n",
      "step: 2546.0, loss: 935.040961265564\n",
      "step: 2547.0, loss: 935.6695833206177\n",
      "step: 2548.0, loss: 934.6327877044678\n",
      "step: 2549.0, loss: 932.8704643249512\n",
      "step: 2550.0, loss: 934.31077003479\n",
      "step: 2551.0, loss: 934.6767864227295\n",
      "step: 2552.0, loss: 935.4801597595215\n",
      "step: 2553.0, loss: 935.8647365570068\n",
      "step: 2554.0, loss: 934.5071306228638\n",
      "step: 2555.0, loss: 935.3427381515503\n",
      "step: 2556.0, loss: 933.5240306854248\n",
      "step: 2557.0, loss: 933.5112800598145\n",
      "step: 2558.0, loss: 934.0738859176636\n",
      "step: 2559.0, loss: 934.231162071228\n",
      "step: 2560.0, loss: 935.5568895339966\n",
      "step: 2561.0, loss: 934.7094240188599\n",
      "step: 2562.0, loss: 934.0362386703491\n",
      "step: 2563.0, loss: 934.1017465591431\n",
      "step: 2564.0, loss: 935.214864730835\n",
      "step: 2565.0, loss: 933.1007661819458\n",
      "step: 2566.0, loss: 935.0687503814697\n",
      "step: 2567.0, loss: 935.5142030715942\n",
      "step: 2568.0, loss: 933.593204498291\n",
      "step: 2569.0, loss: 935.6392087936401\n",
      "step: 2570.0, loss: 934.4700469970703\n",
      "step: 2571.0, loss: 933.6575469970703\n",
      "step: 2572.0, loss: 933.3397932052612\n",
      "step: 2573.0, loss: 933.7307767868042\n",
      "step: 2574.0, loss: 935.2215528488159\n",
      "step: 2575.0, loss: 933.2976188659668\n",
      "step: 2576.0, loss: 935.0506782531738\n",
      "step: 2577.0, loss: 934.1550388336182\n",
      "step: 2578.0, loss: 935.8281354904175\n",
      "step: 2579.0, loss: 934.2642412185669\n",
      "step: 2580.0, loss: 934.025707244873\n",
      "step: 2581.0, loss: 933.1974544525146\n",
      "step: 2582.0, loss: 934.2179136276245\n",
      "step: 2583.0, loss: 932.9397106170654\n",
      "step: 2584.0, loss: 936.104193687439\n",
      "step: 2585.0, loss: 936.437481880188\n",
      "step: 2586.0, loss: 932.5006580352783\n",
      "step: 2587.0, loss: 935.573187828064\n",
      "step: 2588.0, loss: 934.4111766815186\n",
      "step: 2589.0, loss: 934.1149730682373\n",
      "step: 2590.0, loss: 935.1641654968262\n",
      "step: 2591.0, loss: 936.2450504302979\n",
      "step: 2592.0, loss: 933.9595384597778\n",
      "step: 2593.0, loss: 935.0321578979492\n",
      "step: 2594.0, loss: 932.9848184585571\n",
      "step: 2595.0, loss: 935.8984079360962\n",
      "step: 2596.0, loss: 935.6402034759521\n",
      "step: 2597.0, loss: 936.6368541717529\n",
      "step: 2598.0, loss: 935.6776609420776\n",
      "step: 2599.0, loss: 933.7879524230957\n",
      "step: 2600.0, loss: 934.59974193573\n",
      "step: 2601.0, loss: 934.2434024810791\n",
      "step: 2602.0, loss: 936.4581661224365\n",
      "step: 2603.0, loss: 935.5960273742676\n",
      "step: 2604.0, loss: 931.4583702087402\n",
      "step: 2605.0, loss: 934.777063369751\n",
      "step: 2606.0, loss: 934.6918125152588\n",
      "step: 2607.0, loss: 934.5184268951416\n",
      "step: 2608.0, loss: 934.017050743103\n",
      "step: 2609.0, loss: 933.816990852356\n",
      "step: 2610.0, loss: 932.9234294891357\n",
      "step: 2611.0, loss: 935.1154947280884\n",
      "step: 2612.0, loss: 934.8544483184814\n",
      "step: 2613.0, loss: 934.4027585983276\n",
      "step: 2614.0, loss: 935.9601745605469\n",
      "step: 2615.0, loss: 935.9494781494141\n",
      "step: 2616.0, loss: 934.4603223800659\n",
      "step: 2617.0, loss: 935.7667007446289\n",
      "step: 2618.0, loss: 932.7458591461182\n",
      "step: 2619.0, loss: 932.7968997955322\n",
      "step: 2620.0, loss: 933.4986324310303\n",
      "step: 2621.0, loss: 935.3665580749512\n",
      "step: 2622.0, loss: 935.5793876647949\n",
      "step: 2623.0, loss: 933.9505815505981\n",
      "step: 2624.0, loss: 936.2463359832764\n",
      "step: 2625.0, loss: 934.0761280059814\n",
      "step: 2626.0, loss: 932.9798994064331\n",
      "step: 2627.0, loss: 933.0831279754639\n",
      "step: 2628.0, loss: 936.0447664260864\n",
      "step: 2629.0, loss: 934.0656986236572\n",
      "step: 2630.0, loss: 934.3843851089478\n",
      "step: 2631.0, loss: 933.1155776977539\n",
      "step: 2632.0, loss: 934.0013751983643\n",
      "step: 2633.0, loss: 933.1581811904907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2634.0, loss: 934.3588027954102\n",
      "step: 2635.0, loss: 933.0270557403564\n",
      "step: 2636.0, loss: 933.2917032241821\n",
      "step: 2637.0, loss: 934.3446359634399\n",
      "step: 2638.0, loss: 934.9290933609009\n",
      "step: 2639.0, loss: 934.8968706130981\n",
      "step: 2640.0, loss: 932.2770481109619\n",
      "step: 2641.0, loss: 934.6261739730835\n",
      "step: 2642.0, loss: 936.2178945541382\n",
      "step: 2643.0, loss: 934.5002479553223\n",
      "step: 2644.0, loss: 933.4315357208252\n",
      "step: 2645.0, loss: 933.5829963684082\n",
      "step: 2646.0, loss: 934.4740552902222\n",
      "step: 2647.0, loss: 934.7699356079102\n",
      "step: 2648.0, loss: 935.5895986557007\n",
      "step: 2649.0, loss: 932.8310861587524\n",
      "step: 2650.0, loss: 935.2654857635498\n",
      "step: 2651.0, loss: 934.9812240600586\n",
      "step: 2652.0, loss: 934.8908138275146\n",
      "step: 2653.0, loss: 934.6364507675171\n",
      "step: 2654.0, loss: 934.5891342163086\n",
      "step: 2655.0, loss: 933.8985586166382\n",
      "step: 2656.0, loss: 935.5271968841553\n",
      "step: 2657.0, loss: 934.8487682342529\n",
      "step: 2658.0, loss: 933.4266576766968\n",
      "step: 2659.0, loss: 932.5295782089233\n",
      "step: 2660.0, loss: 934.196798324585\n",
      "step: 2661.0, loss: 935.0463056564331\n",
      "step: 2662.0, loss: 933.0813884735107\n",
      "step: 2663.0, loss: 936.085280418396\n",
      "step: 2664.0, loss: 934.3191652297974\n",
      "step: 2665.0, loss: 934.9548215866089\n",
      "step: 2666.0, loss: 933.2278804779053\n",
      "step: 2667.0, loss: 935.8639240264893\n",
      "step: 2668.0, loss: 933.8232545852661\n",
      "step: 2669.0, loss: 934.2567796707153\n",
      "step: 2670.0, loss: 934.7558183670044\n",
      "step: 2671.0, loss: 934.1131572723389\n",
      "step: 2672.0, loss: 933.5439348220825\n",
      "step: 2673.0, loss: 935.1435775756836\n",
      "step: 2674.0, loss: 936.3678512573242\n",
      "step: 2675.0, loss: 932.8506546020508\n",
      "step: 2676.0, loss: 934.6077747344971\n",
      "step: 2677.0, loss: 933.1778173446655\n",
      "step: 2678.0, loss: 935.6428480148315\n",
      "step: 2679.0, loss: 933.9863300323486\n",
      "step: 2680.0, loss: 935.2417316436768\n",
      "step: 2681.0, loss: 934.8938465118408\n",
      "step: 2682.0, loss: 933.6025066375732\n",
      "step: 2683.0, loss: 933.4605770111084\n",
      "step: 2684.0, loss: 933.0865468978882\n",
      "step: 2685.0, loss: 934.641339302063\n",
      "step: 2686.0, loss: 933.4259243011475\n",
      "step: 2687.0, loss: 933.4000787734985\n",
      "step: 2688.0, loss: 934.067024230957\n",
      "step: 2689.0, loss: 934.4207372665405\n",
      "step: 2690.0, loss: 934.3876571655273\n",
      "step: 2691.0, loss: 934.0573415756226\n",
      "step: 2692.0, loss: 935.4484844207764\n",
      "step: 2693.0, loss: 933.2912368774414\n",
      "step: 2694.0, loss: 934.0952777862549\n",
      "step: 2695.0, loss: 933.304178237915\n",
      "step: 2696.0, loss: 934.0648736953735\n",
      "step: 2697.0, loss: 934.581524848938\n",
      "step: 2698.0, loss: 933.2937335968018\n",
      "step: 2699.0, loss: 932.8037786483765\n",
      "step: 2700.0, loss: 933.3787260055542\n",
      "step: 2701.0, loss: 933.940336227417\n",
      "step: 2702.0, loss: 935.0587272644043\n",
      "step: 2703.0, loss: 934.7158050537109\n",
      "step: 2704.0, loss: 934.6822576522827\n",
      "step: 2705.0, loss: 933.1426334381104\n",
      "step: 2706.0, loss: 935.8332767486572\n",
      "step: 2707.0, loss: 934.3294563293457\n",
      "step: 2708.0, loss: 934.0108242034912\n",
      "step: 2709.0, loss: 933.3419761657715\n",
      "step: 2710.0, loss: 933.3790979385376\n",
      "step: 2711.0, loss: 934.0565757751465\n",
      "step: 2712.0, loss: 933.8651580810547\n",
      "step: 2713.0, loss: 934.4618988037109\n",
      "step: 2714.0, loss: 933.7309970855713\n",
      "step: 2715.0, loss: 932.5526008605957\n",
      "step: 2716.0, loss: 934.7514457702637\n",
      "step: 2717.0, loss: 934.0014238357544\n",
      "step: 2718.0, loss: 935.016695022583\n",
      "step: 2719.0, loss: 934.7663946151733\n",
      "step: 2720.0, loss: 934.583514213562\n",
      "step: 2721.0, loss: 933.65163230896\n",
      "step: 2722.0, loss: 934.6190185546875\n",
      "step: 2723.0, loss: 933.3809747695923\n",
      "step: 2724.0, loss: 933.5633707046509\n",
      "step: 2725.0, loss: 933.7385740280151\n",
      "step: 2726.0, loss: 935.5557661056519\n",
      "step: 2727.0, loss: 934.4881582260132\n",
      "step: 2728.0, loss: 931.7769737243652\n",
      "step: 2729.0, loss: 933.2192630767822\n",
      "step: 2730.0, loss: 933.8785305023193\n",
      "step: 2731.0, loss: 933.6519899368286\n",
      "step: 2732.0, loss: 934.2620296478271\n",
      "step: 2733.0, loss: 933.9830942153931\n",
      "step: 2734.0, loss: 933.1330146789551\n",
      "step: 2735.0, loss: 935.5679998397827\n",
      "step: 2736.0, loss: 933.5109310150146\n",
      "step: 2737.0, loss: 935.4973058700562\n",
      "step: 2738.0, loss: 934.091064453125\n",
      "step: 2739.0, loss: 932.9813203811646\n",
      "step: 2740.0, loss: 935.2102489471436\n",
      "step: 2741.0, loss: 933.599461555481\n",
      "step: 2742.0, loss: 934.2110271453857\n",
      "step: 2743.0, loss: 932.137936592102\n",
      "step: 2744.0, loss: 934.993818283081\n",
      "step: 2745.0, loss: 934.7513341903687\n",
      "step: 2746.0, loss: 932.8977460861206\n",
      "step: 2747.0, loss: 934.4085445404053\n",
      "step: 2748.0, loss: 933.3933563232422\n",
      "step: 2749.0, loss: 935.5676860809326\n",
      "step: 2750.0, loss: 934.1832284927368\n",
      "step: 2751.0, loss: 933.4295606613159\n",
      "step: 2752.0, loss: 931.8389148712158\n",
      "step: 2753.0, loss: 932.4848165512085\n",
      "step: 2754.0, loss: 933.2125120162964\n",
      "step: 2755.0, loss: 933.6535034179688\n",
      "step: 2756.0, loss: 933.1636972427368\n",
      "step: 2757.0, loss: 932.8418798446655\n",
      "step: 2758.0, loss: 933.9635105133057\n",
      "step: 2759.0, loss: 934.1337757110596\n",
      "step: 2760.0, loss: 934.4675855636597\n",
      "step: 2761.0, loss: 933.5381097793579\n",
      "step: 2762.0, loss: 933.1109323501587\n",
      "step: 2763.0, loss: 934.4683694839478\n",
      "step: 2764.0, loss: 932.5353918075562\n",
      "step: 2765.0, loss: 932.151782989502\n",
      "step: 2766.0, loss: 933.675329208374\n",
      "step: 2767.0, loss: 933.5768756866455\n",
      "step: 2768.0, loss: 933.5503816604614\n",
      "step: 2769.0, loss: 935.1201839447021\n",
      "step: 2770.0, loss: 935.5507707595825\n",
      "step: 2771.0, loss: 936.4012699127197\n",
      "step: 2772.0, loss: 934.3437919616699\n",
      "step: 2773.0, loss: 933.2565755844116\n",
      "step: 2774.0, loss: 934.1144113540649\n",
      "step: 2775.0, loss: 932.197283744812\n",
      "step: 2776.0, loss: 932.9815616607666\n",
      "step: 2777.0, loss: 934.7288179397583\n",
      "step: 2778.0, loss: 934.4700031280518\n",
      "step: 2779.0, loss: 935.3615818023682\n",
      "step: 2780.0, loss: 935.3205699920654\n",
      "step: 2781.0, loss: 932.6568584442139\n",
      "step: 2782.0, loss: 934.3861455917358\n",
      "step: 2783.0, loss: 931.6399431228638\n",
      "step: 2784.0, loss: 934.554404258728\n",
      "step: 2785.0, loss: 934.9350709915161\n",
      "step: 2786.0, loss: 932.3138608932495\n",
      "step: 2787.0, loss: 932.7018918991089\n",
      "step: 2788.0, loss: 934.6068859100342\n",
      "step: 2789.0, loss: 932.1194429397583\n",
      "step: 2790.0, loss: 931.4033737182617\n",
      "step: 2791.0, loss: 933.8896389007568\n",
      "step: 2792.0, loss: 931.8898963928223\n",
      "step: 2793.0, loss: 931.1990938186646\n",
      "step: 2794.0, loss: 933.5448198318481\n",
      "step: 2795.0, loss: 933.9678421020508\n",
      "step: 2796.0, loss: 932.6903829574585\n",
      "step: 2797.0, loss: 934.698878288269\n",
      "step: 2798.0, loss: 934.5936594009399\n",
      "step: 2799.0, loss: 933.6514234542847\n",
      "step: 2800.0, loss: 933.4545497894287\n",
      "step: 2801.0, loss: 933.7236957550049\n",
      "step: 2802.0, loss: 933.3601961135864\n",
      "step: 2803.0, loss: 932.7067832946777\n",
      "step: 2804.0, loss: 934.0521669387817\n",
      "step: 2805.0, loss: 932.9111261367798\n",
      "step: 2806.0, loss: 933.9040384292603\n",
      "step: 2807.0, loss: 933.7680931091309\n",
      "step: 2808.0, loss: 936.0660057067871\n",
      "step: 2809.0, loss: 933.2587299346924\n",
      "step: 2810.0, loss: 934.338210105896\n",
      "step: 2811.0, loss: 933.3729972839355\n",
      "step: 2812.0, loss: 933.5575342178345\n",
      "step: 2813.0, loss: 933.7090797424316\n",
      "step: 2814.0, loss: 933.3096494674683\n",
      "step: 2815.0, loss: 933.8572101593018\n",
      "step: 2816.0, loss: 934.4512319564819\n",
      "step: 2817.0, loss: 932.7360677719116\n",
      "step: 2818.0, loss: 934.3030071258545\n",
      "step: 2819.0, loss: 933.1407499313354\n",
      "step: 2820.0, loss: 934.2866840362549\n",
      "step: 2821.0, loss: 933.6687288284302\n",
      "step: 2822.0, loss: 934.3436393737793\n",
      "step: 2823.0, loss: 933.8812532424927\n",
      "step: 2824.0, loss: 933.0309562683105\n",
      "step: 2825.0, loss: 933.3521022796631\n",
      "step: 2826.0, loss: 934.2568607330322\n",
      "step: 2827.0, loss: 931.4968500137329\n",
      "step: 2828.0, loss: 934.8088541030884\n",
      "step: 2829.0, loss: 933.6095943450928\n",
      "step: 2830.0, loss: 934.2529592514038\n",
      "step: 2831.0, loss: 934.2945938110352\n",
      "step: 2832.0, loss: 933.580662727356\n",
      "step: 2833.0, loss: 934.1772975921631\n",
      "step: 2834.0, loss: 932.5249786376953\n",
      "step: 2835.0, loss: 933.8589315414429\n",
      "step: 2836.0, loss: 931.7944622039795\n",
      "step: 2837.0, loss: 933.1742811203003\n",
      "step: 2838.0, loss: 935.5651435852051\n",
      "step: 2839.0, loss: 934.4253578186035\n",
      "step: 2840.0, loss: 934.1707973480225\n",
      "step: 2841.0, loss: 934.5544452667236\n",
      "step: 2842.0, loss: 932.7774515151978\n",
      "step: 2843.0, loss: 934.0209264755249\n",
      "step: 2844.0, loss: 935.4418535232544\n",
      "step: 2845.0, loss: 932.3855972290039\n",
      "step: 2846.0, loss: 934.5618467330933\n",
      "step: 2847.0, loss: 931.8269081115723\n",
      "step: 2848.0, loss: 933.5059595108032\n",
      "step: 2849.0, loss: 932.112401008606\n",
      "step: 2850.0, loss: 933.4032583236694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2851.0, loss: 933.5920963287354\n",
      "step: 2852.0, loss: 932.6431932449341\n",
      "step: 2853.0, loss: 933.3189725875854\n",
      "step: 2854.0, loss: 932.0446538925171\n",
      "step: 2855.0, loss: 932.7365055084229\n",
      "step: 2856.0, loss: 932.5995922088623\n",
      "step: 2857.0, loss: 930.4834518432617\n",
      "step: 2858.0, loss: 932.8815050125122\n",
      "step: 2859.0, loss: 933.211049079895\n",
      "step: 2860.0, loss: 934.5097236633301\n",
      "step: 2861.0, loss: 932.6766958236694\n",
      "step: 2862.0, loss: 932.6852436065674\n",
      "step: 2863.0, loss: 934.0045499801636\n",
      "step: 2864.0, loss: 930.8081045150757\n",
      "step: 2865.0, loss: 933.8530855178833\n",
      "step: 2866.0, loss: 934.9052639007568\n",
      "step: 2867.0, loss: 934.5434427261353\n",
      "step: 2868.0, loss: 933.7998371124268\n",
      "step: 2869.0, loss: 933.1909589767456\n",
      "step: 2870.0, loss: 933.5907859802246\n",
      "step: 2871.0, loss: 933.0205812454224\n",
      "step: 2872.0, loss: 933.9000387191772\n",
      "step: 2873.0, loss: 932.1073570251465\n",
      "step: 2874.0, loss: 931.1006298065186\n",
      "step: 2875.0, loss: 933.9249000549316\n",
      "step: 2876.0, loss: 932.5892848968506\n",
      "step: 2877.0, loss: 933.2421207427979\n",
      "step: 2878.0, loss: 934.2650375366211\n",
      "step: 2879.0, loss: 932.6968402862549\n",
      "step: 2880.0, loss: 934.7700729370117\n",
      "step: 2881.0, loss: 935.0927200317383\n",
      "step: 2882.0, loss: 931.8342666625977\n",
      "step: 2883.0, loss: 935.1406316757202\n",
      "step: 2884.0, loss: 932.7059459686279\n",
      "step: 2885.0, loss: 933.846755027771\n",
      "step: 2886.0, loss: 935.3267621994019\n",
      "step: 2887.0, loss: 933.2071809768677\n",
      "step: 2888.0, loss: 932.6153984069824\n",
      "step: 2889.0, loss: 932.4517126083374\n",
      "step: 2890.0, loss: 934.9869031906128\n",
      "step: 2891.0, loss: 934.6858024597168\n",
      "step: 2892.0, loss: 932.7062273025513\n",
      "step: 2893.0, loss: 933.0517015457153\n",
      "step: 2894.0, loss: 933.8592109680176\n",
      "step: 2895.0, loss: 932.2911586761475\n",
      "step: 2896.0, loss: 933.637677192688\n",
      "step: 2897.0, loss: 933.6448707580566\n",
      "step: 2898.0, loss: 933.3263940811157\n",
      "step: 2899.0, loss: 933.3685493469238\n",
      "step: 2900.0, loss: 933.5623245239258\n",
      "step: 2901.0, loss: 932.2524366378784\n",
      "step: 2902.0, loss: 934.0883741378784\n",
      "step: 2903.0, loss: 933.6670694351196\n",
      "step: 2904.0, loss: 932.9820785522461\n",
      "step: 2905.0, loss: 934.1762170791626\n",
      "step: 2906.0, loss: 931.8827857971191\n",
      "step: 2907.0, loss: 933.1952676773071\n",
      "step: 2908.0, loss: 933.2707071304321\n",
      "step: 2909.0, loss: 933.5747575759888\n",
      "step: 2910.0, loss: 933.823392868042\n",
      "step: 2911.0, loss: 932.6012411117554\n",
      "step: 2912.0, loss: 933.3718032836914\n",
      "step: 2913.0, loss: 933.7730922698975\n",
      "step: 2914.0, loss: 933.3352241516113\n",
      "step: 2915.0, loss: 933.760383605957\n",
      "step: 2916.0, loss: 933.2396640777588\n",
      "step: 2917.0, loss: 932.7150011062622\n",
      "step: 2918.0, loss: 931.740481376648\n",
      "step: 2919.0, loss: 932.9143781661987\n",
      "step: 2920.0, loss: 932.9381418228149\n",
      "step: 2921.0, loss: 933.8401508331299\n",
      "step: 2922.0, loss: 935.4348955154419\n",
      "step: 2923.0, loss: 934.2736902236938\n",
      "step: 2924.0, loss: 934.2464656829834\n",
      "step: 2925.0, loss: 933.1472263336182\n",
      "step: 2926.0, loss: 930.6978511810303\n",
      "step: 2927.0, loss: 932.1403217315674\n",
      "step: 2928.0, loss: 933.2531290054321\n",
      "step: 2929.0, loss: 931.8456287384033\n",
      "step: 2930.0, loss: 932.3414783477783\n",
      "step: 2931.0, loss: 935.3786058425903\n",
      "step: 2932.0, loss: 932.5094528198242\n",
      "step: 2933.0, loss: 933.5587825775146\n",
      "step: 2934.0, loss: 932.482873916626\n",
      "step: 2935.0, loss: 932.9298810958862\n",
      "step: 2936.0, loss: 933.4926080703735\n",
      "step: 2937.0, loss: 933.3391103744507\n",
      "step: 2938.0, loss: 932.4452791213989\n",
      "step: 2939.0, loss: 932.9927959442139\n",
      "step: 2940.0, loss: 932.3297281265259\n",
      "step: 2941.0, loss: 933.4826049804688\n",
      "step: 2942.0, loss: 932.7277059555054\n",
      "step: 2943.0, loss: 933.9785194396973\n",
      "step: 2944.0, loss: 933.7016115188599\n",
      "step: 2945.0, loss: 934.6086435317993\n",
      "step: 2946.0, loss: 931.5584487915039\n",
      "step: 2947.0, loss: 933.6612749099731\n",
      "step: 2948.0, loss: 932.8966455459595\n",
      "step: 2949.0, loss: 934.4199829101562\n",
      "step: 2950.0, loss: 933.0849285125732\n",
      "step: 2951.0, loss: 933.8134574890137\n",
      "step: 2952.0, loss: 933.1090641021729\n",
      "step: 2953.0, loss: 930.6754179000854\n",
      "step: 2954.0, loss: 931.4984979629517\n",
      "step: 2955.0, loss: 934.9256639480591\n",
      "step: 2956.0, loss: 931.8415546417236\n",
      "step: 2957.0, loss: 933.5792942047119\n",
      "step: 2958.0, loss: 932.6347789764404\n",
      "step: 2959.0, loss: 933.0109300613403\n",
      "step: 2960.0, loss: 933.197961807251\n",
      "step: 2961.0, loss: 932.0545883178711\n",
      "step: 2962.0, loss: 932.0068969726562\n",
      "step: 2963.0, loss: 933.723536491394\n",
      "step: 2964.0, loss: 932.5914697647095\n",
      "step: 2965.0, loss: 935.0614938735962\n",
      "step: 2966.0, loss: 933.282205581665\n",
      "step: 2967.0, loss: 933.5019702911377\n",
      "step: 2968.0, loss: 934.6712732315063\n",
      "step: 2969.0, loss: 935.376971244812\n",
      "step: 2970.0, loss: 932.4258441925049\n",
      "step: 2971.0, loss: 932.9930582046509\n",
      "step: 2972.0, loss: 931.0935964584351\n",
      "step: 2973.0, loss: 934.428017616272\n",
      "step: 2974.0, loss: 932.7996988296509\n",
      "step: 2975.0, loss: 933.1846036911011\n",
      "step: 2976.0, loss: 933.4888792037964\n",
      "step: 2977.0, loss: 932.6829891204834\n",
      "step: 2978.0, loss: 934.0387859344482\n",
      "step: 2979.0, loss: 933.0939702987671\n",
      "step: 2980.0, loss: 932.9510021209717\n",
      "step: 2981.0, loss: 932.2686243057251\n",
      "step: 2982.0, loss: 933.6190614700317\n",
      "step: 2983.0, loss: 933.2231464385986\n",
      "step: 2984.0, loss: 931.9228801727295\n",
      "step: 2985.0, loss: 932.3223333358765\n",
      "step: 2986.0, loss: 932.2349720001221\n",
      "step: 2987.0, loss: 933.1808204650879\n",
      "step: 2988.0, loss: 933.9931221008301\n",
      "step: 2989.0, loss: 932.6815147399902\n",
      "step: 2990.0, loss: 934.2221345901489\n",
      "step: 2991.0, loss: 933.7172594070435\n",
      "step: 2992.0, loss: 933.8515872955322\n",
      "step: 2993.0, loss: 934.2124643325806\n",
      "step: 2994.0, loss: 934.110029220581\n",
      "step: 2995.0, loss: 933.4333229064941\n",
      "step: 2996.0, loss: 932.3995485305786\n",
      "step: 2997.0, loss: 932.8967342376709\n",
      "step: 2998.0, loss: 931.8304109573364\n",
      "step: 2999.0, loss: 932.6212873458862\n",
      "step: 3000.0, loss: 932.9436416625977\n",
      "step: 3001.0, loss: 932.4822092056274\n",
      "step: 3002.0, loss: 933.406286239624\n",
      "step: 3003.0, loss: 932.6723785400391\n",
      "step: 3004.0, loss: 934.1153287887573\n",
      "step: 3005.0, loss: 934.1393661499023\n",
      "step: 3006.0, loss: 933.9217901229858\n",
      "step: 3007.0, loss: 933.824462890625\n",
      "step: 3008.0, loss: 933.2612590789795\n",
      "step: 3009.0, loss: 933.6194486618042\n",
      "step: 3010.0, loss: 933.9701700210571\n",
      "step: 3011.0, loss: 933.3093137741089\n",
      "step: 3012.0, loss: 933.5409116744995\n",
      "step: 3013.0, loss: 935.645016670227\n",
      "step: 3014.0, loss: 932.3006324768066\n",
      "step: 3015.0, loss: 931.158278465271\n",
      "step: 3016.0, loss: 933.033881187439\n",
      "step: 3017.0, loss: 930.7803907394409\n",
      "step: 3018.0, loss: 933.8177375793457\n",
      "step: 3019.0, loss: 932.9973077774048\n",
      "step: 3020.0, loss: 932.8704452514648\n",
      "step: 3021.0, loss: 934.2621650695801\n",
      "step: 3022.0, loss: 932.3445491790771\n",
      "step: 3023.0, loss: 932.4098863601685\n",
      "step: 3024.0, loss: 931.3653573989868\n",
      "step: 3025.0, loss: 932.670576095581\n",
      "step: 3026.0, loss: 932.7714881896973\n",
      "step: 3027.0, loss: 934.750864982605\n",
      "step: 3028.0, loss: 932.1009073257446\n",
      "step: 3029.0, loss: 933.9737319946289\n",
      "step: 3030.0, loss: 933.0614728927612\n",
      "step: 3031.0, loss: 933.5591249465942\n",
      "step: 3032.0, loss: 932.5611209869385\n",
      "step: 3033.0, loss: 932.8941631317139\n",
      "step: 3034.0, loss: 933.3931875228882\n",
      "step: 3035.0, loss: 932.0876312255859\n",
      "step: 3036.0, loss: 932.7750606536865\n",
      "step: 3037.0, loss: 933.4890594482422\n",
      "step: 3038.0, loss: 931.6361865997314\n",
      "step: 3039.0, loss: 931.4645690917969\n",
      "step: 3040.0, loss: 932.156397819519\n",
      "step: 3041.0, loss: 931.988133430481\n",
      "step: 3042.0, loss: 932.9651107788086\n",
      "step: 3043.0, loss: 930.7695140838623\n",
      "step: 3044.0, loss: 932.5070667266846\n",
      "step: 3045.0, loss: 931.9476366043091\n",
      "step: 3046.0, loss: 931.2345676422119\n",
      "step: 3047.0, loss: 933.1483936309814\n",
      "step: 3048.0, loss: 930.3806638717651\n",
      "step: 3049.0, loss: 933.4031524658203\n",
      "step: 3050.0, loss: 931.6886720657349\n",
      "step: 3051.0, loss: 931.2534456253052\n",
      "step: 3052.0, loss: 930.522310256958\n",
      "step: 3053.0, loss: 933.8069953918457\n",
      "step: 3054.0, loss: 934.7020721435547\n",
      "step: 3055.0, loss: 931.171627998352\n",
      "step: 3056.0, loss: 932.7888174057007\n",
      "step: 3057.0, loss: 932.5675115585327\n",
      "step: 3058.0, loss: 931.9749412536621\n",
      "step: 3059.0, loss: 932.8486499786377\n",
      "step: 3060.0, loss: 933.073317527771\n",
      "step: 3061.0, loss: 934.6350002288818\n",
      "step: 3062.0, loss: 933.3059349060059\n",
      "step: 3063.0, loss: 933.4721517562866\n",
      "step: 3064.0, loss: 932.1604127883911\n",
      "step: 3065.0, loss: 934.6357669830322\n",
      "step: 3066.0, loss: 931.9379205703735\n",
      "step: 3067.0, loss: 932.3798847198486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 3068.0, loss: 933.347222328186\n",
      "step: 3069.0, loss: 931.930757522583\n",
      "step: 3070.0, loss: 933.099531173706\n",
      "step: 3071.0, loss: 933.8819065093994\n",
      "step: 3072.0, loss: 933.226634979248\n",
      "step: 3073.0, loss: 934.5516109466553\n",
      "step: 3074.0, loss: 931.8333053588867\n",
      "step: 3075.0, loss: 933.5544500350952\n",
      "step: 3076.0, loss: 933.8000068664551\n",
      "step: 3077.0, loss: 933.2422437667847\n",
      "step: 3078.0, loss: 930.9967823028564\n",
      "step: 3079.0, loss: 932.8048267364502\n",
      "step: 3080.0, loss: 932.7478113174438\n",
      "step: 3081.0, loss: 932.1985788345337\n",
      "step: 3082.0, loss: 933.1841220855713\n",
      "step: 3083.0, loss: 930.8227005004883\n",
      "step: 3084.0, loss: 932.9833469390869\n",
      "step: 3085.0, loss: 931.6265487670898\n",
      "step: 3086.0, loss: 933.5678224563599\n",
      "step: 3087.0, loss: 931.5357351303101\n",
      "step: 3088.0, loss: 931.8658018112183\n",
      "step: 3089.0, loss: 933.1269826889038\n",
      "step: 3090.0, loss: 932.28076171875\n",
      "step: 3091.0, loss: 931.8474225997925\n",
      "step: 3092.0, loss: 933.0799627304077\n",
      "step: 3093.0, loss: 932.4896583557129\n",
      "step: 3094.0, loss: 932.1638517379761\n",
      "step: 3095.0, loss: 933.2272262573242\n",
      "step: 3096.0, loss: 933.2366285324097\n",
      "step: 3097.0, loss: 932.875376701355\n",
      "step: 3098.0, loss: 931.6653108596802\n",
      "step: 3099.0, loss: 934.9228410720825\n",
      "step: 3100.0, loss: 932.0336771011353\n",
      "step: 3101.0, loss: 932.2499961853027\n",
      "step: 3102.0, loss: 933.8245277404785\n",
      "step: 3103.0, loss: 933.2686491012573\n",
      "step: 3104.0, loss: 932.3202171325684\n",
      "step: 3105.0, loss: 931.6159563064575\n",
      "step: 3106.0, loss: 931.6201944351196\n",
      "step: 3107.0, loss: 931.200870513916\n",
      "step: 3108.0, loss: 932.1388025283813\n",
      "step: 3109.0, loss: 932.0694999694824\n",
      "step: 3110.0, loss: 933.5281038284302\n",
      "step: 3111.0, loss: 932.1679096221924\n",
      "step: 3112.0, loss: 934.303708076477\n",
      "step: 3113.0, loss: 931.1662540435791\n",
      "step: 3114.0, loss: 931.7954835891724\n",
      "step: 3115.0, loss: 933.3408250808716\n",
      "step: 3116.0, loss: 931.9149465560913\n",
      "step: 3117.0, loss: 934.1672134399414\n",
      "step: 3118.0, loss: 931.9996881484985\n",
      "step: 3119.0, loss: 932.9466943740845\n",
      "step: 3120.0, loss: 930.4065275192261\n",
      "step: 3121.0, loss: 933.5577020645142\n",
      "step: 3122.0, loss: 933.4974279403687\n",
      "step: 3123.0, loss: 933.3792448043823\n",
      "step: 3124.0, loss: 932.5849618911743\n",
      "step: 3125.0, loss: 933.5861988067627\n",
      "step: 3126.0, loss: 932.0398483276367\n",
      "step: 3127.0, loss: 934.3268909454346\n",
      "step: 3128.0, loss: 931.8559656143188\n",
      "step: 3129.0, loss: 932.349175453186\n",
      "step: 3130.0, loss: 932.7286233901978\n",
      "step: 3131.0, loss: 931.0240812301636\n",
      "step: 3132.0, loss: 931.8414869308472\n",
      "step: 3133.0, loss: 932.3688125610352\n",
      "step: 3134.0, loss: 932.3851375579834\n",
      "step: 3135.0, loss: 932.1747894287109\n",
      "step: 3136.0, loss: 932.1342926025391\n",
      "step: 3137.0, loss: 930.6891098022461\n",
      "step: 3138.0, loss: 930.2033615112305\n",
      "step: 3139.0, loss: 934.6191835403442\n",
      "step: 3140.0, loss: 931.1613998413086\n",
      "step: 3141.0, loss: 933.3746299743652\n",
      "step: 3142.0, loss: 932.6908102035522\n",
      "step: 3143.0, loss: 932.1046800613403\n",
      "step: 3144.0, loss: 930.4229230880737\n",
      "step: 3145.0, loss: 932.8181018829346\n",
      "step: 3146.0, loss: 934.3230905532837\n",
      "step: 3147.0, loss: 932.7210988998413\n",
      "step: 3148.0, loss: 931.537675857544\n",
      "step: 3149.0, loss: 932.0377702713013\n",
      "step: 3150.0, loss: 931.7584228515625\n",
      "step: 3151.0, loss: 933.0261125564575\n",
      "step: 3152.0, loss: 932.6539554595947\n",
      "step: 3153.0, loss: 933.0067319869995\n",
      "step: 3154.0, loss: 933.3298511505127\n",
      "step: 3155.0, loss: 932.1514110565186\n",
      "step: 3156.0, loss: 932.9937076568604\n",
      "step: 3157.0, loss: 931.7300519943237\n",
      "step: 3158.0, loss: 932.4707155227661\n",
      "step: 3159.0, loss: 932.2969446182251\n",
      "step: 3160.0, loss: 931.6551160812378\n",
      "step: 3161.0, loss: 932.5132989883423\n",
      "step: 3162.0, loss: 931.8215026855469\n",
      "step: 3163.0, loss: 932.8305292129517\n",
      "step: 3164.0, loss: 931.5032234191895\n",
      "step: 3165.0, loss: 933.6714019775391\n",
      "step: 3166.0, loss: 934.2741441726685\n",
      "step: 3167.0, loss: 933.2393426895142\n",
      "step: 3168.0, loss: 932.8855333328247\n",
      "step: 3169.0, loss: 930.2777423858643\n",
      "step: 3170.0, loss: 932.2834310531616\n",
      "step: 3171.0, loss: 931.6837015151978\n",
      "step: 3172.0, loss: 931.9787673950195\n",
      "step: 3173.0, loss: 933.2221078872681\n",
      "step: 3174.0, loss: 931.1766872406006\n",
      "step: 3175.0, loss: 932.1018505096436\n",
      "step: 3176.0, loss: 932.4489936828613\n",
      "step: 3177.0, loss: 931.7951574325562\n",
      "step: 3178.0, loss: 932.6633491516113\n",
      "step: 3179.0, loss: 930.222393989563\n",
      "step: 3180.0, loss: 930.7863521575928\n",
      "step: 3181.0, loss: 931.1856842041016\n",
      "step: 3182.0, loss: 932.6774749755859\n",
      "step: 3183.0, loss: 932.1190929412842\n",
      "step: 3184.0, loss: 931.1029052734375\n",
      "step: 3185.0, loss: 933.1449928283691\n",
      "step: 3186.0, loss: 932.1789417266846\n",
      "step: 3187.0, loss: 931.9895648956299\n",
      "step: 3188.0, loss: 933.1749496459961\n",
      "step: 3189.0, loss: 932.5144243240356\n",
      "step: 3190.0, loss: 932.3097925186157\n",
      "step: 3191.0, loss: 931.7149143218994\n",
      "step: 3192.0, loss: 933.0184488296509\n",
      "step: 3193.0, loss: 932.5990142822266\n",
      "step: 3194.0, loss: 930.2361698150635\n",
      "step: 3195.0, loss: 930.8281345367432\n",
      "step: 3196.0, loss: 929.2681865692139\n",
      "step: 3197.0, loss: 932.8898944854736\n",
      "step: 3198.0, loss: 931.3081903457642\n",
      "step: 3199.0, loss: 932.4807443618774\n",
      "step: 3200.0, loss: 932.7387313842773\n",
      "step: 3201.0, loss: 931.7528409957886\n",
      "step: 3202.0, loss: 932.4355487823486\n",
      "step: 3203.0, loss: 933.2076864242554\n",
      "step: 3204.0, loss: 932.7122964859009\n",
      "step: 3205.0, loss: 931.7988882064819\n",
      "step: 3206.0, loss: 930.8726062774658\n",
      "step: 3207.0, loss: 933.5848703384399\n",
      "step: 3208.0, loss: 932.8377628326416\n",
      "step: 3209.0, loss: 932.8908376693726\n",
      "step: 3210.0, loss: 934.012035369873\n",
      "step: 3211.0, loss: 930.9419355392456\n",
      "step: 3212.0, loss: 933.010576248169\n",
      "step: 3213.0, loss: 932.6603765487671\n",
      "step: 3214.0, loss: 932.5754928588867\n",
      "step: 3215.0, loss: 931.2644968032837\n",
      "step: 3216.0, loss: 931.5005159378052\n",
      "step: 3217.0, loss: 932.4169273376465\n",
      "step: 3218.0, loss: 932.8459978103638\n",
      "step: 3219.0, loss: 931.3305196762085\n",
      "step: 3220.0, loss: 932.1448354721069\n",
      "step: 3221.0, loss: 933.3085041046143\n",
      "step: 3222.0, loss: 931.2210512161255\n",
      "step: 3223.0, loss: 930.5142965316772\n",
      "step: 3224.0, loss: 933.4464063644409\n",
      "step: 3225.0, loss: 931.9425020217896\n",
      "step: 3226.0, loss: 932.5470781326294\n",
      "step: 3227.0, loss: 931.3718547821045\n",
      "step: 3228.0, loss: 930.8996229171753\n",
      "step: 3229.0, loss: 932.1353006362915\n",
      "step: 3230.0, loss: 933.2499732971191\n",
      "step: 3231.0, loss: 931.353006362915\n",
      "step: 3232.0, loss: 932.5200300216675\n",
      "step: 3233.0, loss: 932.1367959976196\n",
      "step: 3234.0, loss: 932.3511810302734\n",
      "step: 3235.0, loss: 932.5134677886963\n",
      "step: 3236.0, loss: 931.7322015762329\n",
      "step: 3237.0, loss: 934.2505493164062\n",
      "step: 3238.0, loss: 931.1929693222046\n",
      "step: 3239.0, loss: 931.9687957763672\n",
      "step: 3240.0, loss: 933.1577615737915\n",
      "step: 3241.0, loss: 932.22119140625\n",
      "step: 3242.0, loss: 933.6166229248047\n",
      "step: 3243.0, loss: 932.6478967666626\n",
      "step: 3244.0, loss: 933.3699541091919\n",
      "step: 3245.0, loss: 931.5011987686157\n",
      "step: 3246.0, loss: 933.6416625976562\n",
      "step: 3247.0, loss: 931.6991214752197\n",
      "step: 3248.0, loss: 930.8198776245117\n",
      "step: 3249.0, loss: 930.6715269088745\n",
      "step: 3250.0, loss: 930.7746648788452\n",
      "step: 3251.0, loss: 933.3649425506592\n",
      "step: 3252.0, loss: 931.3926677703857\n",
      "step: 3253.0, loss: 930.902346611023\n",
      "step: 3254.0, loss: 931.2451362609863\n",
      "step: 3255.0, loss: 933.0394515991211\n",
      "step: 3256.0, loss: 932.6874132156372\n",
      "step: 3257.0, loss: 930.9128465652466\n",
      "step: 3258.0, loss: 933.6603193283081\n",
      "step: 3259.0, loss: 932.8996810913086\n",
      "step: 3260.0, loss: 931.9806146621704\n",
      "step: 3261.0, loss: 932.030442237854\n",
      "step: 3262.0, loss: 932.525671005249\n",
      "step: 3263.0, loss: 932.1095685958862\n",
      "step: 3264.0, loss: 930.4231615066528\n",
      "step: 3265.0, loss: 932.8263273239136\n",
      "step: 3266.0, loss: 931.8183755874634\n",
      "step: 3267.0, loss: 931.5612144470215\n",
      "step: 3268.0, loss: 932.4412908554077\n",
      "step: 3269.0, loss: 930.8985347747803\n",
      "step: 3270.0, loss: 931.7605295181274\n",
      "step: 3271.0, loss: 931.8178482055664\n",
      "step: 3272.0, loss: 930.3660955429077\n",
      "step: 3273.0, loss: 932.3625106811523\n",
      "step: 3274.0, loss: 932.7678213119507\n",
      "step: 3275.0, loss: 929.580451965332\n",
      "step: 3276.0, loss: 931.7160739898682\n",
      "step: 3277.0, loss: 932.4662714004517\n",
      "step: 3278.0, loss: 932.8370933532715\n",
      "step: 3279.0, loss: 930.7713479995728\n",
      "step: 3280.0, loss: 931.5382432937622\n",
      "step: 3281.0, loss: 931.0762596130371\n",
      "step: 3282.0, loss: 931.4564867019653\n",
      "step: 3283.0, loss: 933.2633962631226\n",
      "step: 3284.0, loss: 932.2622337341309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 3285.0, loss: 931.2797679901123\n",
      "step: 3286.0, loss: 930.6429061889648\n",
      "step: 3287.0, loss: 931.309362411499\n",
      "step: 3288.0, loss: 929.2084722518921\n",
      "step: 3289.0, loss: 933.1454277038574\n",
      "step: 3290.0, loss: 931.4534034729004\n",
      "step: 3291.0, loss: 933.0913982391357\n",
      "step: 3292.0, loss: 932.0553483963013\n",
      "step: 3293.0, loss: 932.7287979125977\n",
      "step: 3294.0, loss: 930.0855474472046\n",
      "step: 3295.0, loss: 931.2230834960938\n",
      "step: 3296.0, loss: 930.3442516326904\n",
      "step: 3297.0, loss: 931.9533081054688\n",
      "step: 3298.0, loss: 932.3346786499023\n",
      "step: 3299.0, loss: 932.5460157394409\n",
      "step: 3300.0, loss: 931.186164855957\n",
      "step: 3301.0, loss: 933.6751260757446\n",
      "step: 3302.0, loss: 932.1824102401733\n",
      "step: 3303.0, loss: 931.2944107055664\n",
      "step: 3304.0, loss: 932.932505607605\n",
      "step: 3305.0, loss: 930.2734155654907\n",
      "step: 3306.0, loss: 930.7019319534302\n",
      "step: 3307.0, loss: 931.9998712539673\n",
      "step: 3308.0, loss: 930.5406274795532\n",
      "step: 3309.0, loss: 931.8671321868896\n",
      "step: 3310.0, loss: 929.4327774047852\n",
      "step: 3311.0, loss: 930.1792249679565\n",
      "step: 3312.0, loss: 934.7412214279175\n",
      "step: 3313.0, loss: 931.6461324691772\n",
      "step: 3314.0, loss: 931.7247362136841\n",
      "step: 3315.0, loss: 933.2563076019287\n",
      "step: 3316.0, loss: 933.9336385726929\n",
      "step: 3317.0, loss: 931.4343509674072\n",
      "step: 3318.0, loss: 933.1621427536011\n",
      "step: 3319.0, loss: 930.3741617202759\n",
      "step: 3320.0, loss: 932.8136472702026\n",
      "step: 3321.0, loss: 931.2529945373535\n",
      "step: 3322.0, loss: 932.2098875045776\n",
      "step: 3323.0, loss: 931.7250442504883\n",
      "step: 3324.0, loss: 933.0892429351807\n",
      "step: 3325.0, loss: 930.5300922393799\n",
      "step: 3326.0, loss: 931.4059791564941\n",
      "step: 3327.0, loss: 931.7242012023926\n",
      "step: 3328.0, loss: 932.4580450057983\n",
      "step: 3329.0, loss: 931.3418340682983\n",
      "step: 3330.0, loss: 931.2494258880615\n",
      "step: 3331.0, loss: 932.8130836486816\n",
      "step: 3332.0, loss: 930.8404655456543\n",
      "step: 3333.0, loss: 933.4524908065796\n",
      "step: 3334.0, loss: 930.5156087875366\n",
      "step: 3335.0, loss: 930.3344802856445\n",
      "step: 3336.0, loss: 933.0470542907715\n",
      "step: 3337.0, loss: 931.230131149292\n",
      "step: 3338.0, loss: 932.0528726577759\n",
      "step: 3339.0, loss: 930.8871221542358\n",
      "step: 3340.0, loss: 932.0612630844116\n",
      "step: 3341.0, loss: 932.4073505401611\n",
      "step: 3342.0, loss: 931.096736907959\n",
      "step: 3343.0, loss: 930.5259342193604\n",
      "step: 3344.0, loss: 932.0845680236816\n",
      "step: 3345.0, loss: 931.4232006072998\n",
      "step: 3346.0, loss: 930.2968482971191\n",
      "step: 3347.0, loss: 929.4406156539917\n",
      "step: 3348.0, loss: 931.5221014022827\n",
      "step: 3349.0, loss: 933.0943832397461\n",
      "step: 3350.0, loss: 930.2013063430786\n",
      "step: 3351.0, loss: 931.1190462112427\n",
      "step: 3352.0, loss: 932.1163482666016\n",
      "step: 3353.0, loss: 931.6719331741333\n",
      "step: 3354.0, loss: 932.0324277877808\n",
      "step: 3355.0, loss: 930.8618564605713\n",
      "step: 3356.0, loss: 930.2449817657471\n",
      "step: 3357.0, loss: 932.7635679244995\n",
      "step: 3358.0, loss: 929.6315507888794\n",
      "step: 3359.0, loss: 931.4493246078491\n",
      "step: 3360.0, loss: 933.7100038528442\n",
      "step: 3361.0, loss: 933.9257574081421\n",
      "step: 3362.0, loss: 933.4345083236694\n",
      "step: 3363.0, loss: 931.9016847610474\n",
      "step: 3364.0, loss: 930.1268472671509\n",
      "step: 3365.0, loss: 931.8266248703003\n",
      "step: 3366.0, loss: 931.734540939331\n",
      "step: 3367.0, loss: 932.1083879470825\n",
      "step: 3368.0, loss: 933.916916847229\n",
      "step: 3369.0, loss: 933.379768371582\n",
      "step: 3370.0, loss: 929.8552627563477\n",
      "step: 3371.0, loss: 931.5161066055298\n",
      "step: 3372.0, loss: 928.7134952545166\n",
      "step: 3373.0, loss: 930.6306457519531\n",
      "step: 3374.0, loss: 929.348406791687\n",
      "step: 3375.0, loss: 930.9675521850586\n",
      "step: 3376.0, loss: 931.7474555969238\n",
      "step: 3377.0, loss: 931.5249729156494\n",
      "step: 3378.0, loss: 931.0847721099854\n",
      "step: 3379.0, loss: 932.1358032226562\n",
      "step: 3380.0, loss: 932.1049613952637\n",
      "step: 3381.0, loss: 932.3509721755981\n",
      "step: 3382.0, loss: 929.9401998519897\n",
      "step: 3383.0, loss: 930.6007022857666\n",
      "step: 3384.0, loss: 932.2233018875122\n",
      "step: 3385.0, loss: 931.33762550354\n",
      "step: 3386.0, loss: 931.6973857879639\n",
      "step: 3387.0, loss: 931.395453453064\n",
      "step: 3388.0, loss: 931.3653478622437\n",
      "step: 3389.0, loss: 930.8786334991455\n",
      "step: 3390.0, loss: 931.2571907043457\n",
      "step: 3391.0, loss: 931.4063997268677\n",
      "step: 3392.0, loss: 933.4543972015381\n",
      "step: 3393.0, loss: 931.9203329086304\n",
      "step: 3394.0, loss: 931.5812282562256\n",
      "step: 3395.0, loss: 930.1605443954468\n",
      "step: 3396.0, loss: 932.102313041687\n",
      "step: 3397.0, loss: 932.5457639694214\n",
      "step: 3398.0, loss: 929.7158498764038\n",
      "step: 3399.0, loss: 932.6056776046753\n",
      "step: 3400.0, loss: 930.9871921539307\n",
      "step: 3401.0, loss: 930.7413082122803\n",
      "step: 3402.0, loss: 933.2738227844238\n",
      "step: 3403.0, loss: 933.1683387756348\n",
      "step: 3404.0, loss: 932.0207929611206\n",
      "step: 3405.0, loss: 930.1425695419312\n",
      "step: 3406.0, loss: 932.0517425537109\n",
      "step: 3407.0, loss: 932.1693344116211\n",
      "step: 3408.0, loss: 931.1217460632324\n",
      "step: 3409.0, loss: 931.5010290145874\n",
      "step: 3410.0, loss: 932.3088073730469\n",
      "step: 3411.0, loss: 931.7770118713379\n",
      "step: 3412.0, loss: 930.2955780029297\n",
      "step: 3413.0, loss: 932.9059820175171\n",
      "step: 3414.0, loss: 931.3371295928955\n",
      "step: 3415.0, loss: 930.2808589935303\n",
      "step: 3416.0, loss: 928.6685562133789\n",
      "step: 3417.0, loss: 932.328519821167\n",
      "step: 3418.0, loss: 930.818362236023\n",
      "step: 3419.0, loss: 931.6561861038208\n",
      "step: 3420.0, loss: 930.9402523040771\n",
      "step: 3421.0, loss: 930.8503980636597\n",
      "step: 3422.0, loss: 932.3465881347656\n",
      "step: 3423.0, loss: 931.5565595626831\n",
      "step: 3424.0, loss: 932.8393888473511\n",
      "step: 3425.0, loss: 931.5851078033447\n",
      "step: 3426.0, loss: 930.7079467773438\n",
      "step: 3427.0, loss: 929.0860109329224\n",
      "step: 3428.0, loss: 931.7739744186401\n",
      "step: 3429.0, loss: 929.4568462371826\n",
      "step: 3430.0, loss: 930.6645975112915\n",
      "step: 3431.0, loss: 933.4168748855591\n",
      "step: 3432.0, loss: 932.7721080780029\n",
      "step: 3433.0, loss: 930.6718788146973\n",
      "step: 3434.0, loss: 933.3864850997925\n",
      "step: 3435.0, loss: 929.2815780639648\n",
      "step: 3436.0, loss: 931.6467952728271\n",
      "step: 3437.0, loss: 928.9533634185791\n",
      "step: 3438.0, loss: 931.7108764648438\n",
      "step: 3439.0, loss: 930.3440313339233\n",
      "step: 3440.0, loss: 931.4730978012085\n",
      "step: 3441.0, loss: 931.5767574310303\n",
      "step: 3442.0, loss: 932.3601913452148\n",
      "step: 3443.0, loss: 931.5683288574219\n",
      "step: 3444.0, loss: 930.9084949493408\n",
      "step: 3445.0, loss: 931.0576791763306\n",
      "step: 3446.0, loss: 932.4671640396118\n",
      "step: 3447.0, loss: 931.8091287612915\n",
      "step: 3448.0, loss: 931.7315807342529\n",
      "step: 3449.0, loss: 930.4041318893433\n",
      "step: 3450.0, loss: 931.9128341674805\n",
      "step: 3451.0, loss: 932.8452863693237\n",
      "step: 3452.0, loss: 930.7756643295288\n",
      "step: 3453.0, loss: 930.6850051879883\n",
      "step: 3454.0, loss: 931.4202079772949\n",
      "step: 3455.0, loss: 932.1875867843628\n",
      "step: 3456.0, loss: 930.6602191925049\n",
      "step: 3457.0, loss: 931.1582174301147\n",
      "step: 3458.0, loss: 930.9365148544312\n",
      "step: 3459.0, loss: 928.7219934463501\n",
      "step: 3460.0, loss: 930.542028427124\n",
      "step: 3461.0, loss: 931.7461223602295\n",
      "step: 3462.0, loss: 931.1375179290771\n",
      "step: 3463.0, loss: 932.6201496124268\n",
      "step: 3464.0, loss: 929.0681648254395\n",
      "step: 3465.0, loss: 931.7397718429565\n",
      "step: 3466.0, loss: 930.6947145462036\n",
      "step: 3467.0, loss: 931.8365125656128\n",
      "step: 3468.0, loss: 931.2344751358032\n",
      "step: 3469.0, loss: 932.0932703018188\n",
      "step: 3470.0, loss: 931.7127714157104\n",
      "step: 3471.0, loss: 930.1784963607788\n",
      "step: 3472.0, loss: 932.0314655303955\n",
      "step: 3473.0, loss: 931.5523805618286\n",
      "step: 3474.0, loss: 933.3400678634644\n",
      "step: 3475.0, loss: 931.3030118942261\n",
      "step: 3476.0, loss: 932.1500091552734\n",
      "step: 3477.0, loss: 930.806191444397\n",
      "step: 3478.0, loss: 933.0838413238525\n",
      "step: 3479.0, loss: 930.0703868865967\n",
      "step: 3480.0, loss: 932.0772123336792\n",
      "step: 3481.0, loss: 930.3145713806152\n",
      "step: 3482.0, loss: 929.7124404907227\n",
      "step: 3483.0, loss: 932.6683988571167\n",
      "step: 3484.0, loss: 931.6082954406738\n",
      "step: 3485.0, loss: 930.1960353851318\n",
      "step: 3486.0, loss: 929.6489067077637\n",
      "step: 3487.0, loss: 930.6197032928467\n",
      "step: 3488.0, loss: 930.8533363342285\n",
      "step: 3489.0, loss: 931.7831411361694\n",
      "step: 3490.0, loss: 931.4267950057983\n",
      "step: 3491.0, loss: 930.9229145050049\n",
      "step: 3492.0, loss: 931.5381097793579\n",
      "step: 3493.0, loss: 931.451322555542\n",
      "step: 3494.0, loss: 932.7156686782837\n",
      "step: 3495.0, loss: 931.9487743377686\n",
      "step: 3496.0, loss: 932.0392866134644\n",
      "step: 3497.0, loss: 930.1293449401855\n",
      "step: 3498.0, loss: 931.1169853210449\n",
      "step: 3499.0, loss: 931.2982597351074\n",
      "step: 3500.0, loss: 930.0647010803223\n",
      "step: 3501.0, loss: 932.1128520965576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 3502.0, loss: 928.5786428451538\n",
      "step: 3503.0, loss: 932.2247476577759\n",
      "step: 3504.0, loss: 930.6684675216675\n",
      "step: 3505.0, loss: 931.2336254119873\n",
      "step: 3506.0, loss: 931.2948474884033\n",
      "step: 3507.0, loss: 930.7725076675415\n",
      "step: 3508.0, loss: 931.6968050003052\n",
      "step: 3509.0, loss: 930.6842956542969\n",
      "step: 3510.0, loss: 932.3677978515625\n",
      "step: 3511.0, loss: 932.966908454895\n",
      "step: 3512.0, loss: 930.9737958908081\n",
      "step: 3513.0, loss: 931.7335062026978\n",
      "step: 3514.0, loss: 931.1499490737915\n",
      "step: 3515.0, loss: 931.5610218048096\n",
      "step: 3516.0, loss: 931.8597240447998\n",
      "step: 3517.0, loss: 932.3764200210571\n",
      "step: 3518.0, loss: 930.340238571167\n",
      "step: 3519.0, loss: 930.6244430541992\n",
      "step: 3520.0, loss: 930.7793321609497\n",
      "step: 3521.0, loss: 929.9295139312744\n",
      "step: 3522.0, loss: 931.6950702667236\n",
      "step: 3523.0, loss: 931.8751935958862\n",
      "step: 3524.0, loss: 931.7270841598511\n",
      "step: 3525.0, loss: 931.6190881729126\n",
      "step: 3526.0, loss: 929.9074077606201\n",
      "step: 3527.0, loss: 932.0046977996826\n",
      "step: 3528.0, loss: 931.5842981338501\n",
      "step: 3529.0, loss: 931.8907508850098\n",
      "step: 3530.0, loss: 931.0763826370239\n",
      "step: 3531.0, loss: 929.7900705337524\n",
      "step: 3532.0, loss: 930.7596111297607\n",
      "step: 3533.0, loss: 931.4646434783936\n",
      "step: 3534.0, loss: 931.1574554443359\n",
      "step: 3535.0, loss: 930.5166625976562\n",
      "step: 3536.0, loss: 929.881178855896\n",
      "step: 3537.0, loss: 930.5297222137451\n",
      "step: 3538.0, loss: 932.1163349151611\n",
      "step: 3539.0, loss: 929.7180643081665\n",
      "step: 3540.0, loss: 930.3162775039673\n",
      "step: 3541.0, loss: 932.2461156845093\n",
      "step: 3542.0, loss: 930.7794799804688\n",
      "step: 3543.0, loss: 931.1914482116699\n",
      "step: 3544.0, loss: 928.7290840148926\n",
      "step: 3545.0, loss: 930.0218744277954\n",
      "step: 3546.0, loss: 930.3931589126587\n",
      "step: 3547.0, loss: 932.591386795044\n",
      "step: 3548.0, loss: 930.8106279373169\n",
      "step: 3549.0, loss: 931.3376951217651\n",
      "step: 3550.0, loss: 931.0319995880127\n",
      "step: 3551.0, loss: 929.4912614822388\n",
      "step: 3552.0, loss: 931.9641809463501\n",
      "step: 3553.0, loss: 930.1362886428833\n",
      "step: 3554.0, loss: 930.0899047851562\n",
      "step: 3555.0, loss: 931.0266094207764\n",
      "step: 3556.0, loss: 930.4410619735718\n",
      "step: 3557.0, loss: 930.0785608291626\n",
      "step: 3558.0, loss: 929.4007158279419\n",
      "step: 3559.0, loss: 932.6622877120972\n",
      "step: 3560.0, loss: 931.8439693450928\n",
      "step: 3561.0, loss: 930.534384727478\n",
      "step: 3562.0, loss: 929.5837488174438\n",
      "step: 3563.0, loss: 931.3388118743896\n",
      "step: 3564.0, loss: 931.1209859848022\n",
      "step: 3565.0, loss: 931.3610038757324\n",
      "step: 3566.0, loss: 932.0375804901123\n",
      "step: 3567.0, loss: 932.7702465057373\n",
      "step: 3568.0, loss: 929.3098850250244\n",
      "step: 3569.0, loss: 930.9178056716919\n",
      "step: 3570.0, loss: 930.3273391723633\n",
      "step: 3571.0, loss: 931.4993562698364\n",
      "step: 3572.0, loss: 930.8766784667969\n",
      "step: 3573.0, loss: 929.4750623703003\n",
      "step: 3574.0, loss: 931.9370040893555\n",
      "step: 3575.0, loss: 932.6795225143433\n",
      "step: 3576.0, loss: 931.4658555984497\n",
      "step: 3577.0, loss: 930.9047183990479\n",
      "step: 3578.0, loss: 928.5432453155518\n",
      "step: 3579.0, loss: 932.2136001586914\n",
      "step: 3580.0, loss: 929.471471786499\n",
      "step: 3581.0, loss: 929.7243499755859\n",
      "step: 3582.0, loss: 931.5776357650757\n",
      "step: 3583.0, loss: 931.1761684417725\n",
      "step: 3584.0, loss: 932.7632761001587\n",
      "step: 3585.0, loss: 931.0415534973145\n",
      "step: 3586.0, loss: 931.3220233917236\n",
      "step: 3587.0, loss: 930.8922338485718\n",
      "step: 3588.0, loss: 933.4397687911987\n",
      "step: 3589.0, loss: 929.8878049850464\n",
      "step: 3590.0, loss: 930.2861995697021\n",
      "step: 3591.0, loss: 931.7670192718506\n",
      "step: 3592.0, loss: 930.5710563659668\n",
      "step: 3593.0, loss: 929.573353767395\n",
      "step: 3594.0, loss: 930.3458776473999\n",
      "step: 3595.0, loss: 932.3148641586304\n",
      "step: 3596.0, loss: 930.2378816604614\n",
      "step: 3597.0, loss: 931.0497388839722\n",
      "step: 3598.0, loss: 930.2838354110718\n",
      "step: 3599.0, loss: 932.9774894714355\n",
      "step: 3600.0, loss: 929.728404045105\n",
      "step: 3601.0, loss: 930.5865163803101\n",
      "step: 3602.0, loss: 929.3308095932007\n",
      "step: 3603.0, loss: 930.734076499939\n",
      "step: 3604.0, loss: 931.3620872497559\n",
      "step: 3605.0, loss: 930.6243677139282\n",
      "step: 3606.0, loss: 931.3810567855835\n",
      "step: 3607.0, loss: 930.5029001235962\n",
      "step: 3608.0, loss: 930.386435508728\n",
      "step: 3609.0, loss: 932.2545766830444\n",
      "step: 3610.0, loss: 930.7276983261108\n",
      "step: 3611.0, loss: 929.1252241134644\n",
      "step: 3612.0, loss: 930.2438545227051\n",
      "step: 3613.0, loss: 930.7753477096558\n",
      "step: 3614.0, loss: 931.50319480896\n",
      "step: 3615.0, loss: 930.503583908081\n",
      "step: 3616.0, loss: 933.4372282028198\n",
      "step: 3617.0, loss: 932.925479888916\n",
      "step: 3618.0, loss: 930.9295749664307\n",
      "step: 3619.0, loss: 930.7258825302124\n",
      "step: 3620.0, loss: 931.4451904296875\n",
      "step: 3621.0, loss: 930.3532886505127\n",
      "step: 3622.0, loss: 930.1469259262085\n",
      "step: 3623.0, loss: 930.3353958129883\n",
      "step: 3624.0, loss: 929.9036979675293\n",
      "step: 3625.0, loss: 932.1293058395386\n",
      "step: 3626.0, loss: 930.5745706558228\n",
      "step: 3627.0, loss: 930.4641036987305\n",
      "step: 3628.0, loss: 930.9094791412354\n",
      "step: 3629.0, loss: 928.2894344329834\n",
      "step: 3630.0, loss: 930.3842430114746\n",
      "step: 3631.0, loss: 929.9849424362183\n",
      "step: 3632.0, loss: 929.9451284408569\n",
      "step: 3633.0, loss: 929.7281723022461\n",
      "step: 3634.0, loss: 930.4579305648804\n",
      "step: 3635.0, loss: 932.3845500946045\n",
      "step: 3636.0, loss: 929.0537843704224\n",
      "step: 3637.0, loss: 930.8571901321411\n",
      "step: 3638.0, loss: 932.70445728302\n",
      "step: 3639.0, loss: 931.192006111145\n",
      "step: 3640.0, loss: 931.2929153442383\n",
      "step: 3641.0, loss: 930.0646543502808\n",
      "step: 3642.0, loss: 932.2026844024658\n",
      "step: 3643.0, loss: 932.5288887023926\n",
      "step: 3644.0, loss: 931.3316478729248\n",
      "step: 3645.0, loss: 932.6980409622192\n",
      "step: 3646.0, loss: 932.161735534668\n",
      "step: 3647.0, loss: 929.6470899581909\n",
      "step: 3648.0, loss: 932.2120122909546\n",
      "step: 3649.0, loss: 931.2782602310181\n",
      "step: 3650.0, loss: 930.8587017059326\n",
      "step: 3651.0, loss: 930.7015838623047\n",
      "step: 3652.0, loss: 930.8800983428955\n",
      "step: 3653.0, loss: 929.840160369873\n",
      "step: 3654.0, loss: 932.9403324127197\n",
      "step: 3655.0, loss: 931.7846231460571\n",
      "step: 3656.0, loss: 930.3738145828247\n",
      "step: 3657.0, loss: 931.826093673706\n",
      "step: 3658.0, loss: 932.1519956588745\n",
      "step: 3659.0, loss: 931.6904525756836\n",
      "step: 3660.0, loss: 929.6285085678101\n",
      "step: 3661.0, loss: 930.5875215530396\n",
      "step: 3662.0, loss: 931.1950006484985\n",
      "step: 3663.0, loss: 931.4710340499878\n",
      "step: 3664.0, loss: 930.7419519424438\n",
      "step: 3665.0, loss: 930.0740032196045\n",
      "step: 3666.0, loss: 930.9001617431641\n",
      "step: 3667.0, loss: 932.4695291519165\n",
      "step: 3668.0, loss: 931.8820915222168\n",
      "step: 3669.0, loss: 931.725588798523\n",
      "step: 3670.0, loss: 930.3374977111816\n",
      "step: 3671.0, loss: 931.4308032989502\n",
      "step: 3672.0, loss: 931.0499858856201\n",
      "step: 3673.0, loss: 928.5382862091064\n",
      "step: 3674.0, loss: 930.7081604003906\n",
      "step: 3675.0, loss: 931.1978054046631\n",
      "step: 3676.0, loss: 930.3565111160278\n",
      "step: 3677.0, loss: 930.7456855773926\n",
      "step: 3678.0, loss: 932.1222124099731\n",
      "step: 3679.0, loss: 931.6076440811157\n",
      "step: 3680.0, loss: 931.2008361816406\n",
      "step: 3681.0, loss: 930.4134769439697\n",
      "step: 3682.0, loss: 929.7518854141235\n",
      "step: 3683.0, loss: 932.2060699462891\n",
      "step: 3684.0, loss: 930.9809436798096\n",
      "step: 3685.0, loss: 931.9331550598145\n",
      "step: 3686.0, loss: 931.2323904037476\n",
      "step: 3687.0, loss: 930.4924697875977\n",
      "step: 3688.0, loss: 929.044412612915\n",
      "step: 3689.0, loss: 931.575798034668\n",
      "step: 3690.0, loss: 932.8665218353271\n",
      "step: 3691.0, loss: 928.78844165802\n",
      "step: 3692.0, loss: 930.0468826293945\n",
      "step: 3693.0, loss: 929.153564453125\n",
      "step: 3694.0, loss: 929.199610710144\n",
      "step: 3695.0, loss: 931.5731048583984\n",
      "step: 3696.0, loss: 930.2997093200684\n",
      "step: 3697.0, loss: 929.7585935592651\n",
      "step: 3698.0, loss: 929.7741441726685\n",
      "step: 3699.0, loss: 927.9347581863403\n",
      "step: 3700.0, loss: 929.8045406341553\n",
      "step: 3701.0, loss: 929.4428253173828\n",
      "step: 3702.0, loss: 930.6876096725464\n",
      "step: 3703.0, loss: 930.9779281616211\n",
      "step: 3704.0, loss: 931.6278324127197\n",
      "step: 3705.0, loss: 929.3186321258545\n",
      "step: 3706.0, loss: 930.9506378173828\n",
      "step: 3707.0, loss: 931.4512519836426\n",
      "step: 3708.0, loss: 931.7908782958984\n",
      "step: 3709.0, loss: 931.4059171676636\n",
      "step: 3710.0, loss: 932.1536016464233\n",
      "step: 3711.0, loss: 930.5766792297363\n",
      "step: 3712.0, loss: 930.3294715881348\n",
      "step: 3713.0, loss: 929.1621780395508\n",
      "step: 3714.0, loss: 929.1273918151855\n",
      "step: 3715.0, loss: 931.3508138656616\n",
      "step: 3716.0, loss: 931.7510662078857\n",
      "step: 3717.0, loss: 930.1613969802856\n",
      "step: 3718.0, loss: 931.9127426147461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 3719.0, loss: 930.059247970581\n",
      "step: 3720.0, loss: 930.9950037002563\n",
      "step: 3721.0, loss: 931.9522390365601\n",
      "step: 3722.0, loss: 929.32483959198\n",
      "step: 3723.0, loss: 930.7861490249634\n",
      "step: 3724.0, loss: 931.1714000701904\n",
      "step: 3725.0, loss: 929.8603315353394\n",
      "step: 3726.0, loss: 930.5082588195801\n",
      "step: 3727.0, loss: 930.6551008224487\n",
      "step: 3728.0, loss: 932.0551834106445\n",
      "step: 3729.0, loss: 931.0535087585449\n",
      "step: 3730.0, loss: 931.2315378189087\n",
      "step: 3731.0, loss: 929.3781576156616\n",
      "step: 3732.0, loss: 930.3020372390747\n",
      "step: 3733.0, loss: 932.7938079833984\n",
      "step: 3734.0, loss: 929.860445022583\n",
      "step: 3735.0, loss: 930.8994264602661\n",
      "step: 3736.0, loss: 927.6839599609375\n",
      "step: 3737.0, loss: 932.3211870193481\n",
      "step: 3738.0, loss: 928.2170505523682\n",
      "step: 3739.0, loss: 930.501802444458\n",
      "step: 3740.0, loss: 929.9315338134766\n",
      "step: 3741.0, loss: 929.2802972793579\n",
      "step: 3742.0, loss: 931.980767250061\n",
      "step: 3743.0, loss: 929.4492454528809\n",
      "step: 3744.0, loss: 932.4147100448608\n",
      "step: 3745.0, loss: 932.5896463394165\n",
      "step: 3746.0, loss: 930.8654432296753\n",
      "step: 3747.0, loss: 930.7792253494263\n",
      "step: 3748.0, loss: 932.2689838409424\n",
      "step: 3749.0, loss: 930.6548547744751\n",
      "step: 3750.0, loss: 929.995623588562\n",
      "step: 3751.0, loss: 928.5744609832764\n",
      "step: 3752.0, loss: 930.9844017028809\n",
      "step: 3753.0, loss: 930.4504995346069\n",
      "step: 3754.0, loss: 930.1346263885498\n",
      "step: 3755.0, loss: 929.4095706939697\n",
      "step: 3756.0, loss: 929.3459968566895\n",
      "step: 3757.0, loss: 929.6680240631104\n",
      "step: 3758.0, loss: 928.4793310165405\n",
      "step: 3759.0, loss: 931.7054347991943\n",
      "step: 3760.0, loss: 930.8684587478638\n",
      "step: 3761.0, loss: 932.1426200866699\n",
      "step: 3762.0, loss: 929.717435836792\n",
      "step: 3763.0, loss: 928.0924406051636\n",
      "step: 3764.0, loss: 928.8069190979004\n",
      "step: 3765.0, loss: 928.146050453186\n",
      "step: 3766.0, loss: 929.9585103988647\n",
      "step: 3767.0, loss: 930.4197902679443\n",
      "step: 3768.0, loss: 931.6460762023926\n",
      "step: 3769.0, loss: 931.8067817687988\n",
      "step: 3770.0, loss: 932.1170444488525\n",
      "step: 3771.0, loss: 932.5559644699097\n",
      "step: 3772.0, loss: 931.3108911514282\n",
      "step: 3773.0, loss: 930.8413753509521\n",
      "step: 3774.0, loss: 929.8840990066528\n",
      "step: 3775.0, loss: 931.0765705108643\n",
      "step: 3776.0, loss: 931.168291091919\n",
      "step: 3777.0, loss: 928.4441518783569\n",
      "step: 3778.0, loss: 930.1629619598389\n",
      "step: 3779.0, loss: 929.19167137146\n",
      "step: 3780.0, loss: 932.1373100280762\n",
      "step: 3781.0, loss: 930.7788743972778\n",
      "step: 3782.0, loss: 928.7817239761353\n",
      "step: 3783.0, loss: 929.7929487228394\n",
      "step: 3784.0, loss: 932.2688751220703\n",
      "step: 3785.0, loss: 930.6083536148071\n",
      "step: 3786.0, loss: 928.6172924041748\n",
      "step: 3787.0, loss: 928.7315397262573\n",
      "step: 3788.0, loss: 929.5002384185791\n",
      "step: 3789.0, loss: 931.0292682647705\n",
      "step: 3790.0, loss: 930.2353944778442\n",
      "step: 3791.0, loss: 931.2686405181885\n",
      "step: 3792.0, loss: 930.2056074142456\n",
      "step: 3793.0, loss: 930.1168174743652\n",
      "step: 3794.0, loss: 929.797402381897\n",
      "step: 3795.0, loss: 930.6571092605591\n",
      "step: 3796.0, loss: 930.2569894790649\n",
      "step: 3797.0, loss: 932.3237533569336\n",
      "step: 3798.0, loss: 929.7819833755493\n",
      "step: 3799.0, loss: 931.0035600662231\n",
      "step: 3800.0, loss: 928.9855098724365\n",
      "step: 3801.0, loss: 929.1687421798706\n",
      "step: 3802.0, loss: 930.1167345046997\n",
      "step: 3803.0, loss: 929.4732398986816\n",
      "step: 3804.0, loss: 931.7065811157227\n",
      "step: 3805.0, loss: 930.3301191329956\n",
      "step: 3806.0, loss: 932.2358379364014\n",
      "step: 3807.0, loss: 930.9649505615234\n",
      "step: 3808.0, loss: 930.8737468719482\n",
      "step: 3809.0, loss: 931.7066888809204\n",
      "step: 3810.0, loss: 931.5080995559692\n",
      "step: 3811.0, loss: 928.818356513977\n",
      "step: 3812.0, loss: 929.2562971115112\n",
      "step: 3813.0, loss: 929.8493385314941\n",
      "step: 3814.0, loss: 931.931245803833\n",
      "step: 3815.0, loss: 931.4281587600708\n",
      "step: 3816.0, loss: 929.4496240615845\n",
      "step: 3817.0, loss: 931.5825748443604\n",
      "step: 3818.0, loss: 930.8089532852173\n",
      "step: 3819.0, loss: 929.9758939743042\n",
      "step: 3820.0, loss: 929.3291120529175\n",
      "step: 3821.0, loss: 930.3623752593994\n",
      "step: 3822.0, loss: 930.0670404434204\n",
      "step: 3823.0, loss: 929.7580232620239\n",
      "step: 3824.0, loss: 930.56614112854\n",
      "step: 3825.0, loss: 931.4086542129517\n",
      "step: 3826.0, loss: 930.4733085632324\n",
      "step: 3827.0, loss: 928.7300863265991\n",
      "step: 3828.0, loss: 931.6616506576538\n",
      "step: 3829.0, loss: 931.2103862762451\n",
      "step: 3830.0, loss: 929.1824312210083\n",
      "step: 3831.0, loss: 931.4774217605591\n",
      "step: 3832.0, loss: 929.8727521896362\n",
      "step: 3833.0, loss: 930.5016355514526\n",
      "step: 3834.0, loss: 927.2605752944946\n",
      "step: 3835.0, loss: 928.5012826919556\n",
      "step: 3836.0, loss: 929.0035276412964\n",
      "step: 3837.0, loss: 932.7108697891235\n",
      "step: 3838.0, loss: 930.1534128189087\n",
      "step: 3839.0, loss: 930.7906131744385\n",
      "step: 3840.0, loss: 932.4405212402344\n",
      "step: 3841.0, loss: 931.1561851501465\n",
      "step: 3842.0, loss: 930.3652896881104\n",
      "step: 3843.0, loss: 927.8973798751831\n",
      "step: 3844.0, loss: 928.9353694915771\n",
      "step: 3845.0, loss: 930.199951171875\n",
      "step: 3846.0, loss: 930.8284177780151\n",
      "step: 3847.0, loss: 928.14768409729\n",
      "step: 3848.0, loss: 929.0161037445068\n",
      "step: 3849.0, loss: 931.396635055542\n",
      "step: 3850.0, loss: 934.2010765075684\n",
      "step: 3851.0, loss: 929.038950920105\n",
      "step: 3852.0, loss: 932.1012506484985\n",
      "step: 3853.0, loss: 931.4756660461426\n",
      "step: 3854.0, loss: 927.4712734222412\n",
      "step: 3855.0, loss: 930.74169921875\n",
      "step: 3856.0, loss: 929.9320116043091\n",
      "step: 3857.0, loss: 928.7126264572144\n",
      "step: 3858.0, loss: 931.4635324478149\n",
      "step: 3859.0, loss: 930.5490283966064\n",
      "step: 3860.0, loss: 927.7823486328125\n",
      "step: 3861.0, loss: 929.8867416381836\n",
      "step: 3862.0, loss: 931.3749761581421\n",
      "step: 3863.0, loss: 929.9649839401245\n",
      "step: 3864.0, loss: 930.0670366287231\n",
      "step: 3865.0, loss: 930.6231746673584\n",
      "step: 3866.0, loss: 930.546555519104\n",
      "step: 3867.0, loss: 928.8401536941528\n",
      "step: 3868.0, loss: 930.1422338485718\n",
      "step: 3869.0, loss: 930.333384513855\n",
      "step: 3870.0, loss: 932.1905269622803\n",
      "step: 3871.0, loss: 930.5379838943481\n",
      "step: 3872.0, loss: 931.0414762496948\n",
      "step: 3873.0, loss: 928.909478187561\n",
      "step: 3874.0, loss: 929.739803314209\n",
      "step: 3875.0, loss: 929.232180595398\n",
      "step: 3876.0, loss: 929.355565071106\n",
      "step: 3877.0, loss: 929.5049905776978\n",
      "step: 3878.0, loss: 929.9455099105835\n",
      "step: 3879.0, loss: 929.3878736495972\n",
      "step: 3880.0, loss: 931.3024730682373\n",
      "step: 3881.0, loss: 931.3676223754883\n",
      "step: 3882.0, loss: 930.4506406784058\n",
      "step: 3883.0, loss: 929.2577838897705\n",
      "step: 3884.0, loss: 930.5305509567261\n",
      "step: 3885.0, loss: 929.0851449966431\n",
      "step: 3886.0, loss: 930.843584060669\n",
      "step: 3887.0, loss: 930.6623706817627\n",
      "step: 3888.0, loss: 931.6700630187988\n",
      "step: 3889.0, loss: 930.7807903289795\n",
      "step: 3890.0, loss: 931.0384407043457\n",
      "step: 3891.0, loss: 931.1217355728149\n",
      "step: 3892.0, loss: 929.030930519104\n",
      "step: 3893.0, loss: 931.4056901931763\n",
      "step: 3894.0, loss: 931.7296028137207\n",
      "step: 3895.0, loss: 929.3267364501953\n",
      "step: 3896.0, loss: 928.9694690704346\n",
      "step: 3897.0, loss: 929.147439956665\n",
      "step: 3898.0, loss: 931.6368389129639\n",
      "step: 3899.0, loss: 929.9757766723633\n",
      "step: 3900.0, loss: 931.4438972473145\n",
      "step: 3901.0, loss: 931.2527256011963\n",
      "step: 3902.0, loss: 928.9884023666382\n",
      "step: 3903.0, loss: 930.6661357879639\n",
      "step: 3904.0, loss: 929.9287300109863\n",
      "step: 3905.0, loss: 931.2011651992798\n",
      "step: 3906.0, loss: 929.0018892288208\n",
      "step: 3907.0, loss: 929.8159084320068\n",
      "step: 3908.0, loss: 929.5345087051392\n",
      "step: 3909.0, loss: 929.6962814331055\n",
      "step: 3910.0, loss: 930.5051002502441\n",
      "step: 3911.0, loss: 930.1788568496704\n",
      "step: 3912.0, loss: 930.9053726196289\n",
      "step: 3913.0, loss: 930.4708433151245\n",
      "step: 3914.0, loss: 930.6446466445923\n",
      "step: 3915.0, loss: 930.7970428466797\n",
      "step: 3916.0, loss: 929.5075101852417\n",
      "step: 3917.0, loss: 931.0034999847412\n",
      "step: 3918.0, loss: 929.374719619751\n",
      "step: 3919.0, loss: 931.2784337997437\n",
      "step: 3920.0, loss: 930.9808721542358\n",
      "step: 3921.0, loss: 929.032075881958\n",
      "step: 3922.0, loss: 929.5178031921387\n",
      "step: 3923.0, loss: 931.237135887146\n",
      "step: 3924.0, loss: 929.5777349472046\n",
      "step: 3925.0, loss: 929.6563177108765\n",
      "step: 3926.0, loss: 930.879467010498\n",
      "step: 3927.0, loss: 929.4785108566284\n",
      "step: 3928.0, loss: 928.8357257843018\n",
      "step: 3929.0, loss: 930.6372699737549\n",
      "step: 3930.0, loss: 930.3907194137573\n",
      "step: 3931.0, loss: 930.3784818649292\n",
      "step: 3932.0, loss: 929.5331974029541\n",
      "step: 3933.0, loss: 926.411979675293\n",
      "step: 3934.0, loss: 928.9956960678101\n",
      "step: 3935.0, loss: 929.0000114440918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 3936.0, loss: 932.4290981292725\n",
      "step: 3937.0, loss: 930.9258365631104\n",
      "step: 3938.0, loss: 929.6519794464111\n",
      "step: 3939.0, loss: 930.4063339233398\n",
      "step: 3940.0, loss: 931.1158103942871\n",
      "step: 3941.0, loss: 932.312611579895\n",
      "step: 3942.0, loss: 928.6077117919922\n",
      "step: 3943.0, loss: 929.696270942688\n",
      "step: 3944.0, loss: 930.5576782226562\n",
      "step: 3945.0, loss: 928.913182258606\n",
      "step: 3946.0, loss: 929.3403387069702\n",
      "step: 3947.0, loss: 930.3846883773804\n",
      "step: 3948.0, loss: 927.561206817627\n",
      "step: 3949.0, loss: 927.7757987976074\n",
      "step: 3950.0, loss: 929.7991762161255\n",
      "step: 3951.0, loss: 930.203706741333\n",
      "step: 3952.0, loss: 931.0035715103149\n",
      "step: 3953.0, loss: 930.5503549575806\n",
      "step: 3954.0, loss: 929.7762956619263\n",
      "step: 3955.0, loss: 930.336012840271\n",
      "step: 3956.0, loss: 930.9998397827148\n",
      "step: 3957.0, loss: 929.1129684448242\n",
      "step: 3958.0, loss: 929.4779920578003\n",
      "step: 3959.0, loss: 931.6118030548096\n",
      "step: 3960.0, loss: 928.6691274642944\n",
      "step: 3961.0, loss: 930.3280172348022\n",
      "step: 3962.0, loss: 931.6236534118652\n",
      "step: 3963.0, loss: 931.9689865112305\n",
      "step: 3964.0, loss: 931.6132774353027\n",
      "step: 3965.0, loss: 930.2096042633057\n",
      "step: 3966.0, loss: 929.9582843780518\n",
      "step: 3967.0, loss: 928.8587770462036\n",
      "step: 3968.0, loss: 929.790602684021\n",
      "step: 3969.0, loss: 929.9858493804932\n",
      "step: 3970.0, loss: 929.3883018493652\n",
      "step: 3971.0, loss: 929.3873777389526\n",
      "step: 3972.0, loss: 929.6156587600708\n",
      "step: 3973.0, loss: 930.8397855758667\n",
      "step: 3974.0, loss: 930.7758359909058\n",
      "step: 3975.0, loss: 929.121636390686\n",
      "step: 3976.0, loss: 930.5727138519287\n",
      "step: 3977.0, loss: 930.3948440551758\n",
      "step: 3978.0, loss: 930.1168823242188\n",
      "step: 3979.0, loss: 929.8875532150269\n",
      "step: 3980.0, loss: 929.7109518051147\n",
      "step: 3981.0, loss: 929.7210245132446\n",
      "step: 3982.0, loss: 929.5389347076416\n",
      "step: 3983.0, loss: 930.7367324829102\n",
      "step: 3984.0, loss: 929.5278196334839\n",
      "step: 3985.0, loss: 931.3268241882324\n",
      "step: 3986.0, loss: 929.2683773040771\n",
      "step: 3987.0, loss: 929.5001077651978\n",
      "step: 3988.0, loss: 929.1504487991333\n",
      "step: 3989.0, loss: 929.7701234817505\n",
      "step: 3990.0, loss: 929.6152210235596\n",
      "step: 3991.0, loss: 929.8869171142578\n",
      "step: 3992.0, loss: 931.1364879608154\n",
      "step: 3993.0, loss: 931.9472732543945\n",
      "step: 3994.0, loss: 928.463436126709\n",
      "step: 3995.0, loss: 930.9886674880981\n",
      "step: 3996.0, loss: 929.3331298828125\n",
      "step: 3997.0, loss: 932.9820680618286\n",
      "step: 3998.0, loss: 930.072434425354\n",
      "step: 3999.0, loss: 929.7297849655151\n",
      "step: 4000.0, loss: 931.268895149231\n",
      "step: 4001.0, loss: 932.1672706604004\n",
      "step: 4002.0, loss: 931.142557144165\n",
      "step: 4003.0, loss: 930.168454170227\n",
      "step: 4004.0, loss: 929.2268524169922\n",
      "step: 4005.0, loss: 931.2037477493286\n",
      "step: 4006.0, loss: 929.4902400970459\n",
      "step: 4007.0, loss: 929.4706344604492\n",
      "step: 4008.0, loss: 929.0690145492554\n",
      "step: 4009.0, loss: 930.4189920425415\n",
      "step: 4010.0, loss: 929.8617296218872\n",
      "step: 4011.0, loss: 928.5230522155762\n",
      "step: 4012.0, loss: 930.7575063705444\n",
      "step: 4013.0, loss: 928.4820890426636\n",
      "step: 4014.0, loss: 931.1290874481201\n",
      "step: 4015.0, loss: 930.1407604217529\n",
      "step: 4016.0, loss: 931.4420337677002\n",
      "step: 4017.0, loss: 928.7203931808472\n",
      "step: 4018.0, loss: 928.7803430557251\n",
      "step: 4019.0, loss: 930.6235752105713\n",
      "step: 4020.0, loss: 930.1711187362671\n",
      "step: 4021.0, loss: 931.3629388809204\n",
      "step: 4022.0, loss: 929.5293989181519\n",
      "step: 4023.0, loss: 930.6633911132812\n",
      "step: 4024.0, loss: 931.2690191268921\n",
      "step: 4025.0, loss: 928.4811744689941\n",
      "step: 4026.0, loss: 930.0997314453125\n",
      "step: 4027.0, loss: 930.4949789047241\n",
      "step: 4028.0, loss: 929.7387943267822\n",
      "step: 4029.0, loss: 927.9108333587646\n",
      "step: 4030.0, loss: 928.675121307373\n",
      "step: 4031.0, loss: 930.4867744445801\n",
      "step: 4032.0, loss: 930.5485305786133\n",
      "step: 4033.0, loss: 930.7588577270508\n",
      "step: 4034.0, loss: 931.2308940887451\n",
      "step: 4035.0, loss: 930.3191537857056\n",
      "step: 4036.0, loss: 930.3292999267578\n",
      "step: 4037.0, loss: 929.0540819168091\n",
      "step: 4038.0, loss: 929.5194501876831\n",
      "step: 4039.0, loss: 930.923134803772\n",
      "step: 4040.0, loss: 932.2727298736572\n",
      "step: 4041.0, loss: 931.4410009384155\n",
      "step: 4042.0, loss: 929.7555112838745\n",
      "step: 4043.0, loss: 929.9170980453491\n",
      "step: 4044.0, loss: 930.2773342132568\n",
      "step: 4045.0, loss: 928.2877798080444\n",
      "step: 4046.0, loss: 928.5906200408936\n",
      "step: 4047.0, loss: 928.7581634521484\n",
      "step: 4048.0, loss: 929.8677024841309\n",
      "step: 4049.0, loss: 932.218635559082\n",
      "step: 4050.0, loss: 930.6919660568237\n",
      "step: 4051.0, loss: 930.3297252655029\n",
      "step: 4052.0, loss: 931.0525789260864\n",
      "step: 4053.0, loss: 929.5104217529297\n",
      "step: 4054.0, loss: 928.4480667114258\n",
      "step: 4055.0, loss: 930.640796661377\n",
      "step: 4056.0, loss: 930.1447982788086\n",
      "step: 4057.0, loss: 928.7262144088745\n",
      "step: 4058.0, loss: 930.9611892700195\n",
      "step: 4059.0, loss: 930.7218341827393\n",
      "step: 4060.0, loss: 927.3738241195679\n",
      "step: 4061.0, loss: 929.7315988540649\n",
      "step: 4062.0, loss: 930.0940399169922\n",
      "step: 4063.0, loss: 931.6023988723755\n",
      "step: 4064.0, loss: 930.7037115097046\n",
      "step: 4065.0, loss: 930.3822565078735\n",
      "step: 4066.0, loss: 930.3255271911621\n",
      "step: 4067.0, loss: 930.315710067749\n",
      "step: 4068.0, loss: 931.4227228164673\n",
      "step: 4069.0, loss: 929.696439743042\n",
      "step: 4070.0, loss: 929.6693534851074\n",
      "step: 4071.0, loss: 930.3761405944824\n",
      "step: 4072.0, loss: 928.7573337554932\n",
      "step: 4073.0, loss: 930.1746139526367\n",
      "step: 4074.0, loss: 930.1289138793945\n",
      "step: 4075.0, loss: 928.6650857925415\n",
      "step: 4076.0, loss: 928.8104801177979\n",
      "step: 4077.0, loss: 930.5552034378052\n",
      "step: 4078.0, loss: 930.8828268051147\n",
      "step: 4079.0, loss: 927.035496711731\n",
      "step: 4080.0, loss: 929.2669639587402\n",
      "step: 4081.0, loss: 930.5117063522339\n",
      "step: 4082.0, loss: 932.3701419830322\n",
      "step: 4083.0, loss: 929.8514003753662\n",
      "step: 4084.0, loss: 930.5763149261475\n",
      "step: 4085.0, loss: 929.3527927398682\n",
      "step: 4086.0, loss: 928.119236946106\n",
      "step: 4087.0, loss: 929.1157283782959\n",
      "step: 4088.0, loss: 930.4635515213013\n",
      "step: 4089.0, loss: 928.3725690841675\n",
      "step: 4090.0, loss: 931.9024467468262\n",
      "step: 4091.0, loss: 928.9268932342529\n",
      "step: 4092.0, loss: 930.1255617141724\n",
      "step: 4093.0, loss: 929.8563470840454\n",
      "step: 4094.0, loss: 930.7436618804932\n",
      "step: 4095.0, loss: 930.4595832824707\n",
      "step: 4096.0, loss: 928.566873550415\n",
      "step: 4097.0, loss: 929.9228506088257\n",
      "step: 4098.0, loss: 930.4748687744141\n",
      "step: 4099.0, loss: 928.1955375671387\n",
      "step: 4100.0, loss: 929.1214647293091\n",
      "step: 4101.0, loss: 929.6078519821167\n",
      "step: 4102.0, loss: 929.3313455581665\n",
      "step: 4103.0, loss: 929.8994283676147\n",
      "step: 4104.0, loss: 931.7208566665649\n",
      "step: 4105.0, loss: 929.5113172531128\n",
      "step: 4106.0, loss: 928.9932012557983\n",
      "step: 4107.0, loss: 929.6065626144409\n",
      "step: 4108.0, loss: 931.4588918685913\n",
      "step: 4109.0, loss: 929.8223543167114\n",
      "step: 4110.0, loss: 929.8670673370361\n",
      "step: 4111.0, loss: 930.0138387680054\n",
      "step: 4112.0, loss: 929.0226640701294\n",
      "step: 4113.0, loss: 928.9589147567749\n",
      "step: 4114.0, loss: 930.5907545089722\n",
      "step: 4115.0, loss: 930.2502212524414\n",
      "step: 4116.0, loss: 929.3531541824341\n",
      "step: 4117.0, loss: 929.0078802108765\n",
      "step: 4118.0, loss: 929.6878671646118\n",
      "step: 4119.0, loss: 929.3840951919556\n",
      "step: 4120.0, loss: 928.2866802215576\n",
      "step: 4121.0, loss: 928.7628164291382\n",
      "step: 4122.0, loss: 928.545919418335\n",
      "step: 4123.0, loss: 929.3407039642334\n",
      "step: 4124.0, loss: 929.8469667434692\n",
      "step: 4125.0, loss: 927.843599319458\n",
      "step: 4126.0, loss: 930.090163230896\n",
      "step: 4127.0, loss: 929.8375148773193\n",
      "step: 4128.0, loss: 928.9046249389648\n",
      "step: 4129.0, loss: 927.7254571914673\n",
      "step: 4130.0, loss: 930.9319858551025\n",
      "step: 4131.0, loss: 927.9496412277222\n",
      "step: 4132.0, loss: 928.9603595733643\n",
      "step: 4133.0, loss: 930.4485788345337\n",
      "step: 4134.0, loss: 928.2302103042603\n",
      "step: 4135.0, loss: 928.5141944885254\n",
      "step: 4136.0, loss: 929.4901266098022\n",
      "step: 4137.0, loss: 929.0804538726807\n",
      "step: 4138.0, loss: 930.7344484329224\n",
      "step: 4139.0, loss: 931.1300086975098\n",
      "step: 4140.0, loss: 929.9122638702393\n",
      "step: 4141.0, loss: 930.9261693954468\n",
      "step: 4142.0, loss: 929.9133768081665\n",
      "step: 4143.0, loss: 931.7383756637573\n",
      "step: 4144.0, loss: 929.0693321228027\n",
      "step: 4145.0, loss: 929.3855295181274\n",
      "step: 4146.0, loss: 928.3965167999268\n",
      "step: 4147.0, loss: 929.2704763412476\n",
      "step: 4148.0, loss: 928.411545753479\n",
      "step: 4149.0, loss: 929.0354633331299\n",
      "step: 4150.0, loss: 929.4069881439209\n",
      "step: 4151.0, loss: 928.9262504577637\n",
      "step: 4152.0, loss: 931.0679092407227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 4153.0, loss: 928.57386302948\n",
      "step: 4154.0, loss: 930.424711227417\n",
      "step: 4155.0, loss: 928.3412799835205\n",
      "step: 4156.0, loss: 929.4496793746948\n",
      "step: 4157.0, loss: 928.0555782318115\n",
      "step: 4158.0, loss: 930.7072858810425\n",
      "step: 4159.0, loss: 929.7327394485474\n",
      "step: 4160.0, loss: 929.2076206207275\n",
      "step: 4161.0, loss: 929.6869955062866\n",
      "step: 4162.0, loss: 928.5763626098633\n",
      "step: 4163.0, loss: 928.0258350372314\n",
      "step: 4164.0, loss: 929.4675817489624\n",
      "step: 4165.0, loss: 930.1134366989136\n",
      "step: 4166.0, loss: 927.799220085144\n",
      "step: 4167.0, loss: 929.1633758544922\n",
      "step: 4168.0, loss: 930.2700929641724\n",
      "step: 4169.0, loss: 929.847734451294\n",
      "step: 4170.0, loss: 928.6156139373779\n",
      "step: 4171.0, loss: 930.0815916061401\n",
      "step: 4172.0, loss: 931.226616859436\n",
      "step: 4173.0, loss: 929.5512142181396\n",
      "step: 4174.0, loss: 929.3357992172241\n",
      "step: 4175.0, loss: 929.66175365448\n",
      "step: 4176.0, loss: 926.959846496582\n",
      "step: 4177.0, loss: 929.1335496902466\n",
      "step: 4178.0, loss: 928.9241189956665\n",
      "step: 4179.0, loss: 929.5662679672241\n",
      "step: 4180.0, loss: 927.9855432510376\n",
      "step: 4181.0, loss: 929.1698350906372\n",
      "step: 4182.0, loss: 928.3192691802979\n",
      "step: 4183.0, loss: 930.3331165313721\n",
      "step: 4184.0, loss: 929.1506834030151\n",
      "step: 4185.0, loss: 931.1839923858643\n",
      "step: 4186.0, loss: 930.767804145813\n",
      "step: 4187.0, loss: 928.1549606323242\n",
      "step: 4188.0, loss: 927.6902952194214\n",
      "step: 4189.0, loss: 930.7988548278809\n",
      "step: 4190.0, loss: 929.860053062439\n",
      "step: 4191.0, loss: 929.3019466400146\n",
      "step: 4192.0, loss: 929.8676881790161\n",
      "step: 4193.0, loss: 930.0091190338135\n",
      "step: 4194.0, loss: 928.5376348495483\n",
      "step: 4195.0, loss: 929.7134342193604\n",
      "step: 4196.0, loss: 928.777590751648\n",
      "step: 4197.0, loss: 929.4598798751831\n",
      "step: 4198.0, loss: 929.7620296478271\n",
      "step: 4199.0, loss: 929.5541582107544\n",
      "step: 4200.0, loss: 928.278697013855\n",
      "step: 4201.0, loss: 930.3624830245972\n",
      "step: 4202.0, loss: 928.7420177459717\n",
      "step: 4203.0, loss: 930.2384128570557\n",
      "step: 4204.0, loss: 931.4838600158691\n",
      "step: 4205.0, loss: 929.547604560852\n",
      "step: 4206.0, loss: 927.6557264328003\n",
      "step: 4207.0, loss: 930.6767244338989\n",
      "step: 4208.0, loss: 928.9564542770386\n",
      "step: 4209.0, loss: 928.2762069702148\n",
      "step: 4210.0, loss: 929.9154415130615\n",
      "step: 4211.0, loss: 929.2195796966553\n",
      "step: 4212.0, loss: 928.6778497695923\n",
      "step: 4213.0, loss: 929.7784118652344\n",
      "step: 4214.0, loss: 929.6326370239258\n",
      "step: 4215.0, loss: 927.4610786437988\n",
      "step: 4216.0, loss: 930.4129428863525\n",
      "step: 4217.0, loss: 927.6964931488037\n",
      "step: 4218.0, loss: 927.7563314437866\n",
      "step: 4219.0, loss: 930.1126499176025\n",
      "step: 4220.0, loss: 926.964602470398\n",
      "step: 4221.0, loss: 930.2214956283569\n",
      "step: 4222.0, loss: 928.3642539978027\n",
      "step: 4223.0, loss: 931.4287281036377\n",
      "step: 4224.0, loss: 929.0047798156738\n",
      "step: 4225.0, loss: 929.3225049972534\n",
      "step: 4226.0, loss: 929.8227834701538\n",
      "step: 4227.0, loss: 928.9283380508423\n",
      "step: 4228.0, loss: 929.9626560211182\n",
      "step: 4229.0, loss: 928.7782049179077\n",
      "step: 4230.0, loss: 928.947865486145\n",
      "step: 4231.0, loss: 929.5402002334595\n",
      "step: 4232.0, loss: 929.0510864257812\n",
      "step: 4233.0, loss: 930.7319898605347\n",
      "step: 4234.0, loss: 928.3399534225464\n",
      "step: 4235.0, loss: 929.6826028823853\n",
      "step: 4236.0, loss: 929.5188341140747\n",
      "step: 4237.0, loss: 929.7604732513428\n",
      "step: 4238.0, loss: 930.639702796936\n",
      "step: 4239.0, loss: 929.8436317443848\n",
      "step: 4240.0, loss: 929.6597633361816\n",
      "step: 4241.0, loss: 931.0626640319824\n",
      "step: 4242.0, loss: 929.75465965271\n",
      "step: 4243.0, loss: 927.723949432373\n",
      "step: 4244.0, loss: 928.5975408554077\n",
      "step: 4245.0, loss: 930.0762491226196\n",
      "step: 4246.0, loss: 929.0083808898926\n",
      "step: 4247.0, loss: 930.3016262054443\n",
      "step: 4248.0, loss: 930.6337871551514\n",
      "step: 4249.0, loss: 929.270902633667\n",
      "step: 4250.0, loss: 927.9077787399292\n",
      "step: 4251.0, loss: 929.7516222000122\n",
      "step: 4252.0, loss: 930.4343328475952\n",
      "step: 4253.0, loss: 929.3061237335205\n",
      "step: 4254.0, loss: 930.3946886062622\n",
      "step: 4255.0, loss: 931.8799362182617\n",
      "step: 4256.0, loss: 928.1407690048218\n",
      "step: 4257.0, loss: 928.837306022644\n",
      "step: 4258.0, loss: 930.6435413360596\n",
      "step: 4259.0, loss: 928.206883430481\n",
      "step: 4260.0, loss: 929.8878898620605\n",
      "step: 4261.0, loss: 928.3711795806885\n",
      "step: 4262.0, loss: 929.4607791900635\n",
      "step: 4263.0, loss: 931.6947431564331\n",
      "step: 4264.0, loss: 929.4220323562622\n",
      "step: 4265.0, loss: 929.5957479476929\n",
      "step: 4266.0, loss: 930.8519134521484\n",
      "step: 4267.0, loss: 927.2145347595215\n",
      "step: 4268.0, loss: 928.0356407165527\n",
      "step: 4269.0, loss: 929.429102897644\n",
      "step: 4270.0, loss: 929.4071474075317\n",
      "step: 4271.0, loss: 929.001916885376\n",
      "step: 4272.0, loss: 928.2440586090088\n",
      "step: 4273.0, loss: 929.8488187789917\n",
      "step: 4274.0, loss: 929.9755334854126\n",
      "step: 4275.0, loss: 928.4492168426514\n",
      "step: 4276.0, loss: 930.8070116043091\n",
      "step: 4277.0, loss: 929.0770473480225\n",
      "step: 4278.0, loss: 928.836893081665\n",
      "step: 4279.0, loss: 927.9447288513184\n",
      "step: 4280.0, loss: 929.9767599105835\n",
      "step: 4281.0, loss: 929.9592456817627\n",
      "step: 4282.0, loss: 929.7162628173828\n",
      "step: 4283.0, loss: 929.0983009338379\n",
      "step: 4284.0, loss: 928.9311876296997\n",
      "step: 4285.0, loss: 930.052864074707\n",
      "step: 4286.0, loss: 928.6496915817261\n",
      "step: 4287.0, loss: 928.4894151687622\n",
      "step: 4288.0, loss: 929.1508407592773\n",
      "step: 4289.0, loss: 929.4409923553467\n",
      "step: 4290.0, loss: 929.2920837402344\n",
      "step: 4291.0, loss: 928.8993244171143\n",
      "step: 4292.0, loss: 929.4139804840088\n",
      "step: 4293.0, loss: 929.272572517395\n",
      "step: 4294.0, loss: 929.7053346633911\n",
      "step: 4295.0, loss: 929.8464202880859\n",
      "step: 4296.0, loss: 929.6272926330566\n",
      "step: 4297.0, loss: 928.8995923995972\n",
      "step: 4298.0, loss: 928.2784423828125\n",
      "step: 4299.0, loss: 929.8657598495483\n",
      "step: 4300.0, loss: 927.2917137145996\n",
      "step: 4301.0, loss: 927.8286056518555\n",
      "step: 4302.0, loss: 930.2589263916016\n",
      "step: 4303.0, loss: 929.6718626022339\n",
      "step: 4304.0, loss: 928.3500175476074\n",
      "step: 4305.0, loss: 929.8661766052246\n",
      "step: 4306.0, loss: 928.4947776794434\n",
      "step: 4307.0, loss: 929.9048376083374\n",
      "step: 4308.0, loss: 928.1642656326294\n",
      "step: 4309.0, loss: 928.3335008621216\n",
      "step: 4310.0, loss: 928.4418096542358\n",
      "step: 4311.0, loss: 929.528468132019\n",
      "step: 4312.0, loss: 930.2817401885986\n",
      "step: 4313.0, loss: 929.1567621231079\n",
      "step: 4314.0, loss: 927.4235382080078\n",
      "step: 4315.0, loss: 928.8051700592041\n",
      "step: 4316.0, loss: 928.6572790145874\n",
      "step: 4317.0, loss: 930.7585401535034\n",
      "step: 4318.0, loss: 929.1911811828613\n",
      "step: 4319.0, loss: 929.3644857406616\n",
      "step: 4320.0, loss: 930.2491579055786\n",
      "step: 4321.0, loss: 930.1799926757812\n",
      "step: 4322.0, loss: 929.5031099319458\n",
      "step: 4323.0, loss: 929.760500907898\n",
      "step: 4324.0, loss: 926.7767133712769\n",
      "step: 4325.0, loss: 930.6714715957642\n",
      "step: 4326.0, loss: 927.3578462600708\n",
      "step: 4327.0, loss: 929.811728477478\n",
      "step: 4328.0, loss: 929.2163972854614\n",
      "step: 4329.0, loss: 928.8811454772949\n",
      "step: 4330.0, loss: 928.7126779556274\n",
      "step: 4331.0, loss: 928.9870500564575\n",
      "step: 4332.0, loss: 929.1829252243042\n",
      "step: 4333.0, loss: 928.3421010971069\n",
      "step: 4334.0, loss: 929.4981451034546\n",
      "step: 4335.0, loss: 928.5512838363647\n",
      "step: 4336.0, loss: 929.9718084335327\n",
      "step: 4337.0, loss: 926.2733383178711\n",
      "step: 4338.0, loss: 928.7923965454102\n",
      "step: 4339.0, loss: 928.8564300537109\n",
      "step: 4340.0, loss: 929.4388961791992\n",
      "step: 4341.0, loss: 930.2201776504517\n",
      "step: 4342.0, loss: 930.9922933578491\n",
      "step: 4343.0, loss: 928.7567691802979\n",
      "step: 4344.0, loss: 930.788459777832\n",
      "step: 4345.0, loss: 928.6201629638672\n",
      "step: 4346.0, loss: 929.7684125900269\n",
      "step: 4347.0, loss: 929.9306316375732\n",
      "step: 4348.0, loss: 928.373236656189\n",
      "step: 4349.0, loss: 929.6074419021606\n",
      "step: 4350.0, loss: 930.1163015365601\n",
      "step: 4351.0, loss: 928.5868549346924\n",
      "step: 4352.0, loss: 927.7135238647461\n",
      "step: 4353.0, loss: 929.3793811798096\n",
      "step: 4354.0, loss: 927.8307485580444\n",
      "step: 4355.0, loss: 930.4062757492065\n",
      "step: 4356.0, loss: 928.9101762771606\n",
      "step: 4357.0, loss: 930.0390205383301\n",
      "step: 4358.0, loss: 930.1242971420288\n",
      "step: 4359.0, loss: 930.0180406570435\n",
      "step: 4360.0, loss: 929.9329605102539\n",
      "step: 4361.0, loss: 929.4522256851196\n",
      "step: 4362.0, loss: 929.5121192932129\n",
      "step: 4363.0, loss: 930.2366409301758\n",
      "step: 4364.0, loss: 932.1118354797363\n",
      "step: 4365.0, loss: 928.7145099639893\n",
      "step: 4366.0, loss: 929.848874092102\n",
      "step: 4367.0, loss: 927.7462205886841\n",
      "step: 4368.0, loss: 929.1291084289551\n",
      "step: 4369.0, loss: 928.9832096099854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 4370.0, loss: 927.8826265335083\n",
      "step: 4371.0, loss: 929.7273416519165\n",
      "step: 4372.0, loss: 928.8756866455078\n",
      "step: 4373.0, loss: 930.1254768371582\n",
      "step: 4374.0, loss: 929.4228658676147\n",
      "step: 4375.0, loss: 929.466962814331\n",
      "step: 4376.0, loss: 929.3548736572266\n",
      "step: 4377.0, loss: 927.3959455490112\n",
      "step: 4378.0, loss: 928.8373908996582\n",
      "step: 4379.0, loss: 927.1326560974121\n",
      "step: 4380.0, loss: 928.5503025054932\n",
      "step: 4381.0, loss: 928.4302129745483\n",
      "step: 4382.0, loss: 928.881462097168\n",
      "step: 4383.0, loss: 929.2848262786865\n",
      "step: 4384.0, loss: 929.7127914428711\n",
      "step: 4385.0, loss: 927.7454957962036\n",
      "step: 4386.0, loss: 927.1009120941162\n",
      "step: 4387.0, loss: 927.8554735183716\n",
      "step: 4388.0, loss: 929.4082708358765\n",
      "step: 4389.0, loss: 929.1828603744507\n",
      "step: 4390.0, loss: 928.6387758255005\n",
      "step: 4391.0, loss: 928.1701440811157\n",
      "step: 4392.0, loss: 928.8061170578003\n",
      "step: 4393.0, loss: 928.4895029067993\n",
      "step: 4394.0, loss: 929.6876497268677\n",
      "step: 4395.0, loss: 929.4587182998657\n",
      "step: 4396.0, loss: 930.033748626709\n",
      "step: 4397.0, loss: 929.0932111740112\n",
      "step: 4398.0, loss: 930.5448179244995\n",
      "step: 4399.0, loss: 929.3689613342285\n",
      "step: 4400.0, loss: 927.1617736816406\n",
      "step: 4401.0, loss: 928.5475482940674\n",
      "step: 4402.0, loss: 928.3945903778076\n",
      "step: 4403.0, loss: 927.714825630188\n",
      "step: 4404.0, loss: 928.8772192001343\n",
      "step: 4405.0, loss: 926.4283409118652\n",
      "step: 4406.0, loss: 929.5356731414795\n",
      "step: 4407.0, loss: 929.8056106567383\n",
      "step: 4408.0, loss: 930.4721660614014\n",
      "step: 4409.0, loss: 928.1035976409912\n",
      "step: 4410.0, loss: 926.7902688980103\n",
      "step: 4411.0, loss: 928.5263595581055\n",
      "step: 4412.0, loss: 929.1423349380493\n",
      "step: 4413.0, loss: 930.0896482467651\n",
      "step: 4414.0, loss: 929.1059722900391\n",
      "step: 4415.0, loss: 927.852897644043\n",
      "step: 4416.0, loss: 930.15198802948\n",
      "step: 4417.0, loss: 929.393367767334\n",
      "step: 4418.0, loss: 930.2348966598511\n",
      "step: 4419.0, loss: 928.2851724624634\n",
      "step: 4420.0, loss: 929.8723773956299\n",
      "step: 4421.0, loss: 929.6428775787354\n",
      "step: 4422.0, loss: 930.1457109451294\n",
      "step: 4423.0, loss: 926.6439828872681\n",
      "step: 4424.0, loss: 927.5634355545044\n",
      "step: 4425.0, loss: 927.7926912307739\n",
      "step: 4426.0, loss: 929.4423503875732\n",
      "step: 4427.0, loss: 930.1218214035034\n",
      "step: 4428.0, loss: 928.8781309127808\n",
      "step: 4429.0, loss: 928.7677097320557\n",
      "step: 4430.0, loss: 927.5550985336304\n",
      "step: 4431.0, loss: 927.4741296768188\n",
      "step: 4432.0, loss: 929.5388059616089\n",
      "step: 4433.0, loss: 929.2069416046143\n",
      "step: 4434.0, loss: 928.9017038345337\n",
      "step: 4435.0, loss: 930.0224885940552\n",
      "step: 4436.0, loss: 931.2100315093994\n",
      "step: 4437.0, loss: 929.3702487945557\n",
      "step: 4438.0, loss: 928.0418081283569\n",
      "step: 4439.0, loss: 928.2786235809326\n",
      "step: 4440.0, loss: 929.3514852523804\n",
      "step: 4441.0, loss: 928.5334920883179\n",
      "step: 4442.0, loss: 928.9483308792114\n",
      "step: 4443.0, loss: 927.073429107666\n",
      "step: 4444.0, loss: 930.5687952041626\n",
      "step: 4445.0, loss: 930.3342971801758\n",
      "step: 4446.0, loss: 928.2391300201416\n",
      "step: 4447.0, loss: 929.6362762451172\n",
      "step: 4448.0, loss: 928.1097898483276\n",
      "step: 4449.0, loss: 928.8447380065918\n",
      "step: 4450.0, loss: 929.2129783630371\n",
      "step: 4451.0, loss: 927.4210891723633\n",
      "step: 4452.0, loss: 928.9080429077148\n",
      "step: 4453.0, loss: 930.4499816894531\n",
      "step: 4454.0, loss: 930.2709302902222\n",
      "step: 4455.0, loss: 928.5751943588257\n",
      "step: 4456.0, loss: 928.9405965805054\n",
      "step: 4457.0, loss: 927.1682596206665\n",
      "step: 4458.0, loss: 929.8762807846069\n",
      "step: 4459.0, loss: 929.1096935272217\n",
      "step: 4460.0, loss: 927.2454042434692\n",
      "step: 4461.0, loss: 927.5069799423218\n",
      "step: 4462.0, loss: 928.2262907028198\n",
      "step: 4463.0, loss: 929.7118740081787\n",
      "step: 4464.0, loss: 928.3909645080566\n",
      "step: 4465.0, loss: 928.073971748352\n",
      "step: 4466.0, loss: 931.4425001144409\n",
      "step: 4467.0, loss: 929.2235622406006\n",
      "step: 4468.0, loss: 928.927077293396\n",
      "step: 4469.0, loss: 928.7829313278198\n",
      "step: 4470.0, loss: 927.9984827041626\n",
      "step: 4471.0, loss: 927.5908184051514\n",
      "step: 4472.0, loss: 927.9720592498779\n",
      "step: 4473.0, loss: 928.1348752975464\n",
      "step: 4474.0, loss: 928.9145212173462\n",
      "step: 4475.0, loss: 930.4172487258911\n",
      "step: 4476.0, loss: 928.1603555679321\n",
      "step: 4477.0, loss: 930.2638006210327\n",
      "step: 4478.0, loss: 929.4777345657349\n",
      "step: 4479.0, loss: 930.085319519043\n",
      "step: 4480.0, loss: 928.4198837280273\n",
      "step: 4481.0, loss: 930.6913843154907\n",
      "step: 4482.0, loss: 930.0856828689575\n",
      "step: 4483.0, loss: 930.764274597168\n",
      "step: 4484.0, loss: 930.638650894165\n",
      "step: 4485.0, loss: 927.6686429977417\n",
      "step: 4486.0, loss: 929.2357530593872\n",
      "step: 4487.0, loss: 929.7861928939819\n",
      "step: 4488.0, loss: 927.7876405715942\n",
      "step: 4489.0, loss: 929.095871925354\n",
      "step: 4490.0, loss: 930.5790300369263\n",
      "step: 4491.0, loss: 928.2997159957886\n",
      "step: 4492.0, loss: 929.3644046783447\n",
      "step: 4493.0, loss: 931.4875364303589\n",
      "step: 4494.0, loss: 928.1700201034546\n",
      "step: 4495.0, loss: 928.4388999938965\n",
      "step: 4496.0, loss: 929.2122621536255\n",
      "step: 4497.0, loss: 929.5238418579102\n",
      "step: 4498.0, loss: 929.9747619628906\n",
      "step: 4499.0, loss: 927.2331800460815\n",
      "step: 4500.0, loss: 929.1163682937622\n",
      "step: 4501.0, loss: 931.2490949630737\n",
      "step: 4502.0, loss: 929.7597799301147\n",
      "step: 4503.0, loss: 928.3756828308105\n",
      "step: 4504.0, loss: 926.8373880386353\n",
      "step: 4505.0, loss: 928.480094909668\n",
      "step: 4506.0, loss: 926.5699043273926\n",
      "step: 4507.0, loss: 929.5353918075562\n",
      "step: 4508.0, loss: 926.560754776001\n",
      "step: 4509.0, loss: 928.6649055480957\n",
      "step: 4510.0, loss: 928.0072717666626\n",
      "step: 4511.0, loss: 927.4575967788696\n",
      "step: 4512.0, loss: 929.321213722229\n",
      "step: 4513.0, loss: 928.7667722702026\n",
      "step: 4514.0, loss: 929.0260610580444\n",
      "step: 4515.0, loss: 928.2798776626587\n",
      "step: 4516.0, loss: 929.5662221908569\n",
      "step: 4517.0, loss: 927.8572063446045\n",
      "step: 4518.0, loss: 928.0461101531982\n",
      "step: 4519.0, loss: 928.6209077835083\n",
      "step: 4520.0, loss: 928.9974660873413\n",
      "step: 4521.0, loss: 928.9315824508667\n",
      "step: 4522.0, loss: 929.0462188720703\n",
      "step: 4523.0, loss: 928.08726978302\n",
      "step: 4524.0, loss: 928.1863918304443\n",
      "step: 4525.0, loss: 927.1095275878906\n",
      "step: 4526.0, loss: 928.6007671356201\n",
      "step: 4527.0, loss: 928.9553880691528\n",
      "step: 4528.0, loss: 932.0675802230835\n",
      "step: 4529.0, loss: 929.2445516586304\n",
      "step: 4530.0, loss: 929.816650390625\n",
      "step: 4531.0, loss: 931.2336301803589\n",
      "step: 4532.0, loss: 930.9074182510376\n",
      "step: 4533.0, loss: 928.660943031311\n",
      "step: 4534.0, loss: 929.4451341629028\n",
      "step: 4535.0, loss: 928.4468202590942\n",
      "step: 4536.0, loss: 928.5291471481323\n",
      "step: 4537.0, loss: 927.6670112609863\n",
      "step: 4538.0, loss: 929.6245965957642\n",
      "step: 4539.0, loss: 928.0821924209595\n",
      "step: 4540.0, loss: 928.599100112915\n",
      "step: 4541.0, loss: 928.5402727127075\n",
      "step: 4542.0, loss: 929.0426397323608\n",
      "step: 4543.0, loss: 928.8670635223389\n",
      "step: 4544.0, loss: 927.795895576477\n",
      "step: 4545.0, loss: 930.3723382949829\n",
      "step: 4546.0, loss: 928.9609394073486\n",
      "step: 4547.0, loss: 928.6402988433838\n",
      "step: 4548.0, loss: 929.9865036010742\n",
      "step: 4549.0, loss: 927.0309734344482\n",
      "step: 4550.0, loss: 928.4693021774292\n",
      "step: 4551.0, loss: 930.195592880249\n",
      "step: 4552.0, loss: 926.4671201705933\n",
      "step: 4553.0, loss: 929.8123559951782\n",
      "step: 4554.0, loss: 929.1398105621338\n",
      "step: 4555.0, loss: 927.4404802322388\n",
      "step: 4556.0, loss: 930.3023719787598\n",
      "step: 4557.0, loss: 927.9284658432007\n",
      "step: 4558.0, loss: 929.4263916015625\n",
      "step: 4559.0, loss: 928.0210800170898\n",
      "step: 4560.0, loss: 926.9185743331909\n",
      "step: 4561.0, loss: 928.2230358123779\n",
      "step: 4562.0, loss: 928.044056892395\n",
      "step: 4563.0, loss: 931.147723197937\n",
      "step: 4564.0, loss: 927.6459321975708\n",
      "step: 4565.0, loss: 927.6307287216187\n",
      "step: 4566.0, loss: 928.9968662261963\n",
      "step: 4567.0, loss: 929.2959575653076\n",
      "step: 4568.0, loss: 929.4448337554932\n",
      "step: 4569.0, loss: 929.773491859436\n",
      "step: 4570.0, loss: 927.6314830780029\n",
      "step: 4571.0, loss: 928.9831428527832\n",
      "step: 4572.0, loss: 928.4781265258789\n",
      "step: 4573.0, loss: 928.8032913208008\n",
      "step: 4574.0, loss: 928.3893566131592\n",
      "step: 4575.0, loss: 929.279447555542\n",
      "step: 4576.0, loss: 929.7988710403442\n",
      "step: 4577.0, loss: 927.982744216919\n",
      "step: 4578.0, loss: 928.9960746765137\n",
      "step: 4579.0, loss: 928.166111946106\n",
      "step: 4580.0, loss: 929.8970918655396\n",
      "step: 4581.0, loss: 929.5202035903931\n",
      "step: 4582.0, loss: 929.5093259811401\n",
      "step: 4583.0, loss: 925.8196229934692\n",
      "step: 4584.0, loss: 929.5647010803223\n",
      "step: 4585.0, loss: 928.4525136947632\n",
      "step: 4586.0, loss: 928.6521921157837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 4587.0, loss: 927.5242300033569\n",
      "step: 4588.0, loss: 928.827091217041\n",
      "step: 4589.0, loss: 928.5516872406006\n",
      "step: 4590.0, loss: 929.5521354675293\n",
      "step: 4591.0, loss: 931.1007232666016\n",
      "step: 4592.0, loss: 926.8386001586914\n",
      "step: 4593.0, loss: 928.5071907043457\n",
      "step: 4594.0, loss: 926.7417316436768\n",
      "step: 4595.0, loss: 930.9661321640015\n",
      "step: 4596.0, loss: 929.8210172653198\n",
      "step: 4597.0, loss: 929.6363735198975\n",
      "step: 4598.0, loss: 929.1229028701782\n",
      "step: 4599.0, loss: 928.9426202774048\n",
      "step: 4600.0, loss: 927.6181287765503\n",
      "step: 4601.0, loss: 926.4738988876343\n",
      "step: 4602.0, loss: 928.0473670959473\n",
      "step: 4603.0, loss: 928.7559461593628\n",
      "step: 4604.0, loss: 928.1610355377197\n",
      "step: 4605.0, loss: 929.2332582473755\n",
      "step: 4606.0, loss: 927.6847553253174\n",
      "step: 4607.0, loss: 927.6076946258545\n",
      "step: 4608.0, loss: 927.0197677612305\n",
      "step: 4609.0, loss: 930.1621732711792\n",
      "step: 4610.0, loss: 929.2063570022583\n",
      "step: 4611.0, loss: 928.5281810760498\n",
      "step: 4612.0, loss: 928.3722047805786\n",
      "step: 4613.0, loss: 929.073091506958\n",
      "step: 4614.0, loss: 926.464693069458\n",
      "step: 4615.0, loss: 928.9463262557983\n",
      "step: 4616.0, loss: 926.6301069259644\n",
      "step: 4617.0, loss: 928.7595415115356\n",
      "step: 4618.0, loss: 928.1828527450562\n",
      "step: 4619.0, loss: 929.5555458068848\n",
      "step: 4620.0, loss: 929.1355381011963\n",
      "step: 4621.0, loss: 927.981369972229\n",
      "step: 4622.0, loss: 928.463231086731\n",
      "step: 4623.0, loss: 928.0766258239746\n",
      "step: 4624.0, loss: 928.1038026809692\n",
      "step: 4625.0, loss: 931.6679677963257\n",
      "step: 4626.0, loss: 928.3107213973999\n",
      "step: 4627.0, loss: 927.6755285263062\n",
      "step: 4628.0, loss: 928.4734525680542\n",
      "step: 4629.0, loss: 928.5821561813354\n",
      "step: 4630.0, loss: 928.6010065078735\n",
      "step: 4631.0, loss: 929.9700479507446\n",
      "step: 4632.0, loss: 929.5225811004639\n",
      "step: 4633.0, loss: 931.256609916687\n",
      "step: 4634.0, loss: 927.6289443969727\n",
      "step: 4635.0, loss: 928.1737375259399\n",
      "step: 4636.0, loss: 928.9328184127808\n",
      "step: 4637.0, loss: 927.2302656173706\n",
      "step: 4638.0, loss: 929.0370969772339\n",
      "step: 4639.0, loss: 930.0762624740601\n",
      "step: 4640.0, loss: 928.9204378128052\n",
      "step: 4641.0, loss: 929.1389026641846\n",
      "step: 4642.0, loss: 927.9875822067261\n",
      "step: 4643.0, loss: 927.8364782333374\n",
      "step: 4644.0, loss: 930.733229637146\n",
      "step: 4645.0, loss: 928.4806337356567\n",
      "step: 4646.0, loss: 929.6747636795044\n",
      "step: 4647.0, loss: 929.2444095611572\n",
      "step: 4648.0, loss: 927.8661518096924\n",
      "step: 4649.0, loss: 928.7330627441406\n",
      "step: 4650.0, loss: 928.7834224700928\n",
      "step: 4651.0, loss: 928.186840057373\n",
      "step: 4652.0, loss: 928.0951061248779\n",
      "step: 4653.0, loss: 927.4623117446899\n",
      "step: 4654.0, loss: 928.6161403656006\n",
      "step: 4655.0, loss: 927.5319156646729\n",
      "step: 4656.0, loss: 929.6153726577759\n",
      "step: 4657.0, loss: 929.0333499908447\n",
      "step: 4658.0, loss: 929.8217601776123\n",
      "step: 4659.0, loss: 926.6072835922241\n",
      "step: 4660.0, loss: 928.3441371917725\n",
      "step: 4661.0, loss: 929.3951368331909\n",
      "step: 4662.0, loss: 927.2581615447998\n",
      "step: 4663.0, loss: 927.7703199386597\n",
      "step: 4664.0, loss: 928.9569606781006\n",
      "step: 4665.0, loss: 927.2162485122681\n",
      "step: 4666.0, loss: 929.9755744934082\n",
      "step: 4667.0, loss: 926.3615217208862\n",
      "step: 4668.0, loss: 927.7665452957153\n",
      "step: 4669.0, loss: 929.6131553649902\n",
      "step: 4670.0, loss: 930.039737701416\n",
      "step: 4671.0, loss: 928.457444190979\n",
      "step: 4672.0, loss: 926.0711345672607\n",
      "step: 4673.0, loss: 927.0178623199463\n",
      "step: 4674.0, loss: 926.8079586029053\n",
      "step: 4675.0, loss: 927.9756956100464\n",
      "step: 4676.0, loss: 927.1339912414551\n",
      "step: 4677.0, loss: 927.4227209091187\n",
      "step: 4678.0, loss: 928.7833490371704\n",
      "step: 4679.0, loss: 927.6707229614258\n",
      "step: 4680.0, loss: 928.0427122116089\n",
      "step: 4681.0, loss: 928.5625371932983\n",
      "step: 4682.0, loss: 928.4631872177124\n",
      "step: 4683.0, loss: 926.8703031539917\n",
      "step: 4684.0, loss: 926.6941528320312\n",
      "step: 4685.0, loss: 928.6747970581055\n",
      "step: 4686.0, loss: 925.9056749343872\n",
      "step: 4687.0, loss: 927.2197036743164\n",
      "step: 4688.0, loss: 927.9053611755371\n",
      "step: 4689.0, loss: 928.3686399459839\n",
      "step: 4690.0, loss: 926.347071647644\n",
      "step: 4691.0, loss: 929.0483570098877\n",
      "step: 4692.0, loss: 927.8791179656982\n",
      "step: 4693.0, loss: 929.1481914520264\n",
      "step: 4694.0, loss: 928.8230619430542\n",
      "step: 4695.0, loss: 928.6254959106445\n",
      "step: 4696.0, loss: 930.0082473754883\n",
      "step: 4697.0, loss: 926.3519144058228\n",
      "step: 4698.0, loss: 927.4215507507324\n",
      "step: 4699.0, loss: 929.1831741333008\n",
      "step: 4700.0, loss: 928.3910675048828\n",
      "step: 4701.0, loss: 928.862250328064\n",
      "step: 4702.0, loss: 927.9399642944336\n",
      "step: 4703.0, loss: 930.6413440704346\n",
      "step: 4704.0, loss: 928.2679710388184\n",
      "step: 4705.0, loss: 928.3492851257324\n",
      "step: 4706.0, loss: 927.9170484542847\n",
      "step: 4707.0, loss: 926.3790626525879\n",
      "step: 4708.0, loss: 928.8137979507446\n",
      "step: 4709.0, loss: 927.1733665466309\n",
      "step: 4710.0, loss: 926.7622203826904\n",
      "step: 4711.0, loss: 929.0341806411743\n",
      "step: 4712.0, loss: 927.7636957168579\n",
      "step: 4713.0, loss: 930.1539602279663\n",
      "step: 4714.0, loss: 927.8860025405884\n",
      "step: 4715.0, loss: 927.7785787582397\n",
      "step: 4716.0, loss: 928.4242324829102\n",
      "step: 4717.0, loss: 928.6284799575806\n",
      "step: 4718.0, loss: 928.4107551574707\n",
      "step: 4719.0, loss: 927.8545379638672\n",
      "step: 4720.0, loss: 927.5309677124023\n",
      "step: 4721.0, loss: 929.3695125579834\n",
      "step: 4722.0, loss: 929.8664712905884\n",
      "step: 4723.0, loss: 929.6000366210938\n",
      "step: 4724.0, loss: 927.0924777984619\n",
      "step: 4725.0, loss: 929.3518962860107\n",
      "step: 4726.0, loss: 928.0034122467041\n",
      "step: 4727.0, loss: 928.6978664398193\n",
      "step: 4728.0, loss: 927.9103384017944\n",
      "step: 4729.0, loss: 929.3837451934814\n",
      "step: 4730.0, loss: 927.6165752410889\n",
      "step: 4731.0, loss: 929.2961530685425\n",
      "step: 4732.0, loss: 929.2216577529907\n",
      "step: 4733.0, loss: 929.2029399871826\n",
      "step: 4734.0, loss: 928.8719511032104\n",
      "step: 4735.0, loss: 926.054967880249\n",
      "step: 4736.0, loss: 929.567759513855\n",
      "step: 4737.0, loss: 929.3588552474976\n",
      "step: 4738.0, loss: 928.5001220703125\n",
      "step: 4739.0, loss: 927.8667230606079\n",
      "step: 4740.0, loss: 929.7562761306763\n",
      "step: 4741.0, loss: 927.5136547088623\n",
      "step: 4742.0, loss: 929.6749849319458\n",
      "step: 4743.0, loss: 926.9383335113525\n",
      "step: 4744.0, loss: 928.2183141708374\n",
      "step: 4745.0, loss: 928.5508546829224\n",
      "step: 4746.0, loss: 927.7261371612549\n",
      "step: 4747.0, loss: 926.5048561096191\n",
      "step: 4748.0, loss: 928.5751419067383\n",
      "step: 4749.0, loss: 929.0437517166138\n",
      "step: 4750.0, loss: 928.9421453475952\n",
      "step: 4751.0, loss: 927.813362121582\n",
      "step: 4752.0, loss: 927.672233581543\n",
      "step: 4753.0, loss: 929.5085468292236\n",
      "step: 4754.0, loss: 927.385684967041\n",
      "step: 4755.0, loss: 929.4475650787354\n",
      "step: 4756.0, loss: 929.7652578353882\n",
      "step: 4757.0, loss: 929.0354356765747\n",
      "step: 4758.0, loss: 928.7496395111084\n",
      "step: 4759.0, loss: 927.7248449325562\n",
      "step: 4760.0, loss: 928.5960121154785\n",
      "step: 4761.0, loss: 927.9051675796509\n",
      "step: 4762.0, loss: 927.5865831375122\n",
      "step: 4763.0, loss: 927.0664587020874\n",
      "step: 4764.0, loss: 926.5740089416504\n",
      "step: 4765.0, loss: 927.9119729995728\n",
      "step: 4766.0, loss: 929.7367191314697\n",
      "step: 4767.0, loss: 928.3067083358765\n",
      "step: 4768.0, loss: 930.2111806869507\n",
      "step: 4769.0, loss: 927.4563999176025\n",
      "step: 4770.0, loss: 927.3231019973755\n",
      "step: 4771.0, loss: 928.1392221450806\n",
      "step: 4772.0, loss: 929.7669668197632\n",
      "step: 4773.0, loss: 928.3389024734497\n",
      "step: 4774.0, loss: 926.5183944702148\n",
      "step: 4775.0, loss: 928.4363975524902\n",
      "step: 4776.0, loss: 928.7343311309814\n",
      "step: 4777.0, loss: 927.0239200592041\n",
      "step: 4778.0, loss: 927.4619989395142\n",
      "step: 4779.0, loss: 926.6874628067017\n",
      "step: 4780.0, loss: 928.3785943984985\n",
      "step: 4781.0, loss: 928.9309520721436\n",
      "step: 4782.0, loss: 927.5216989517212\n",
      "step: 4783.0, loss: 928.9132432937622\n",
      "step: 4784.0, loss: 927.4678497314453\n",
      "step: 4785.0, loss: 926.2494583129883\n",
      "step: 4786.0, loss: 928.1999168395996\n",
      "step: 4787.0, loss: 928.6787986755371\n",
      "step: 4788.0, loss: 926.4340448379517\n",
      "step: 4789.0, loss: 930.2074184417725\n",
      "step: 4790.0, loss: 926.6433210372925\n",
      "step: 4791.0, loss: 927.3803148269653\n",
      "step: 4792.0, loss: 930.7955083847046\n",
      "step: 4793.0, loss: 927.8816518783569\n",
      "step: 4794.0, loss: 929.2807769775391\n",
      "step: 4795.0, loss: 927.0355587005615\n",
      "step: 4796.0, loss: 929.7077331542969\n",
      "step: 4797.0, loss: 929.264820098877\n",
      "step: 4798.0, loss: 927.0963363647461\n",
      "step: 4799.0, loss: 928.3178253173828\n",
      "step: 4800.0, loss: 927.4525537490845\n",
      "step: 4801.0, loss: 930.02707862854\n",
      "step: 4802.0, loss: 929.3592348098755\n",
      "step: 4803.0, loss: 928.0646381378174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 4804.0, loss: 928.6363801956177\n",
      "step: 4805.0, loss: 927.9791383743286\n",
      "step: 4806.0, loss: 927.3365364074707\n",
      "step: 4807.0, loss: 928.7543659210205\n",
      "step: 4808.0, loss: 928.217568397522\n",
      "step: 4809.0, loss: 926.7154359817505\n",
      "step: 4810.0, loss: 926.7655658721924\n",
      "step: 4811.0, loss: 927.8786745071411\n",
      "step: 4812.0, loss: 928.5929737091064\n",
      "step: 4813.0, loss: 928.2302780151367\n",
      "step: 4814.0, loss: 927.8473434448242\n",
      "step: 4815.0, loss: 926.7913627624512\n",
      "step: 4816.0, loss: 926.7819900512695\n",
      "step: 4817.0, loss: 928.3995027542114\n",
      "step: 4818.0, loss: 926.0919828414917\n",
      "step: 4819.0, loss: 926.9299125671387\n",
      "step: 4820.0, loss: 929.0039691925049\n",
      "step: 4821.0, loss: 930.0972757339478\n",
      "step: 4822.0, loss: 929.0681819915771\n",
      "step: 4823.0, loss: 927.6367702484131\n",
      "step: 4824.0, loss: 929.5845651626587\n",
      "step: 4825.0, loss: 929.5541858673096\n",
      "step: 4826.0, loss: 928.1742105484009\n",
      "step: 4827.0, loss: 929.3362474441528\n",
      "step: 4828.0, loss: 930.0488452911377\n",
      "step: 4829.0, loss: 929.0548067092896\n",
      "step: 4830.0, loss: 927.8740930557251\n",
      "step: 4831.0, loss: 927.6064834594727\n",
      "step: 4832.0, loss: 927.256386756897\n",
      "step: 4833.0, loss: 927.5205230712891\n",
      "step: 4834.0, loss: 926.4525108337402\n",
      "step: 4835.0, loss: 927.792272567749\n",
      "step: 4836.0, loss: 928.0844326019287\n",
      "step: 4837.0, loss: 928.7597713470459\n",
      "step: 4838.0, loss: 928.8456478118896\n",
      "step: 4839.0, loss: 928.4086408615112\n",
      "step: 4840.0, loss: 928.4782438278198\n",
      "step: 4841.0, loss: 928.9033451080322\n",
      "step: 4842.0, loss: 929.3815307617188\n",
      "step: 4843.0, loss: 928.3117456436157\n",
      "step: 4844.0, loss: 927.7514381408691\n",
      "step: 4845.0, loss: 927.9944448471069\n",
      "step: 4846.0, loss: 927.0543041229248\n",
      "step: 4847.0, loss: 927.7191963195801\n",
      "step: 4848.0, loss: 929.1228942871094\n",
      "step: 4849.0, loss: 928.5100221633911\n",
      "step: 4850.0, loss: 926.4471473693848\n",
      "step: 4851.0, loss: 928.473744392395\n",
      "step: 4852.0, loss: 925.7739953994751\n",
      "step: 4853.0, loss: 928.4490003585815\n",
      "step: 4854.0, loss: 931.4870100021362\n",
      "step: 4855.0, loss: 926.2408981323242\n",
      "step: 4856.0, loss: 928.6213722229004\n",
      "step: 4857.0, loss: 929.9316549301147\n",
      "step: 4858.0, loss: 929.5190324783325\n",
      "step: 4859.0, loss: 929.4591283798218\n",
      "step: 4860.0, loss: 928.5305967330933\n",
      "step: 4861.0, loss: 926.4327268600464\n",
      "step: 4862.0, loss: 928.0649471282959\n",
      "step: 4863.0, loss: 927.9144849777222\n",
      "step: 4864.0, loss: 928.9771738052368\n",
      "step: 4865.0, loss: 927.7519245147705\n",
      "step: 4866.0, loss: 927.84264087677\n",
      "step: 4867.0, loss: 927.152515411377\n",
      "step: 4868.0, loss: 929.567548751831\n",
      "step: 4869.0, loss: 927.5320510864258\n",
      "step: 4870.0, loss: 927.4928741455078\n",
      "step: 4871.0, loss: 927.9464883804321\n",
      "step: 4872.0, loss: 927.908971786499\n",
      "step: 4873.0, loss: 927.6002759933472\n",
      "step: 4874.0, loss: 928.1160249710083\n",
      "step: 4875.0, loss: 929.0403432846069\n",
      "step: 4876.0, loss: 929.758867263794\n",
      "step: 4877.0, loss: 929.4735412597656\n",
      "step: 4878.0, loss: 928.3672161102295\n",
      "step: 4879.0, loss: 928.181981086731\n",
      "step: 4880.0, loss: 926.5311517715454\n",
      "step: 4881.0, loss: 928.190544128418\n",
      "step: 4882.0, loss: 926.9161329269409\n",
      "step: 4883.0, loss: 928.5025882720947\n",
      "step: 4884.0, loss: 928.7912321090698\n",
      "step: 4885.0, loss: 927.740406036377\n",
      "step: 4886.0, loss: 927.6334924697876\n",
      "step: 4887.0, loss: 927.5633850097656\n",
      "step: 4888.0, loss: 929.8184242248535\n",
      "step: 4889.0, loss: 929.6671094894409\n",
      "step: 4890.0, loss: 929.4743995666504\n",
      "step: 4891.0, loss: 929.4720344543457\n",
      "step: 4892.0, loss: 927.9490728378296\n",
      "step: 4893.0, loss: 928.3832025527954\n",
      "step: 4894.0, loss: 927.3214731216431\n",
      "step: 4895.0, loss: 926.2971963882446\n",
      "step: 4896.0, loss: 928.1044454574585\n",
      "step: 4897.0, loss: 928.9059114456177\n",
      "step: 4898.0, loss: 927.9426584243774\n",
      "step: 4899.0, loss: 926.8369703292847\n",
      "step: 4900.0, loss: 927.6963663101196\n",
      "step: 4901.0, loss: 927.9285402297974\n",
      "step: 4902.0, loss: 928.3616323471069\n",
      "step: 4903.0, loss: 926.9568452835083\n",
      "step: 4904.0, loss: 928.6655893325806\n",
      "step: 4905.0, loss: 928.3437023162842\n",
      "step: 4906.0, loss: 928.5858335494995\n",
      "step: 4907.0, loss: 929.0955696105957\n",
      "step: 4908.0, loss: 928.8860664367676\n",
      "step: 4909.0, loss: 929.1001415252686\n",
      "step: 4910.0, loss: 928.6171636581421\n",
      "step: 4911.0, loss: 926.1719312667847\n",
      "step: 4912.0, loss: 926.1964712142944\n",
      "step: 4913.0, loss: 928.2648286819458\n",
      "step: 4914.0, loss: 929.4282026290894\n",
      "step: 4915.0, loss: 929.446662902832\n",
      "step: 4916.0, loss: 929.8361349105835\n",
      "step: 4917.0, loss: 927.2815542221069\n",
      "step: 4918.0, loss: 929.2906894683838\n",
      "step: 4919.0, loss: 925.7289514541626\n",
      "step: 4920.0, loss: 929.2924938201904\n",
      "step: 4921.0, loss: 927.9720430374146\n",
      "step: 4922.0, loss: 927.5228729248047\n",
      "step: 4923.0, loss: 926.9335680007935\n",
      "step: 4924.0, loss: 925.8231468200684\n",
      "step: 4925.0, loss: 929.231122970581\n",
      "step: 4926.0, loss: 928.0973386764526\n",
      "step: 4927.0, loss: 927.7671041488647\n",
      "step: 4928.0, loss: 927.0188274383545\n",
      "step: 4929.0, loss: 928.3184671401978\n",
      "step: 4930.0, loss: 928.7291145324707\n",
      "step: 4931.0, loss: 928.4815835952759\n",
      "step: 4932.0, loss: 927.7881622314453\n",
      "step: 4933.0, loss: 926.632607460022\n",
      "step: 4934.0, loss: 928.3572397232056\n",
      "step: 4935.0, loss: 928.9681224822998\n",
      "step: 4936.0, loss: 928.2425537109375\n",
      "step: 4937.0, loss: 927.822509765625\n",
      "step: 4938.0, loss: 928.8334054946899\n",
      "step: 4939.0, loss: 929.2989749908447\n",
      "step: 4940.0, loss: 929.7652721405029\n",
      "step: 4941.0, loss: 928.4376010894775\n",
      "step: 4942.0, loss: 926.5432319641113\n",
      "step: 4943.0, loss: 927.2362089157104\n",
      "step: 4944.0, loss: 927.6097240447998\n",
      "step: 4945.0, loss: 927.4294557571411\n",
      "step: 4946.0, loss: 928.5277719497681\n",
      "step: 4947.0, loss: 928.2587461471558\n",
      "step: 4948.0, loss: 927.5492277145386\n",
      "step: 4949.0, loss: 931.8828020095825\n",
      "step: 4950.0, loss: 926.793267250061\n",
      "step: 4951.0, loss: 930.3721303939819\n",
      "step: 4952.0, loss: 928.8798589706421\n",
      "step: 4953.0, loss: 927.5156450271606\n",
      "step: 4954.0, loss: 930.8015079498291\n",
      "step: 4955.0, loss: 928.8579416275024\n",
      "step: 4956.0, loss: 925.8774557113647\n",
      "step: 4957.0, loss: 927.5964727401733\n",
      "step: 4958.0, loss: 927.4293041229248\n",
      "step: 4959.0, loss: 930.4049339294434\n",
      "step: 4960.0, loss: 926.8209609985352\n",
      "step: 4961.0, loss: 927.406569480896\n",
      "step: 4962.0, loss: 928.4651498794556\n",
      "step: 4963.0, loss: 926.596248626709\n",
      "step: 4964.0, loss: 927.3637437820435\n",
      "step: 4965.0, loss: 928.7507581710815\n",
      "step: 4966.0, loss: 928.659200668335\n",
      "step: 4967.0, loss: 926.659613609314\n",
      "step: 4968.0, loss: 927.6058292388916\n",
      "step: 4969.0, loss: 928.734058380127\n",
      "step: 4970.0, loss: 927.0453405380249\n",
      "step: 4971.0, loss: 927.9275703430176\n",
      "step: 4972.0, loss: 927.5632953643799\n",
      "step: 4973.0, loss: 927.6218271255493\n",
      "step: 4974.0, loss: 928.7039785385132\n",
      "step: 4975.0, loss: 928.9640407562256\n",
      "step: 4976.0, loss: 926.7034587860107\n",
      "step: 4977.0, loss: 927.8768529891968\n",
      "step: 4978.0, loss: 929.00266456604\n",
      "step: 4979.0, loss: 927.2950134277344\n",
      "step: 4980.0, loss: 928.6863660812378\n",
      "step: 4981.0, loss: 928.8100309371948\n",
      "step: 4982.0, loss: 928.2758979797363\n",
      "step: 4983.0, loss: 927.1838531494141\n",
      "step: 4984.0, loss: 929.2855863571167\n",
      "step: 4985.0, loss: 927.462911605835\n",
      "step: 4986.0, loss: 928.208080291748\n",
      "step: 4987.0, loss: 927.0549421310425\n",
      "step: 4988.0, loss: 928.4500026702881\n",
      "step: 4989.0, loss: 926.3124179840088\n",
      "step: 4990.0, loss: 927.2750186920166\n",
      "step: 4991.0, loss: 928.4906673431396\n",
      "step: 4992.0, loss: 928.32506275177\n",
      "step: 4993.0, loss: 928.792552947998\n",
      "step: 4994.0, loss: 929.1043281555176\n",
      "step: 4995.0, loss: 927.6754264831543\n",
      "step: 4996.0, loss: 926.0085678100586\n",
      "step: 4997.0, loss: 929.3535394668579\n",
      "step: 4998.0, loss: 928.0024976730347\n",
      "step: 4999.0, loss: 928.1102533340454\n",
      "step: 5000.0, loss: 928.7022218704224\n",
      "step: 5001.0, loss: 927.142255783081\n",
      "step: 5002.0, loss: 928.3652648925781\n",
      "step: 5003.0, loss: 928.4053773880005\n",
      "step: 5004.0, loss: 926.5880107879639\n",
      "step: 5005.0, loss: 925.9239530563354\n",
      "step: 5006.0, loss: 926.952672958374\n",
      "step: 5007.0, loss: 928.6621150970459\n",
      "step: 5008.0, loss: 928.9602270126343\n",
      "step: 5009.0, loss: 929.5947132110596\n",
      "step: 5010.0, loss: 927.8361225128174\n",
      "step: 5011.0, loss: 928.3694524765015\n",
      "step: 5012.0, loss: 927.6048736572266\n",
      "step: 5013.0, loss: 927.6878070831299\n",
      "step: 5014.0, loss: 926.1624889373779\n",
      "step: 5015.0, loss: 927.8024015426636\n",
      "step: 5016.0, loss: 926.6602439880371\n",
      "step: 5017.0, loss: 927.7369718551636\n",
      "step: 5018.0, loss: 928.0703887939453\n",
      "step: 5019.0, loss: 927.5908651351929\n",
      "step: 5020.0, loss: 929.2208108901978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 5021.0, loss: 928.3972940444946\n",
      "step: 5022.0, loss: 927.0612745285034\n",
      "step: 5023.0, loss: 927.1076650619507\n",
      "step: 5024.0, loss: 928.4850416183472\n",
      "step: 5025.0, loss: 926.9273319244385\n",
      "step: 5026.0, loss: 929.258469581604\n",
      "step: 5027.0, loss: 925.7146406173706\n",
      "step: 5028.0, loss: 927.5148668289185\n",
      "step: 5029.0, loss: 927.6539354324341\n",
      "step: 5030.0, loss: 926.7701587677002\n",
      "step: 5031.0, loss: 927.7277793884277\n",
      "step: 5032.0, loss: 927.0305166244507\n",
      "step: 5033.0, loss: 925.9129209518433\n",
      "step: 5034.0, loss: 928.6880226135254\n",
      "step: 5035.0, loss: 926.2261610031128\n",
      "step: 5036.0, loss: 928.351879119873\n",
      "step: 5037.0, loss: 930.9259872436523\n",
      "step: 5038.0, loss: 928.3978385925293\n",
      "step: 5039.0, loss: 928.0360040664673\n",
      "step: 5040.0, loss: 926.0653114318848\n",
      "step: 5041.0, loss: 929.890832901001\n",
      "step: 5042.0, loss: 930.0204162597656\n",
      "step: 5043.0, loss: 927.5476980209351\n",
      "step: 5044.0, loss: 928.1413526535034\n",
      "step: 5045.0, loss: 929.2116899490356\n",
      "step: 5046.0, loss: 925.6990623474121\n",
      "step: 5047.0, loss: 929.3690881729126\n",
      "step: 5048.0, loss: 926.9260120391846\n",
      "step: 5049.0, loss: 927.9465923309326\n",
      "step: 5050.0, loss: 927.6345291137695\n",
      "step: 5051.0, loss: 926.6205863952637\n",
      "step: 5052.0, loss: 926.8022813796997\n",
      "step: 5053.0, loss: 928.4195537567139\n",
      "step: 5054.0, loss: 928.3426027297974\n",
      "step: 5055.0, loss: 927.8201389312744\n",
      "step: 5056.0, loss: 928.6221256256104\n",
      "step: 5057.0, loss: 927.3540811538696\n",
      "step: 5058.0, loss: 926.9806642532349\n",
      "step: 5059.0, loss: 927.1743669509888\n",
      "step: 5060.0, loss: 929.3290042877197\n",
      "step: 5061.0, loss: 928.6377763748169\n",
      "step: 5062.0, loss: 928.0002851486206\n",
      "step: 5063.0, loss: 927.448733329773\n",
      "step: 5064.0, loss: 927.8548269271851\n",
      "step: 5065.0, loss: 927.9646434783936\n",
      "step: 5066.0, loss: 928.4560403823853\n",
      "step: 5067.0, loss: 930.099401473999\n",
      "step: 5068.0, loss: 926.636589050293\n",
      "step: 5069.0, loss: 929.0316171646118\n",
      "step: 5070.0, loss: 927.2344741821289\n",
      "step: 5071.0, loss: 927.7863054275513\n",
      "step: 5072.0, loss: 925.4961004257202\n",
      "step: 5073.0, loss: 925.3008680343628\n",
      "step: 5074.0, loss: 927.2068271636963\n",
      "step: 5075.0, loss: 928.2243223190308\n",
      "step: 5076.0, loss: 925.9432687759399\n",
      "step: 5077.0, loss: 927.7824935913086\n",
      "step: 5078.0, loss: 927.1534433364868\n",
      "step: 5079.0, loss: 928.11279296875\n",
      "step: 5080.0, loss: 927.1956787109375\n",
      "step: 5081.0, loss: 927.911548614502\n",
      "step: 5082.0, loss: 929.1105842590332\n",
      "step: 5083.0, loss: 924.9542503356934\n",
      "step: 5084.0, loss: 930.2228851318359\n",
      "step: 5085.0, loss: 927.9934873580933\n",
      "step: 5086.0, loss: 929.8512020111084\n",
      "step: 5087.0, loss: 927.8799161911011\n",
      "step: 5088.0, loss: 928.1015548706055\n",
      "step: 5089.0, loss: 927.9361352920532\n",
      "step: 5090.0, loss: 927.2084245681763\n",
      "step: 5091.0, loss: 928.4156684875488\n",
      "step: 5092.0, loss: 927.8540754318237\n",
      "step: 5093.0, loss: 929.2134675979614\n",
      "step: 5094.0, loss: 925.9468202590942\n",
      "step: 5095.0, loss: 929.1121530532837\n",
      "step: 5096.0, loss: 928.1484966278076\n",
      "step: 5097.0, loss: 926.6182680130005\n",
      "step: 5098.0, loss: 926.418378829956\n",
      "step: 5099.0, loss: 929.1000204086304\n",
      "step: 5100.0, loss: 928.0028963088989\n",
      "step: 5101.0, loss: 927.1365776062012\n",
      "step: 5102.0, loss: 927.1054220199585\n",
      "step: 5103.0, loss: 926.4810447692871\n",
      "step: 5104.0, loss: 927.6578226089478\n",
      "step: 5105.0, loss: 928.6302766799927\n",
      "step: 5106.0, loss: 927.7224645614624\n",
      "step: 5107.0, loss: 926.9917888641357\n",
      "step: 5108.0, loss: 928.9224863052368\n",
      "step: 5109.0, loss: 926.8872766494751\n",
      "step: 5110.0, loss: 927.3237829208374\n",
      "step: 5111.0, loss: 926.5238313674927\n",
      "step: 5112.0, loss: 926.8875398635864\n",
      "step: 5113.0, loss: 926.3784227371216\n",
      "step: 5114.0, loss: 926.7082662582397\n",
      "step: 5115.0, loss: 928.9739179611206\n",
      "step: 5116.0, loss: 927.7741527557373\n",
      "step: 5117.0, loss: 926.5092153549194\n",
      "step: 5118.0, loss: 926.6314306259155\n",
      "step: 5119.0, loss: 925.9673299789429\n",
      "step: 5120.0, loss: 926.8115720748901\n",
      "step: 5121.0, loss: 925.9989919662476\n",
      "step: 5122.0, loss: 927.9664058685303\n",
      "step: 5123.0, loss: 927.6454734802246\n",
      "step: 5124.0, loss: 928.6380739212036\n",
      "step: 5125.0, loss: 927.348316192627\n",
      "step: 5126.0, loss: 928.0715618133545\n",
      "step: 5127.0, loss: 928.2785339355469\n",
      "step: 5128.0, loss: 928.1682424545288\n",
      "step: 5129.0, loss: 928.1037893295288\n",
      "step: 5130.0, loss: 929.3572664260864\n",
      "step: 5131.0, loss: 927.9979009628296\n",
      "step: 5132.0, loss: 927.6426105499268\n",
      "step: 5133.0, loss: 926.7749557495117\n",
      "step: 5134.0, loss: 927.5340566635132\n",
      "step: 5135.0, loss: 928.8073320388794\n",
      "step: 5136.0, loss: 926.5682878494263\n",
      "step: 5137.0, loss: 927.4124507904053\n",
      "step: 5138.0, loss: 928.9215326309204\n",
      "step: 5139.0, loss: 928.7931671142578\n",
      "step: 5140.0, loss: 926.3908386230469\n",
      "step: 5141.0, loss: 927.2840328216553\n",
      "step: 5142.0, loss: 927.9764757156372\n",
      "step: 5143.0, loss: 926.4195098876953\n",
      "step: 5144.0, loss: 927.9658346176147\n",
      "step: 5145.0, loss: 927.6542081832886\n",
      "step: 5146.0, loss: 927.8369464874268\n",
      "step: 5147.0, loss: 927.8313035964966\n",
      "step: 5148.0, loss: 926.8678789138794\n",
      "step: 5149.0, loss: 928.3104095458984\n",
      "step: 5150.0, loss: 925.4110841751099\n",
      "step: 5151.0, loss: 927.2236785888672\n",
      "step: 5152.0, loss: 927.9634647369385\n",
      "step: 5153.0, loss: 925.7243804931641\n",
      "step: 5154.0, loss: 928.7863759994507\n",
      "step: 5155.0, loss: 929.4636249542236\n",
      "step: 5156.0, loss: 927.6183414459229\n",
      "step: 5157.0, loss: 927.7661094665527\n",
      "step: 5158.0, loss: 927.5705413818359\n",
      "step: 5159.0, loss: 927.8041696548462\n",
      "step: 5160.0, loss: 928.1455154418945\n",
      "step: 5161.0, loss: 929.5560522079468\n",
      "step: 5162.0, loss: 927.8480730056763\n",
      "step: 5163.0, loss: 927.1679906845093\n",
      "step: 5164.0, loss: 928.0645008087158\n",
      "step: 5165.0, loss: 926.8480892181396\n",
      "step: 5166.0, loss: 927.7583570480347\n",
      "step: 5167.0, loss: 928.6516380310059\n",
      "step: 5168.0, loss: 928.8240346908569\n",
      "step: 5169.0, loss: 927.4842262268066\n",
      "step: 5170.0, loss: 927.3149738311768\n",
      "step: 5171.0, loss: 927.9442453384399\n",
      "step: 5172.0, loss: 927.1362981796265\n",
      "step: 5173.0, loss: 928.2836494445801\n",
      "step: 5174.0, loss: 926.7183523178101\n",
      "step: 5175.0, loss: 928.7892026901245\n",
      "step: 5176.0, loss: 928.4689836502075\n",
      "step: 5177.0, loss: 925.9336357116699\n",
      "step: 5178.0, loss: 926.1897554397583\n",
      "step: 5179.0, loss: 927.8200931549072\n",
      "step: 5180.0, loss: 926.8872365951538\n",
      "step: 5181.0, loss: 927.7235441207886\n",
      "step: 5182.0, loss: 926.5840425491333\n",
      "step: 5183.0, loss: 925.5793304443359\n",
      "step: 5184.0, loss: 926.8627872467041\n",
      "step: 5185.0, loss: 927.5138816833496\n",
      "step: 5186.0, loss: 928.1005716323853\n",
      "step: 5187.0, loss: 927.8908348083496\n",
      "step: 5188.0, loss: 926.3761434555054\n",
      "step: 5189.0, loss: 927.7417821884155\n",
      "step: 5190.0, loss: 926.7105255126953\n",
      "step: 5191.0, loss: 929.3391904830933\n",
      "step: 5192.0, loss: 927.0789394378662\n",
      "step: 5193.0, loss: 926.9230394363403\n",
      "step: 5194.0, loss: 927.451940536499\n",
      "step: 5195.0, loss: 927.143313407898\n",
      "step: 5196.0, loss: 928.3380241394043\n",
      "step: 5197.0, loss: 928.9403858184814\n",
      "step: 5198.0, loss: 928.6678457260132\n",
      "step: 5199.0, loss: 927.602614402771\n",
      "step: 5200.0, loss: 925.8108997344971\n",
      "step: 5201.0, loss: 927.9130048751831\n",
      "step: 5202.0, loss: 927.4010887145996\n",
      "step: 5203.0, loss: 927.409649848938\n",
      "step: 5204.0, loss: 927.3690509796143\n",
      "step: 5205.0, loss: 928.9338436126709\n",
      "step: 5206.0, loss: 927.5043411254883\n",
      "step: 5207.0, loss: 929.0070085525513\n",
      "step: 5208.0, loss: 927.3837738037109\n",
      "step: 5209.0, loss: 926.5077819824219\n",
      "step: 5210.0, loss: 928.0576829910278\n",
      "step: 5211.0, loss: 927.912389755249\n",
      "step: 5212.0, loss: 927.4054708480835\n",
      "step: 5213.0, loss: 927.4619464874268\n",
      "step: 5214.0, loss: 925.9197082519531\n",
      "step: 5215.0, loss: 929.5757732391357\n",
      "step: 5216.0, loss: 928.8607530593872\n",
      "step: 5217.0, loss: 927.4432821273804\n",
      "step: 5218.0, loss: 926.6164455413818\n",
      "step: 5219.0, loss: 927.1233644485474\n",
      "step: 5220.0, loss: 926.1483821868896\n",
      "step: 5221.0, loss: 927.6458492279053\n",
      "step: 5222.0, loss: 927.4483890533447\n",
      "step: 5223.0, loss: 928.520302772522\n",
      "step: 5224.0, loss: 927.7004623413086\n",
      "step: 5225.0, loss: 928.6688280105591\n",
      "step: 5226.0, loss: 926.2822465896606\n",
      "step: 5227.0, loss: 927.9860305786133\n",
      "step: 5228.0, loss: 927.0712480545044\n",
      "step: 5229.0, loss: 927.5806293487549\n",
      "step: 5230.0, loss: 928.4879350662231\n",
      "step: 5231.0, loss: 926.8175439834595\n",
      "step: 5232.0, loss: 927.0159215927124\n",
      "step: 5233.0, loss: 923.8141164779663\n",
      "step: 5234.0, loss: 927.444504737854\n",
      "step: 5235.0, loss: 926.273289680481\n",
      "step: 5236.0, loss: 926.4551753997803\n",
      "step: 5237.0, loss: 926.9336366653442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 5238.0, loss: 928.3800859451294\n",
      "step: 5239.0, loss: 927.9781303405762\n",
      "step: 5240.0, loss: 928.3550357818604\n",
      "step: 5241.0, loss: 928.0846166610718\n",
      "step: 5242.0, loss: 927.5406503677368\n",
      "step: 5243.0, loss: 926.002462387085\n",
      "step: 5244.0, loss: 927.5350036621094\n",
      "step: 5245.0, loss: 928.0909585952759\n",
      "step: 5246.0, loss: 928.608829498291\n",
      "step: 5247.0, loss: 926.7859735488892\n",
      "step: 5248.0, loss: 927.2096862792969\n",
      "step: 5249.0, loss: 927.0492372512817\n",
      "step: 5250.0, loss: 931.632472038269\n",
      "step: 5251.0, loss: 926.8669862747192\n",
      "step: 5252.0, loss: 927.4257574081421\n",
      "step: 5253.0, loss: 924.8555374145508\n",
      "step: 5254.0, loss: 928.3254642486572\n",
      "step: 5255.0, loss: 928.2244882583618\n",
      "step: 5256.0, loss: 927.7565317153931\n",
      "step: 5257.0, loss: 927.4199028015137\n",
      "step: 5258.0, loss: 927.1826210021973\n",
      "step: 5259.0, loss: 927.5138282775879\n",
      "step: 5260.0, loss: 926.7519302368164\n",
      "step: 5261.0, loss: 927.1589002609253\n",
      "step: 5262.0, loss: 926.2559328079224\n",
      "step: 5263.0, loss: 928.0781526565552\n",
      "step: 5264.0, loss: 926.5682668685913\n",
      "step: 5265.0, loss: 928.0847024917603\n",
      "step: 5266.0, loss: 927.7905759811401\n",
      "step: 5267.0, loss: 926.7206544876099\n",
      "step: 5268.0, loss: 927.1709203720093\n",
      "step: 5269.0, loss: 927.8259391784668\n",
      "step: 5270.0, loss: 926.6107816696167\n",
      "step: 5271.0, loss: 928.2169895172119\n",
      "step: 5272.0, loss: 926.526104927063\n",
      "step: 5273.0, loss: 927.6049957275391\n",
      "step: 5274.0, loss: 927.9202079772949\n",
      "step: 5275.0, loss: 927.7010765075684\n",
      "step: 5276.0, loss: 927.8587017059326\n",
      "step: 5277.0, loss: 927.4718418121338\n",
      "step: 5278.0, loss: 927.4211616516113\n",
      "step: 5279.0, loss: 927.2427711486816\n",
      "step: 5280.0, loss: 927.3078279495239\n",
      "step: 5281.0, loss: 929.9644498825073\n",
      "step: 5282.0, loss: 928.6749839782715\n",
      "step: 5283.0, loss: 926.1973762512207\n",
      "step: 5284.0, loss: 928.3778085708618\n",
      "step: 5285.0, loss: 927.5142765045166\n",
      "step: 5286.0, loss: 927.4902420043945\n",
      "step: 5287.0, loss: 927.887583732605\n",
      "step: 5288.0, loss: 926.4746208190918\n",
      "step: 5289.0, loss: 926.4477434158325\n",
      "step: 5290.0, loss: 926.5362005233765\n",
      "step: 5291.0, loss: 928.5900955200195\n",
      "step: 5292.0, loss: 929.9707670211792\n",
      "step: 5293.0, loss: 927.6523742675781\n",
      "step: 5294.0, loss: 926.3865737915039\n",
      "step: 5295.0, loss: 928.1598787307739\n",
      "step: 5296.0, loss: 927.4316577911377\n",
      "step: 5297.0, loss: 927.4498119354248\n",
      "step: 5298.0, loss: 929.1346273422241\n",
      "step: 5299.0, loss: 927.4315700531006\n",
      "step: 5300.0, loss: 927.330623626709\n",
      "step: 5301.0, loss: 927.0040216445923\n",
      "step: 5302.0, loss: 928.8424434661865\n",
      "step: 5303.0, loss: 928.7000226974487\n",
      "step: 5304.0, loss: 926.4161701202393\n",
      "step: 5305.0, loss: 925.0534038543701\n",
      "step: 5306.0, loss: 926.8623762130737\n",
      "step: 5307.0, loss: 926.5642757415771\n",
      "step: 5308.0, loss: 929.2681484222412\n",
      "step: 5309.0, loss: 927.5625104904175\n",
      "step: 5310.0, loss: 926.5086832046509\n",
      "step: 5311.0, loss: 927.8127994537354\n",
      "step: 5312.0, loss: 926.3096170425415\n",
      "step: 5313.0, loss: 927.3065338134766\n",
      "step: 5314.0, loss: 926.787899017334\n",
      "step: 5315.0, loss: 926.957034111023\n",
      "step: 5316.0, loss: 926.246753692627\n",
      "step: 5317.0, loss: 928.0433626174927\n",
      "step: 5318.0, loss: 927.2259206771851\n",
      "step: 5319.0, loss: 926.6099147796631\n",
      "step: 5320.0, loss: 927.1886119842529\n",
      "step: 5321.0, loss: 926.1137542724609\n",
      "step: 5322.0, loss: 925.9879245758057\n",
      "step: 5323.0, loss: 926.9488754272461\n",
      "step: 5324.0, loss: 925.7416591644287\n",
      "step: 5325.0, loss: 926.6121406555176\n",
      "step: 5326.0, loss: 926.8123941421509\n",
      "step: 5327.0, loss: 927.0567932128906\n",
      "step: 5328.0, loss: 926.4912014007568\n",
      "step: 5329.0, loss: 927.3186731338501\n",
      "step: 5330.0, loss: 926.7040939331055\n",
      "step: 5331.0, loss: 927.8127861022949\n",
      "step: 5332.0, loss: 926.879940032959\n",
      "step: 5333.0, loss: 927.258204460144\n",
      "step: 5334.0, loss: 928.8962469100952\n",
      "step: 5335.0, loss: 926.7307729721069\n",
      "step: 5336.0, loss: 925.6667804718018\n",
      "step: 5337.0, loss: 927.972653388977\n",
      "step: 5338.0, loss: 926.6663761138916\n",
      "step: 5339.0, loss: 926.7761344909668\n",
      "step: 5340.0, loss: 926.2625436782837\n",
      "step: 5341.0, loss: 926.4009733200073\n",
      "step: 5342.0, loss: 926.2891597747803\n",
      "step: 5343.0, loss: 925.1962814331055\n",
      "step: 5344.0, loss: 929.3950824737549\n",
      "step: 5345.0, loss: 925.1077585220337\n",
      "step: 5346.0, loss: 926.275876045227\n",
      "step: 5347.0, loss: 926.2762956619263\n",
      "step: 5348.0, loss: 928.224946975708\n",
      "step: 5349.0, loss: 928.7459325790405\n",
      "step: 5350.0, loss: 925.4460687637329\n",
      "step: 5351.0, loss: 927.5759658813477\n",
      "step: 5352.0, loss: 927.8992538452148\n",
      "step: 5353.0, loss: 927.0110139846802\n",
      "step: 5354.0, loss: 926.8648080825806\n",
      "step: 5355.0, loss: 927.1291275024414\n",
      "step: 5356.0, loss: 927.8358383178711\n",
      "step: 5357.0, loss: 927.5196599960327\n",
      "step: 5358.0, loss: 927.2718191146851\n",
      "step: 5359.0, loss: 927.2472085952759\n",
      "step: 5360.0, loss: 926.4981346130371\n",
      "step: 5361.0, loss: 926.6696739196777\n",
      "step: 5362.0, loss: 926.8945016860962\n",
      "step: 5363.0, loss: 928.20920753479\n",
      "step: 5364.0, loss: 923.9334011077881\n",
      "step: 5365.0, loss: 925.8077383041382\n",
      "step: 5366.0, loss: 929.2663478851318\n",
      "step: 5367.0, loss: 927.3092966079712\n",
      "step: 5368.0, loss: 926.5725317001343\n",
      "step: 5369.0, loss: 927.1369037628174\n",
      "step: 5370.0, loss: 926.0018949508667\n",
      "step: 5371.0, loss: 928.9262638092041\n",
      "step: 5372.0, loss: 926.1346654891968\n",
      "step: 5373.0, loss: 928.221607208252\n",
      "step: 5374.0, loss: 924.4974231719971\n",
      "step: 5375.0, loss: 927.5846567153931\n",
      "step: 5376.0, loss: 927.2227935791016\n",
      "step: 5377.0, loss: 928.2051277160645\n",
      "step: 5378.0, loss: 924.6049718856812\n",
      "step: 5379.0, loss: 926.9504261016846\n",
      "step: 5380.0, loss: 927.8012018203735\n",
      "step: 5381.0, loss: 928.5621938705444\n",
      "step: 5382.0, loss: 925.539758682251\n",
      "step: 5383.0, loss: 926.4296531677246\n",
      "step: 5384.0, loss: 928.0121612548828\n",
      "step: 5385.0, loss: 927.5995721817017\n",
      "step: 5386.0, loss: 926.9259443283081\n",
      "step: 5387.0, loss: 926.7903490066528\n",
      "step: 5388.0, loss: 927.3991556167603\n",
      "step: 5389.0, loss: 927.8925113677979\n",
      "step: 5390.0, loss: 927.472601890564\n",
      "step: 5391.0, loss: 929.0186719894409\n",
      "step: 5392.0, loss: 930.5781879425049\n",
      "step: 5393.0, loss: 927.3066558837891\n",
      "step: 5394.0, loss: 929.6562366485596\n",
      "step: 5395.0, loss: 926.1544637680054\n",
      "step: 5396.0, loss: 927.5368089675903\n",
      "step: 5397.0, loss: 929.177586555481\n",
      "step: 5398.0, loss: 926.1761484146118\n",
      "step: 5399.0, loss: 926.2522592544556\n",
      "step: 5400.0, loss: 926.8274021148682\n",
      "step: 5401.0, loss: 926.4376335144043\n",
      "step: 5402.0, loss: 926.8189449310303\n",
      "step: 5403.0, loss: 927.2461824417114\n",
      "step: 5404.0, loss: 925.2409830093384\n",
      "step: 5405.0, loss: 929.5090465545654\n",
      "step: 5406.0, loss: 928.3721446990967\n",
      "step: 5407.0, loss: 924.3825597763062\n",
      "step: 5408.0, loss: 925.6561899185181\n",
      "step: 5409.0, loss: 927.4141931533813\n",
      "step: 5410.0, loss: 927.2852096557617\n",
      "step: 5411.0, loss: 925.7441987991333\n",
      "step: 5412.0, loss: 925.9576072692871\n",
      "step: 5413.0, loss: 926.3272857666016\n",
      "step: 5414.0, loss: 926.8584032058716\n",
      "step: 5415.0, loss: 926.9698839187622\n",
      "step: 5416.0, loss: 928.4775657653809\n",
      "step: 5417.0, loss: 928.3183689117432\n",
      "step: 5418.0, loss: 927.4653024673462\n",
      "step: 5419.0, loss: 927.4751214981079\n",
      "step: 5420.0, loss: 926.5666065216064\n",
      "step: 5421.0, loss: 925.7218723297119\n",
      "step: 5422.0, loss: 927.6026496887207\n",
      "step: 5423.0, loss: 926.5074052810669\n",
      "step: 5424.0, loss: 925.8400936126709\n",
      "step: 5425.0, loss: 926.6068677902222\n",
      "step: 5426.0, loss: 925.8015851974487\n",
      "step: 5427.0, loss: 927.6902132034302\n",
      "step: 5428.0, loss: 927.0063209533691\n",
      "step: 5429.0, loss: 926.5932559967041\n",
      "step: 5430.0, loss: 926.2305335998535\n",
      "step: 5431.0, loss: 924.7736577987671\n",
      "step: 5432.0, loss: 928.4134559631348\n",
      "step: 5433.0, loss: 924.6020317077637\n",
      "step: 5434.0, loss: 926.4646186828613\n",
      "step: 5435.0, loss: 925.8237524032593\n",
      "step: 5436.0, loss: 926.2458171844482\n",
      "step: 5437.0, loss: 927.092173576355\n",
      "step: 5438.0, loss: 926.2049169540405\n",
      "step: 5439.0, loss: 927.686110496521\n",
      "step: 5440.0, loss: 926.1021022796631\n",
      "step: 5441.0, loss: 927.4223403930664\n",
      "step: 5442.0, loss: 926.3270225524902\n",
      "step: 5443.0, loss: 925.2742176055908\n",
      "step: 5444.0, loss: 928.3925971984863\n",
      "step: 5445.0, loss: 927.0993061065674\n",
      "step: 5446.0, loss: 925.4593439102173\n",
      "step: 5447.0, loss: 930.4713106155396\n",
      "step: 5448.0, loss: 926.2415390014648\n",
      "step: 5449.0, loss: 928.513858795166\n",
      "step: 5450.0, loss: 927.4944400787354\n",
      "step: 5451.0, loss: 927.1280469894409\n",
      "step: 5452.0, loss: 928.4350433349609\n",
      "step: 5453.0, loss: 924.883584022522\n",
      "step: 5454.0, loss: 925.5931673049927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 5455.0, loss: 925.6319341659546\n",
      "step: 5456.0, loss: 925.7371416091919\n",
      "step: 5457.0, loss: 927.4452648162842\n",
      "step: 5458.0, loss: 928.037504196167\n",
      "step: 5459.0, loss: 926.0127534866333\n",
      "step: 5460.0, loss: 927.025484085083\n",
      "step: 5461.0, loss: 928.0972528457642\n",
      "step: 5462.0, loss: 929.5643043518066\n",
      "step: 5463.0, loss: 927.1318464279175\n",
      "step: 5464.0, loss: 926.446364402771\n",
      "step: 5465.0, loss: 926.5553541183472\n",
      "step: 5466.0, loss: 925.9247121810913\n",
      "step: 5467.0, loss: 928.9019622802734\n",
      "step: 5468.0, loss: 929.6385889053345\n",
      "step: 5469.0, loss: 925.5175180435181\n",
      "step: 5470.0, loss: 927.9467439651489\n",
      "step: 5471.0, loss: 927.0958204269409\n",
      "step: 5472.0, loss: 926.4108448028564\n",
      "step: 5473.0, loss: 927.5177478790283\n",
      "step: 5474.0, loss: 925.3011121749878\n",
      "step: 5475.0, loss: 927.7882738113403\n",
      "step: 5476.0, loss: 927.5402841567993\n",
      "step: 5477.0, loss: 928.2749261856079\n",
      "step: 5478.0, loss: 927.5007028579712\n",
      "step: 5479.0, loss: 927.7198534011841\n",
      "step: 5480.0, loss: 926.5547952651978\n",
      "step: 5481.0, loss: 928.1473579406738\n",
      "step: 5482.0, loss: 926.4312582015991\n",
      "step: 5483.0, loss: 928.1996459960938\n",
      "step: 5484.0, loss: 926.5697803497314\n",
      "step: 5485.0, loss: 926.5618810653687\n",
      "step: 5486.0, loss: 925.0978384017944\n",
      "step: 5487.0, loss: 925.8616857528687\n",
      "step: 5488.0, loss: 928.0963230133057\n",
      "step: 5489.0, loss: 925.8093385696411\n",
      "step: 5490.0, loss: 926.3161430358887\n",
      "step: 5491.0, loss: 925.7986211776733\n",
      "step: 5492.0, loss: 926.0267343521118\n",
      "step: 5493.0, loss: 928.2801885604858\n",
      "step: 5494.0, loss: 925.8281307220459\n",
      "step: 5495.0, loss: 926.8203401565552\n",
      "step: 5496.0, loss: 927.3994207382202\n",
      "step: 5497.0, loss: 927.2347192764282\n",
      "step: 5498.0, loss: 926.4746322631836\n",
      "step: 5499.0, loss: 925.4543085098267\n",
      "step: 5500.0, loss: 926.1430864334106\n",
      "step: 5501.0, loss: 926.72878074646\n",
      "step: 5502.0, loss: 925.9720573425293\n",
      "step: 5503.0, loss: 925.4290571212769\n",
      "step: 5504.0, loss: 928.1415863037109\n",
      "step: 5505.0, loss: 926.7112245559692\n",
      "step: 5506.0, loss: 926.430718421936\n",
      "step: 5507.0, loss: 928.6483469009399\n",
      "step: 5508.0, loss: 925.0164737701416\n",
      "step: 5509.0, loss: 928.1896066665649\n",
      "step: 5510.0, loss: 928.414701461792\n",
      "step: 5511.0, loss: 927.3023738861084\n",
      "step: 5512.0, loss: 927.1782140731812\n",
      "step: 5513.0, loss: 929.3540601730347\n",
      "step: 5514.0, loss: 928.5972881317139\n",
      "step: 5515.0, loss: 926.2867002487183\n",
      "step: 5516.0, loss: 927.694375038147\n",
      "step: 5517.0, loss: 927.3276109695435\n",
      "step: 5518.0, loss: 928.1616764068604\n",
      "step: 5519.0, loss: 928.1048765182495\n",
      "step: 5520.0, loss: 925.1371202468872\n",
      "step: 5521.0, loss: 926.8733081817627\n",
      "step: 5522.0, loss: 926.6626091003418\n",
      "step: 5523.0, loss: 926.3206052780151\n",
      "step: 5524.0, loss: 926.9318437576294\n",
      "step: 5525.0, loss: 927.9149188995361\n",
      "step: 5526.0, loss: 927.6469926834106\n",
      "step: 5527.0, loss: 926.4386262893677\n",
      "step: 5528.0, loss: 926.6059141159058\n",
      "step: 5529.0, loss: 926.3731002807617\n",
      "step: 5530.0, loss: 926.0643739700317\n",
      "step: 5531.0, loss: 926.9127788543701\n",
      "step: 5532.0, loss: 927.8182983398438\n",
      "step: 5533.0, loss: 926.0544919967651\n",
      "step: 5534.0, loss: 926.7696104049683\n",
      "step: 5535.0, loss: 925.3122329711914\n",
      "step: 5536.0, loss: 926.8633556365967\n",
      "step: 5537.0, loss: 928.6147165298462\n",
      "step: 5538.0, loss: 925.996111869812\n",
      "step: 5539.0, loss: 927.2126216888428\n",
      "step: 5540.0, loss: 926.5842208862305\n",
      "step: 5541.0, loss: 928.1057815551758\n",
      "step: 5542.0, loss: 926.6229906082153\n",
      "step: 5543.0, loss: 927.474856376648\n",
      "step: 5544.0, loss: 927.3472242355347\n",
      "step: 5545.0, loss: 926.9740200042725\n",
      "step: 5546.0, loss: 926.2605667114258\n",
      "step: 5547.0, loss: 928.1751470565796\n",
      "step: 5548.0, loss: 926.3337917327881\n",
      "step: 5549.0, loss: 925.6913528442383\n",
      "step: 5550.0, loss: 928.9300136566162\n",
      "step: 5551.0, loss: 925.9764471054077\n",
      "step: 5552.0, loss: 928.5430889129639\n",
      "step: 5553.0, loss: 926.6252450942993\n",
      "step: 5554.0, loss: 928.3195123672485\n",
      "step: 5555.0, loss: 926.3721675872803\n",
      "step: 5556.0, loss: 928.8266506195068\n",
      "step: 5557.0, loss: 927.2495203018188\n",
      "step: 5558.0, loss: 926.6863040924072\n",
      "step: 5559.0, loss: 927.3890190124512\n",
      "step: 5560.0, loss: 926.0195255279541\n",
      "step: 5561.0, loss: 926.4587211608887\n",
      "step: 5562.0, loss: 926.9115190505981\n",
      "step: 5563.0, loss: 927.5140991210938\n",
      "step: 5564.0, loss: 926.9542875289917\n",
      "step: 5565.0, loss: 926.7785739898682\n",
      "step: 5566.0, loss: 926.9937124252319\n",
      "step: 5567.0, loss: 927.1155605316162\n",
      "step: 5568.0, loss: 928.520206451416\n",
      "step: 5569.0, loss: 928.2048435211182\n",
      "step: 5570.0, loss: 927.6697130203247\n",
      "step: 5571.0, loss: 927.0204248428345\n",
      "step: 5572.0, loss: 929.510124206543\n",
      "step: 5573.0, loss: 926.8269147872925\n",
      "step: 5574.0, loss: 925.8606758117676\n",
      "step: 5575.0, loss: 927.6191501617432\n",
      "step: 5576.0, loss: 928.2680206298828\n",
      "step: 5577.0, loss: 926.3444347381592\n",
      "step: 5578.0, loss: 926.8581094741821\n",
      "step: 5579.0, loss: 925.9886789321899\n",
      "step: 5580.0, loss: 926.4111747741699\n",
      "step: 5581.0, loss: 927.240870475769\n",
      "step: 5582.0, loss: 927.3909044265747\n",
      "step: 5583.0, loss: 925.9796705245972\n",
      "step: 5584.0, loss: 927.6079044342041\n",
      "step: 5585.0, loss: 927.9400625228882\n",
      "step: 5586.0, loss: 927.1209449768066\n",
      "step: 5587.0, loss: 925.5994501113892\n",
      "step: 5588.0, loss: 926.9542655944824\n",
      "step: 5589.0, loss: 927.4851837158203\n",
      "step: 5590.0, loss: 929.7650384902954\n",
      "step: 5591.0, loss: 925.8118009567261\n",
      "step: 5592.0, loss: 927.484525680542\n",
      "step: 5593.0, loss: 928.2897329330444\n",
      "step: 5594.0, loss: 927.6486930847168\n",
      "step: 5595.0, loss: 927.1685428619385\n",
      "step: 5596.0, loss: 926.4812803268433\n",
      "step: 5597.0, loss: 927.4770402908325\n",
      "step: 5598.0, loss: 927.8085613250732\n",
      "step: 5599.0, loss: 926.2273931503296\n",
      "step: 5600.0, loss: 926.1441802978516\n",
      "step: 5601.0, loss: 926.9205751419067\n",
      "step: 5602.0, loss: 927.0037469863892\n",
      "step: 5603.0, loss: 925.6217670440674\n",
      "step: 5604.0, loss: 925.6516523361206\n",
      "step: 5605.0, loss: 926.9336214065552\n",
      "step: 5606.0, loss: 925.963960647583\n",
      "step: 5607.0, loss: 926.6583423614502\n",
      "step: 5608.0, loss: 926.2105445861816\n",
      "step: 5609.0, loss: 926.9263439178467\n",
      "step: 5610.0, loss: 924.3285026550293\n",
      "step: 5611.0, loss: 927.2834396362305\n",
      "step: 5612.0, loss: 927.517858505249\n",
      "step: 5613.0, loss: 927.0820693969727\n",
      "step: 5614.0, loss: 925.8618717193604\n",
      "step: 5615.0, loss: 927.9354333877563\n",
      "step: 5616.0, loss: 924.4084310531616\n",
      "step: 5617.0, loss: 925.5369415283203\n",
      "step: 5618.0, loss: 925.944712638855\n",
      "step: 5619.0, loss: 925.3090763092041\n",
      "step: 5620.0, loss: 926.3052997589111\n",
      "step: 5621.0, loss: 926.4167203903198\n",
      "step: 5622.0, loss: 926.9951286315918\n",
      "step: 5623.0, loss: 926.1159467697144\n",
      "step: 5624.0, loss: 927.1536312103271\n",
      "step: 5625.0, loss: 924.3665418624878\n",
      "step: 5626.0, loss: 927.7685508728027\n",
      "step: 5627.0, loss: 926.494631767273\n",
      "step: 5628.0, loss: 925.6184396743774\n",
      "step: 5629.0, loss: 926.5578212738037\n",
      "step: 5630.0, loss: 928.5422039031982\n",
      "step: 5631.0, loss: 925.4077863693237\n",
      "step: 5632.0, loss: 927.0374803543091\n",
      "step: 5633.0, loss: 924.3122758865356\n",
      "step: 5634.0, loss: 926.3136825561523\n",
      "step: 5635.0, loss: 925.6733026504517\n",
      "step: 5636.0, loss: 924.5868434906006\n",
      "step: 5637.0, loss: 928.0315990447998\n",
      "step: 5638.0, loss: 924.8938512802124\n",
      "step: 5639.0, loss: 925.0870132446289\n",
      "step: 5640.0, loss: 925.3293209075928\n",
      "step: 5641.0, loss: 926.7140645980835\n",
      "step: 5642.0, loss: 926.5359392166138\n",
      "step: 5643.0, loss: 928.9610071182251\n",
      "step: 5644.0, loss: 926.6787719726562\n",
      "step: 5645.0, loss: 928.4713172912598\n",
      "step: 5646.0, loss: 925.2393922805786\n",
      "step: 5647.0, loss: 927.5138292312622\n",
      "step: 5648.0, loss: 926.235068321228\n",
      "step: 5649.0, loss: 925.362006187439\n",
      "step: 5650.0, loss: 926.5703830718994\n",
      "step: 5651.0, loss: 926.628978729248\n",
      "step: 5652.0, loss: 927.4844150543213\n",
      "step: 5653.0, loss: 928.4994897842407\n",
      "step: 5654.0, loss: 925.8989915847778\n",
      "step: 5655.0, loss: 926.9502515792847\n",
      "step: 5656.0, loss: 926.0572090148926\n",
      "step: 5657.0, loss: 925.0276517868042\n",
      "step: 5658.0, loss: 928.1694116592407\n",
      "step: 5659.0, loss: 927.0303506851196\n",
      "step: 5660.0, loss: 926.8324937820435\n",
      "step: 5661.0, loss: 925.3548240661621\n",
      "step: 5662.0, loss: 927.1382656097412\n",
      "step: 5663.0, loss: 926.6577548980713\n",
      "step: 5664.0, loss: 926.2620086669922\n",
      "step: 5665.0, loss: 925.7131786346436\n",
      "step: 5666.0, loss: 927.2328224182129\n",
      "step: 5667.0, loss: 926.7195672988892\n",
      "step: 5668.0, loss: 925.3566045761108\n",
      "step: 5669.0, loss: 925.2154235839844\n",
      "step: 5670.0, loss: 927.804648399353\n",
      "step: 5671.0, loss: 925.3143901824951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 5672.0, loss: 926.3144006729126\n",
      "step: 5673.0, loss: 926.5551023483276\n",
      "step: 5674.0, loss: 926.122465133667\n",
      "step: 5675.0, loss: 925.4101886749268\n",
      "step: 5676.0, loss: 923.734169960022\n",
      "step: 5677.0, loss: 926.5509576797485\n",
      "step: 5678.0, loss: 926.0094652175903\n",
      "step: 5679.0, loss: 925.3033857345581\n",
      "step: 5680.0, loss: 925.2205200195312\n",
      "step: 5681.0, loss: 926.3942918777466\n",
      "step: 5682.0, loss: 926.1440076828003\n",
      "step: 5683.0, loss: 926.5036611557007\n",
      "step: 5684.0, loss: 926.4962882995605\n",
      "step: 5685.0, loss: 926.9912548065186\n",
      "step: 5686.0, loss: 926.067831993103\n",
      "step: 5687.0, loss: 926.3124876022339\n",
      "step: 5688.0, loss: 927.0545749664307\n",
      "step: 5689.0, loss: 925.0815677642822\n",
      "step: 5690.0, loss: 926.6814632415771\n",
      "step: 5691.0, loss: 928.5292539596558\n",
      "step: 5692.0, loss: 925.5945262908936\n",
      "step: 5693.0, loss: 926.5992879867554\n",
      "step: 5694.0, loss: 927.8399238586426\n",
      "step: 5695.0, loss: 927.5632476806641\n",
      "step: 5696.0, loss: 928.0248193740845\n",
      "step: 5697.0, loss: 927.7856550216675\n",
      "step: 5698.0, loss: 925.9898195266724\n",
      "step: 5699.0, loss: 928.1916704177856\n",
      "step: 5700.0, loss: 926.483736038208\n",
      "step: 5701.0, loss: 927.5513210296631\n",
      "step: 5702.0, loss: 926.5341033935547\n",
      "step: 5703.0, loss: 927.6132774353027\n",
      "step: 5704.0, loss: 926.3337783813477\n",
      "step: 5705.0, loss: 926.3603401184082\n",
      "step: 5706.0, loss: 927.2446193695068\n",
      "step: 5707.0, loss: 926.0663404464722\n",
      "step: 5708.0, loss: 926.9542388916016\n",
      "step: 5709.0, loss: 926.4349956512451\n",
      "step: 5710.0, loss: 925.5304660797119\n",
      "step: 5711.0, loss: 926.4962158203125\n",
      "step: 5712.0, loss: 927.3232402801514\n",
      "step: 5713.0, loss: 926.5514030456543\n",
      "step: 5714.0, loss: 926.6823215484619\n",
      "step: 5715.0, loss: 927.0591201782227\n",
      "step: 5716.0, loss: 925.1160707473755\n",
      "step: 5717.0, loss: 925.1787166595459\n",
      "step: 5718.0, loss: 927.6665639877319\n",
      "step: 5719.0, loss: 926.8644599914551\n",
      "step: 5720.0, loss: 925.5529499053955\n",
      "step: 5721.0, loss: 926.7758073806763\n",
      "step: 5722.0, loss: 927.1389713287354\n",
      "step: 5723.0, loss: 926.3401870727539\n",
      "step: 5724.0, loss: 927.1826753616333\n",
      "step: 5725.0, loss: 926.4315538406372\n",
      "step: 5726.0, loss: 926.4600067138672\n",
      "step: 5727.0, loss: 924.7975778579712\n",
      "step: 5728.0, loss: 926.792763710022\n",
      "step: 5729.0, loss: 928.1023368835449\n",
      "step: 5730.0, loss: 928.3532371520996\n",
      "step: 5731.0, loss: 926.2190208435059\n",
      "step: 5732.0, loss: 927.133412361145\n",
      "step: 5733.0, loss: 926.4607782363892\n",
      "step: 5734.0, loss: 927.4754791259766\n",
      "step: 5735.0, loss: 926.6115427017212\n",
      "step: 5736.0, loss: 926.8535852432251\n",
      "step: 5737.0, loss: 928.0327644348145\n",
      "step: 5738.0, loss: 927.1893796920776\n",
      "step: 5739.0, loss: 925.744179725647\n",
      "step: 5740.0, loss: 926.7285842895508\n",
      "step: 5741.0, loss: 926.1750984191895\n",
      "step: 5742.0, loss: 926.8372430801392\n",
      "step: 5743.0, loss: 926.0366201400757\n",
      "step: 5744.0, loss: 926.0050201416016\n",
      "step: 5745.0, loss: 927.8280248641968\n",
      "step: 5746.0, loss: 927.6095771789551\n",
      "step: 5747.0, loss: 927.2837572097778\n",
      "step: 5748.0, loss: 925.9283819198608\n",
      "step: 5749.0, loss: 926.329665184021\n",
      "step: 5750.0, loss: 927.4245729446411\n",
      "step: 5751.0, loss: 926.005277633667\n",
      "step: 5752.0, loss: 926.202223777771\n",
      "step: 5753.0, loss: 926.9517517089844\n",
      "step: 5754.0, loss: 926.105278968811\n",
      "step: 5755.0, loss: 924.7860240936279\n",
      "step: 5756.0, loss: 927.3279085159302\n",
      "step: 5757.0, loss: 926.6220951080322\n",
      "step: 5758.0, loss: 926.1928148269653\n",
      "step: 5759.0, loss: 926.3197469711304\n",
      "step: 5760.0, loss: 927.3022794723511\n",
      "step: 5761.0, loss: 926.7943162918091\n",
      "step: 5762.0, loss: 927.3505773544312\n",
      "step: 5763.0, loss: 924.8731737136841\n",
      "step: 5764.0, loss: 927.412031173706\n",
      "step: 5765.0, loss: 927.4024953842163\n",
      "step: 5766.0, loss: 926.0684022903442\n",
      "step: 5767.0, loss: 925.6845283508301\n",
      "step: 5768.0, loss: 925.6281051635742\n",
      "step: 5769.0, loss: 927.609866142273\n",
      "step: 5770.0, loss: 927.9323806762695\n",
      "step: 5771.0, loss: 925.2910404205322\n",
      "step: 5772.0, loss: 927.2685461044312\n",
      "step: 5773.0, loss: 926.5080375671387\n",
      "step: 5774.0, loss: 927.8581037521362\n",
      "step: 5775.0, loss: 926.2042818069458\n",
      "step: 5776.0, loss: 925.3379564285278\n",
      "step: 5777.0, loss: 925.0044956207275\n",
      "step: 5778.0, loss: 926.0867156982422\n",
      "step: 5779.0, loss: 927.4308223724365\n",
      "step: 5780.0, loss: 926.0344352722168\n",
      "step: 5781.0, loss: 926.418041229248\n",
      "step: 5782.0, loss: 927.5141563415527\n",
      "step: 5783.0, loss: 927.3903741836548\n",
      "step: 5784.0, loss: 927.405650138855\n",
      "step: 5785.0, loss: 927.280312538147\n",
      "step: 5786.0, loss: 927.6241054534912\n",
      "step: 5787.0, loss: 926.0376682281494\n",
      "step: 5788.0, loss: 926.9440326690674\n",
      "step: 5789.0, loss: 927.5974102020264\n",
      "step: 5790.0, loss: 926.5565395355225\n",
      "step: 5791.0, loss: 924.9924211502075\n",
      "step: 5792.0, loss: 925.5529565811157\n",
      "step: 5793.0, loss: 927.3073472976685\n",
      "step: 5794.0, loss: 926.6158857345581\n",
      "step: 5795.0, loss: 924.724739074707\n",
      "step: 5796.0, loss: 925.7126264572144\n",
      "step: 5797.0, loss: 927.806450843811\n",
      "step: 5798.0, loss: 925.2817249298096\n",
      "step: 5799.0, loss: 925.6627607345581\n",
      "step: 5800.0, loss: 928.3933429718018\n",
      "step: 5801.0, loss: 926.9424047470093\n",
      "step: 5802.0, loss: 925.6966247558594\n",
      "step: 5803.0, loss: 926.6252374649048\n",
      "step: 5804.0, loss: 925.5550746917725\n",
      "step: 5805.0, loss: 925.9220180511475\n",
      "step: 5806.0, loss: 927.535322189331\n",
      "step: 5807.0, loss: 927.578537940979\n",
      "step: 5808.0, loss: 925.8060445785522\n",
      "step: 5809.0, loss: 928.2167358398438\n",
      "step: 5810.0, loss: 925.6347455978394\n",
      "step: 5811.0, loss: 925.7941560745239\n",
      "step: 5812.0, loss: 926.3492126464844\n",
      "step: 5813.0, loss: 925.1136064529419\n",
      "step: 5814.0, loss: 925.7311611175537\n",
      "step: 5815.0, loss: 926.8732299804688\n",
      "step: 5816.0, loss: 926.9853687286377\n",
      "step: 5817.0, loss: 927.4934396743774\n",
      "step: 5818.0, loss: 925.9504232406616\n",
      "step: 5819.0, loss: 926.4252910614014\n",
      "step: 5820.0, loss: 925.1987428665161\n",
      "step: 5821.0, loss: 925.7842168807983\n",
      "step: 5822.0, loss: 927.0375490188599\n",
      "step: 5823.0, loss: 925.8343830108643\n",
      "step: 5824.0, loss: 926.5467376708984\n",
      "step: 5825.0, loss: 925.4720458984375\n",
      "step: 5826.0, loss: 927.0135812759399\n",
      "step: 5827.0, loss: 925.5047817230225\n",
      "step: 5828.0, loss: 927.033242225647\n",
      "step: 5829.0, loss: 926.1058378219604\n",
      "step: 5830.0, loss: 924.5092754364014\n",
      "step: 5831.0, loss: 927.3346538543701\n",
      "step: 5832.0, loss: 926.8471260070801\n",
      "step: 5833.0, loss: 926.8956117630005\n",
      "step: 5834.0, loss: 926.4849119186401\n",
      "step: 5835.0, loss: 926.3232517242432\n",
      "step: 5836.0, loss: 927.1894369125366\n",
      "step: 5837.0, loss: 926.2508430480957\n",
      "step: 5838.0, loss: 926.5929584503174\n",
      "step: 5839.0, loss: 925.506142616272\n",
      "step: 5840.0, loss: 927.5712184906006\n",
      "step: 5841.0, loss: 927.3168296813965\n",
      "step: 5842.0, loss: 925.8538246154785\n",
      "step: 5843.0, loss: 927.4936475753784\n",
      "step: 5844.0, loss: 926.1479663848877\n",
      "step: 5845.0, loss: 926.0488433837891\n",
      "step: 5846.0, loss: 928.6260929107666\n",
      "step: 5847.0, loss: 926.7557573318481\n",
      "step: 5848.0, loss: 926.525951385498\n",
      "step: 5849.0, loss: 924.5170555114746\n",
      "step: 5850.0, loss: 926.979585647583\n",
      "step: 5851.0, loss: 926.1901559829712\n",
      "step: 5852.0, loss: 927.2588663101196\n",
      "step: 5853.0, loss: 924.4938068389893\n",
      "step: 5854.0, loss: 925.3962831497192\n",
      "step: 5855.0, loss: 924.105432510376\n",
      "step: 5856.0, loss: 927.5226640701294\n",
      "step: 5857.0, loss: 925.1119432449341\n",
      "step: 5858.0, loss: 926.0185995101929\n",
      "step: 5859.0, loss: 926.203761100769\n",
      "step: 5860.0, loss: 927.4731664657593\n",
      "step: 5861.0, loss: 925.6860504150391\n",
      "step: 5862.0, loss: 927.80029296875\n",
      "step: 5863.0, loss: 925.2662410736084\n",
      "step: 5864.0, loss: 926.0719108581543\n",
      "step: 5865.0, loss: 926.414379119873\n",
      "step: 5866.0, loss: 925.4299364089966\n",
      "step: 5867.0, loss: 925.5100355148315\n",
      "step: 5868.0, loss: 927.6694688796997\n",
      "step: 5869.0, loss: 925.0588827133179\n",
      "step: 5870.0, loss: 926.2311191558838\n",
      "step: 5871.0, loss: 926.9163227081299\n",
      "step: 5872.0, loss: 924.8824996948242\n",
      "step: 5873.0, loss: 925.9811201095581\n",
      "step: 5874.0, loss: 926.6575593948364\n",
      "step: 5875.0, loss: 925.9530506134033\n",
      "step: 5876.0, loss: 927.5708055496216\n",
      "step: 5877.0, loss: 925.0940189361572\n",
      "step: 5878.0, loss: 926.2341899871826\n",
      "step: 5879.0, loss: 923.5977392196655\n",
      "step: 5880.0, loss: 926.0866632461548\n",
      "step: 5881.0, loss: 927.5324220657349\n",
      "step: 5882.0, loss: 926.4880647659302\n",
      "step: 5883.0, loss: 926.1928787231445\n",
      "step: 5884.0, loss: 927.3870258331299\n",
      "step: 5885.0, loss: 926.7848463058472\n",
      "step: 5886.0, loss: 927.0748662948608\n",
      "step: 5887.0, loss: 927.8822154998779\n",
      "step: 5888.0, loss: 926.9496555328369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 5889.0, loss: 925.9449119567871\n",
      "step: 5890.0, loss: 925.8985433578491\n",
      "step: 5891.0, loss: 925.9684247970581\n",
      "step: 5892.0, loss: 926.9565267562866\n",
      "step: 5893.0, loss: 926.6530513763428\n",
      "step: 5894.0, loss: 925.8254880905151\n",
      "step: 5895.0, loss: 927.8571996688843\n",
      "step: 5896.0, loss: 928.1449136734009\n",
      "step: 5897.0, loss: 926.7371501922607\n",
      "step: 5898.0, loss: 925.8038911819458\n",
      "step: 5899.0, loss: 926.5033235549927\n",
      "step: 5900.0, loss: 924.5322494506836\n",
      "step: 5901.0, loss: 925.3994159698486\n",
      "step: 5902.0, loss: 928.2102928161621\n",
      "step: 5903.0, loss: 927.1822481155396\n",
      "step: 5904.0, loss: 927.9933423995972\n",
      "step: 5905.0, loss: 926.4492111206055\n",
      "step: 5906.0, loss: 926.3422899246216\n",
      "step: 5907.0, loss: 928.3331184387207\n",
      "step: 5908.0, loss: 926.3933343887329\n",
      "step: 5909.0, loss: 925.3531999588013\n",
      "step: 5910.0, loss: 926.1817007064819\n",
      "step: 5911.0, loss: 926.1704530715942\n",
      "step: 5912.0, loss: 925.564190864563\n",
      "step: 5913.0, loss: 925.1602792739868\n",
      "step: 5914.0, loss: 924.8855648040771\n",
      "step: 5915.0, loss: 925.2588081359863\n",
      "step: 5916.0, loss: 927.1870803833008\n",
      "step: 5917.0, loss: 927.3956422805786\n",
      "step: 5918.0, loss: 927.4622039794922\n",
      "step: 5919.0, loss: 925.5641345977783\n",
      "step: 5920.0, loss: 925.7353229522705\n",
      "step: 5921.0, loss: 924.8668851852417\n",
      "step: 5922.0, loss: 927.008490562439\n",
      "step: 5923.0, loss: 925.3031311035156\n",
      "step: 5924.0, loss: 927.106406211853\n",
      "step: 5925.0, loss: 924.8009328842163\n",
      "step: 5926.0, loss: 925.3761129379272\n",
      "step: 5927.0, loss: 925.7657670974731\n",
      "step: 5928.0, loss: 926.3682403564453\n",
      "step: 5929.0, loss: 926.9303779602051\n",
      "step: 5930.0, loss: 926.0318937301636\n",
      "step: 5931.0, loss: 923.9182500839233\n",
      "step: 5932.0, loss: 924.6544198989868\n",
      "step: 5933.0, loss: 926.3853425979614\n",
      "step: 5934.0, loss: 927.0284633636475\n",
      "step: 5935.0, loss: 926.0018043518066\n",
      "step: 5936.0, loss: 925.9314470291138\n",
      "step: 5937.0, loss: 926.2839908599854\n",
      "step: 5938.0, loss: 926.769905090332\n",
      "step: 5939.0, loss: 927.5378875732422\n",
      "step: 5940.0, loss: 926.5581903457642\n",
      "step: 5941.0, loss: 927.0827217102051\n",
      "step: 5942.0, loss: 925.9679718017578\n",
      "step: 5943.0, loss: 927.1961212158203\n",
      "step: 5944.0, loss: 925.3118143081665\n",
      "step: 5945.0, loss: 924.9999828338623\n",
      "step: 5946.0, loss: 924.662335395813\n",
      "step: 5947.0, loss: 925.918553352356\n",
      "step: 5948.0, loss: 926.8614377975464\n",
      "step: 5949.0, loss: 926.4968795776367\n",
      "step: 5950.0, loss: 927.3046894073486\n",
      "step: 5951.0, loss: 926.9641647338867\n",
      "step: 5952.0, loss: 924.8690071105957\n",
      "step: 5953.0, loss: 926.1755018234253\n",
      "step: 5954.0, loss: 924.2392663955688\n",
      "step: 5955.0, loss: 924.943362236023\n",
      "step: 5956.0, loss: 925.2970771789551\n",
      "step: 5957.0, loss: 926.3216457366943\n",
      "step: 5958.0, loss: 927.4951152801514\n",
      "step: 5959.0, loss: 925.7094984054565\n",
      "step: 5960.0, loss: 925.3314981460571\n",
      "step: 5961.0, loss: 923.8787174224854\n",
      "step: 5962.0, loss: 927.3022308349609\n",
      "step: 5963.0, loss: 926.7797889709473\n",
      "step: 5964.0, loss: 925.3870325088501\n",
      "step: 5965.0, loss: 925.5211124420166\n",
      "step: 5966.0, loss: 926.9278011322021\n",
      "step: 5967.0, loss: 925.223331451416\n",
      "step: 5968.0, loss: 926.5735769271851\n",
      "step: 5969.0, loss: 925.1462669372559\n",
      "step: 5970.0, loss: 926.8925914764404\n",
      "step: 5971.0, loss: 929.2898750305176\n",
      "step: 5972.0, loss: 927.6108140945435\n",
      "step: 5973.0, loss: 927.4092855453491\n",
      "step: 5974.0, loss: 924.8273267745972\n",
      "step: 5975.0, loss: 926.0838031768799\n",
      "step: 5976.0, loss: 927.8985929489136\n",
      "step: 5977.0, loss: 926.3113822937012\n",
      "step: 5978.0, loss: 926.301342010498\n",
      "step: 5979.0, loss: 925.3971691131592\n",
      "step: 5980.0, loss: 927.5797166824341\n",
      "step: 5981.0, loss: 927.1674165725708\n",
      "step: 5982.0, loss: 924.9638986587524\n",
      "step: 5983.0, loss: 927.0022792816162\n",
      "step: 5984.0, loss: 927.2094440460205\n",
      "step: 5985.0, loss: 924.9081573486328\n",
      "step: 5986.0, loss: 925.8504657745361\n",
      "step: 5987.0, loss: 927.0782346725464\n",
      "step: 5988.0, loss: 926.7544479370117\n",
      "step: 5989.0, loss: 925.0501432418823\n",
      "step: 5990.0, loss: 925.3963718414307\n",
      "step: 5991.0, loss: 926.6825590133667\n",
      "step: 5992.0, loss: 927.0841054916382\n",
      "step: 5993.0, loss: 925.7341365814209\n",
      "step: 5994.0, loss: 925.6099853515625\n",
      "step: 5995.0, loss: 927.3647584915161\n",
      "step: 5996.0, loss: 926.4869241714478\n",
      "step: 5997.0, loss: 925.6981220245361\n",
      "step: 5998.0, loss: 926.0623645782471\n",
      "step: 5999.0, loss: 927.1819658279419\n",
      "step: 6000.0, loss: 927.5028057098389\n",
      "step: 6001.0, loss: 927.297908782959\n",
      "step: 6002.0, loss: 925.7318677902222\n",
      "step: 6003.0, loss: 926.8614263534546\n",
      "step: 6004.0, loss: 925.2169771194458\n",
      "step: 6005.0, loss: 925.4929866790771\n",
      "step: 6006.0, loss: 925.2309150695801\n",
      "step: 6007.0, loss: 924.3156671524048\n",
      "step: 6008.0, loss: 926.8536643981934\n",
      "step: 6009.0, loss: 925.8744564056396\n",
      "step: 6010.0, loss: 926.3683576583862\n",
      "step: 6011.0, loss: 927.6714582443237\n",
      "step: 6012.0, loss: 926.8485822677612\n",
      "step: 6013.0, loss: 926.1207227706909\n",
      "step: 6014.0, loss: 926.6140232086182\n",
      "step: 6015.0, loss: 926.7903165817261\n",
      "step: 6016.0, loss: 925.4294328689575\n",
      "step: 6017.0, loss: 923.0077409744263\n",
      "step: 6018.0, loss: 925.3654851913452\n",
      "step: 6019.0, loss: 926.3321046829224\n",
      "step: 6020.0, loss: 927.1917171478271\n",
      "step: 6021.0, loss: 925.8217210769653\n",
      "step: 6022.0, loss: 925.0600156784058\n",
      "step: 6023.0, loss: 925.6257448196411\n",
      "step: 6024.0, loss: 926.5470676422119\n",
      "step: 6025.0, loss: 926.9377574920654\n",
      "step: 6026.0, loss: 924.1125679016113\n",
      "step: 6027.0, loss: 925.5923824310303\n",
      "step: 6028.0, loss: 925.3677654266357\n",
      "step: 6029.0, loss: 926.21217918396\n",
      "step: 6030.0, loss: 925.5177173614502\n",
      "step: 6031.0, loss: 925.0013580322266\n",
      "step: 6032.0, loss: 926.9472236633301\n",
      "step: 6033.0, loss: 926.324107170105\n",
      "step: 6034.0, loss: 927.4034051895142\n",
      "step: 6035.0, loss: 926.9496250152588\n",
      "step: 6036.0, loss: 927.0948524475098\n",
      "step: 6037.0, loss: 927.0407428741455\n",
      "step: 6038.0, loss: 927.2240409851074\n",
      "step: 6039.0, loss: 924.3434524536133\n",
      "step: 6040.0, loss: 926.4745712280273\n",
      "step: 6041.0, loss: 927.109037399292\n",
      "step: 6042.0, loss: 926.8544549942017\n",
      "step: 6043.0, loss: 924.9074277877808\n",
      "step: 6044.0, loss: 926.9793357849121\n",
      "step: 6045.0, loss: 926.2091035842896\n",
      "step: 6046.0, loss: 923.8494939804077\n",
      "step: 6047.0, loss: 926.2826404571533\n",
      "step: 6048.0, loss: 924.326696395874\n",
      "step: 6049.0, loss: 923.2230453491211\n",
      "step: 6050.0, loss: 926.0346813201904\n",
      "step: 6051.0, loss: 925.4659700393677\n",
      "step: 6052.0, loss: 926.1197128295898\n",
      "step: 6053.0, loss: 925.4134540557861\n",
      "step: 6054.0, loss: 925.7411260604858\n",
      "step: 6055.0, loss: 927.0321836471558\n",
      "step: 6056.0, loss: 925.6289539337158\n",
      "step: 6057.0, loss: 925.4096517562866\n",
      "step: 6058.0, loss: 926.5814552307129\n",
      "step: 6059.0, loss: 926.5012273788452\n",
      "step: 6060.0, loss: 927.6091194152832\n",
      "step: 6061.0, loss: 925.4389390945435\n",
      "step: 6062.0, loss: 926.5874633789062\n",
      "step: 6063.0, loss: 927.6558294296265\n",
      "step: 6064.0, loss: 926.468168258667\n",
      "step: 6065.0, loss: 926.7650356292725\n",
      "step: 6066.0, loss: 924.0368642807007\n",
      "step: 6067.0, loss: 925.3335609436035\n",
      "step: 6068.0, loss: 926.8741579055786\n",
      "step: 6069.0, loss: 924.4264993667603\n",
      "step: 6070.0, loss: 924.7133913040161\n",
      "step: 6071.0, loss: 925.7417783737183\n",
      "step: 6072.0, loss: 926.3273611068726\n",
      "step: 6073.0, loss: 926.2717142105103\n",
      "step: 6074.0, loss: 926.2442817687988\n",
      "step: 6075.0, loss: 928.1434364318848\n",
      "step: 6076.0, loss: 926.0633783340454\n",
      "step: 6077.0, loss: 925.8560285568237\n",
      "step: 6078.0, loss: 925.2203359603882\n",
      "step: 6079.0, loss: 926.0833253860474\n",
      "step: 6080.0, loss: 926.1170711517334\n",
      "step: 6081.0, loss: 927.5530347824097\n",
      "step: 6082.0, loss: 927.2833433151245\n",
      "step: 6083.0, loss: 925.9247817993164\n",
      "step: 6084.0, loss: 927.1308641433716\n",
      "step: 6085.0, loss: 925.6659297943115\n",
      "step: 6086.0, loss: 925.8396615982056\n",
      "step: 6087.0, loss: 927.765908241272\n",
      "step: 6088.0, loss: 927.8848485946655\n",
      "step: 6089.0, loss: 926.7418689727783\n",
      "step: 6090.0, loss: 924.6092319488525\n",
      "step: 6091.0, loss: 927.0674057006836\n",
      "step: 6092.0, loss: 925.3358058929443\n",
      "step: 6093.0, loss: 923.7182445526123\n",
      "step: 6094.0, loss: 924.7284498214722\n",
      "step: 6095.0, loss: 925.8740892410278\n",
      "step: 6096.0, loss: 925.9018220901489\n",
      "step: 6097.0, loss: 926.4222078323364\n",
      "step: 6098.0, loss: 926.5318365097046\n",
      "step: 6099.0, loss: 925.4393939971924\n",
      "step: 6100.0, loss: 926.3946533203125\n",
      "step: 6101.0, loss: 924.8346462249756\n",
      "step: 6102.0, loss: 926.0756406784058\n",
      "step: 6103.0, loss: 926.2828760147095\n",
      "step: 6104.0, loss: 927.52370262146\n",
      "step: 6105.0, loss: 926.4306783676147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 6106.0, loss: 926.6923198699951\n",
      "step: 6107.0, loss: 927.7827730178833\n",
      "step: 6108.0, loss: 927.0943355560303\n",
      "step: 6109.0, loss: 927.1796455383301\n",
      "step: 6110.0, loss: 927.0687828063965\n",
      "step: 6111.0, loss: 926.4156217575073\n",
      "step: 6112.0, loss: 925.0986938476562\n",
      "step: 6113.0, loss: 926.5244874954224\n",
      "step: 6114.0, loss: 925.6469278335571\n",
      "step: 6115.0, loss: 924.581051826477\n",
      "step: 6116.0, loss: 925.0303106307983\n",
      "step: 6117.0, loss: 924.3282880783081\n",
      "step: 6118.0, loss: 925.6087331771851\n",
      "step: 6119.0, loss: 926.0888071060181\n",
      "step: 6120.0, loss: 925.7141246795654\n",
      "step: 6121.0, loss: 924.1774396896362\n",
      "step: 6122.0, loss: 924.9632215499878\n",
      "step: 6123.0, loss: 925.9814739227295\n",
      "step: 6124.0, loss: 924.3520240783691\n",
      "step: 6125.0, loss: 926.4185247421265\n",
      "step: 6126.0, loss: 926.5818948745728\n",
      "step: 6127.0, loss: 927.9224824905396\n",
      "step: 6128.0, loss: 925.0171766281128\n",
      "step: 6129.0, loss: 924.1602010726929\n",
      "step: 6130.0, loss: 925.1161737442017\n",
      "step: 6131.0, loss: 924.9676284790039\n",
      "step: 6132.0, loss: 925.5980434417725\n",
      "step: 6133.0, loss: 926.933403968811\n",
      "step: 6134.0, loss: 926.7351341247559\n",
      "step: 6135.0, loss: 926.4813871383667\n",
      "step: 6136.0, loss: 925.7182893753052\n",
      "step: 6137.0, loss: 925.3902797698975\n",
      "step: 6138.0, loss: 925.7780246734619\n",
      "step: 6139.0, loss: 926.9604587554932\n",
      "step: 6140.0, loss: 925.6153173446655\n",
      "step: 6141.0, loss: 925.3772840499878\n",
      "step: 6142.0, loss: 926.218563079834\n",
      "step: 6143.0, loss: 926.9362173080444\n",
      "step: 6144.0, loss: 925.6646728515625\n",
      "step: 6145.0, loss: 926.8233623504639\n",
      "step: 6146.0, loss: 925.9328947067261\n",
      "step: 6147.0, loss: 926.9819631576538\n",
      "step: 6148.0, loss: 927.0026369094849\n",
      "step: 6149.0, loss: 926.9229679107666\n",
      "step: 6150.0, loss: 926.2973585128784\n",
      "step: 6151.0, loss: 925.8232107162476\n",
      "step: 6152.0, loss: 926.9318723678589\n",
      "step: 6153.0, loss: 925.8795976638794\n",
      "step: 6154.0, loss: 925.9571876525879\n",
      "step: 6155.0, loss: 924.6438465118408\n",
      "step: 6156.0, loss: 925.7818698883057\n",
      "step: 6157.0, loss: 925.4955282211304\n",
      "step: 6158.0, loss: 925.2636909484863\n",
      "step: 6159.0, loss: 925.0986804962158\n",
      "step: 6160.0, loss: 924.3450145721436\n",
      "step: 6161.0, loss: 924.9726572036743\n",
      "step: 6162.0, loss: 927.2929334640503\n",
      "step: 6163.0, loss: 923.5833568572998\n",
      "step: 6164.0, loss: 924.0720853805542\n",
      "step: 6165.0, loss: 926.1629915237427\n",
      "step: 6166.0, loss: 926.2418766021729\n",
      "step: 6167.0, loss: 927.3611850738525\n",
      "step: 6168.0, loss: 927.4106349945068\n",
      "step: 6169.0, loss: 925.1702041625977\n",
      "step: 6170.0, loss: 925.2697744369507\n",
      "step: 6171.0, loss: 923.8433313369751\n",
      "step: 6172.0, loss: 925.1233444213867\n",
      "step: 6173.0, loss: 926.7543773651123\n",
      "step: 6174.0, loss: 925.2168893814087\n",
      "step: 6175.0, loss: 924.8879728317261\n",
      "step: 6176.0, loss: 925.7695016860962\n",
      "step: 6177.0, loss: 923.3988256454468\n",
      "step: 6178.0, loss: 926.1600160598755\n",
      "step: 6179.0, loss: 926.6506080627441\n",
      "step: 6180.0, loss: 925.6833505630493\n",
      "step: 6181.0, loss: 924.4261283874512\n",
      "step: 6182.0, loss: 927.1854696273804\n",
      "step: 6183.0, loss: 925.841025352478\n",
      "step: 6184.0, loss: 924.8921051025391\n",
      "step: 6185.0, loss: 925.3539047241211\n",
      "step: 6186.0, loss: 924.9672384262085\n",
      "step: 6187.0, loss: 927.4207878112793\n",
      "step: 6188.0, loss: 925.5960836410522\n",
      "step: 6189.0, loss: 926.8504943847656\n",
      "step: 6190.0, loss: 924.5934762954712\n",
      "step: 6191.0, loss: 925.3908758163452\n",
      "step: 6192.0, loss: 925.6996126174927\n",
      "step: 6193.0, loss: 926.1560220718384\n",
      "step: 6194.0, loss: 924.8502054214478\n",
      "step: 6195.0, loss: 928.323525428772\n",
      "step: 6196.0, loss: 924.9777517318726\n",
      "step: 6197.0, loss: 926.2151155471802\n",
      "step: 6198.0, loss: 926.3484773635864\n",
      "step: 6199.0, loss: 925.35378074646\n",
      "step: 6200.0, loss: 924.5433025360107\n",
      "step: 6201.0, loss: 923.6873092651367\n",
      "step: 6202.0, loss: 926.8414716720581\n",
      "step: 6203.0, loss: 925.0130271911621\n",
      "step: 6204.0, loss: 927.1225070953369\n",
      "step: 6205.0, loss: 929.4797859191895\n",
      "step: 6206.0, loss: 925.5720586776733\n",
      "step: 6207.0, loss: 925.477897644043\n",
      "step: 6208.0, loss: 925.7625303268433\n",
      "step: 6209.0, loss: 925.4150648117065\n",
      "step: 6210.0, loss: 927.848879814148\n",
      "step: 6211.0, loss: 926.2100534439087\n",
      "step: 6212.0, loss: 925.131986618042\n",
      "step: 6213.0, loss: 926.3536243438721\n",
      "step: 6214.0, loss: 927.0368585586548\n",
      "step: 6215.0, loss: 924.8649234771729\n",
      "step: 6216.0, loss: 926.8177766799927\n",
      "step: 6217.0, loss: 925.6405572891235\n",
      "step: 6218.0, loss: 925.9975109100342\n",
      "step: 6219.0, loss: 924.1620512008667\n",
      "step: 6220.0, loss: 925.4083919525146\n",
      "step: 6221.0, loss: 925.5097360610962\n",
      "step: 6222.0, loss: 925.373046875\n",
      "step: 6223.0, loss: 924.9576473236084\n",
      "step: 6224.0, loss: 926.0233716964722\n",
      "step: 6225.0, loss: 926.9186248779297\n",
      "step: 6226.0, loss: 926.4025630950928\n",
      "step: 6227.0, loss: 924.5288505554199\n",
      "step: 6228.0, loss: 926.4058904647827\n",
      "step: 6229.0, loss: 925.0728883743286\n",
      "step: 6230.0, loss: 924.4286088943481\n",
      "step: 6231.0, loss: 925.1084852218628\n",
      "step: 6232.0, loss: 923.9620428085327\n",
      "step: 6233.0, loss: 925.6651134490967\n",
      "step: 6234.0, loss: 926.2842617034912\n",
      "step: 6235.0, loss: 923.9260101318359\n",
      "step: 6236.0, loss: 925.9767599105835\n",
      "step: 6237.0, loss: 925.3180990219116\n",
      "step: 6238.0, loss: 926.3199768066406\n",
      "step: 6239.0, loss: 926.0128974914551\n",
      "step: 6240.0, loss: 926.3922996520996\n",
      "step: 6241.0, loss: 924.9201288223267\n",
      "step: 6242.0, loss: 925.7268180847168\n",
      "step: 6243.0, loss: 924.2958993911743\n",
      "step: 6244.0, loss: 925.9930419921875\n",
      "step: 6245.0, loss: 926.2966032028198\n",
      "step: 6246.0, loss: 924.6072378158569\n",
      "step: 6247.0, loss: 925.5093040466309\n",
      "step: 6248.0, loss: 926.2902908325195\n",
      "step: 6249.0, loss: 927.3424978256226\n",
      "step: 6250.0, loss: 925.2489547729492\n",
      "step: 6251.0, loss: 925.6861095428467\n",
      "step: 6252.0, loss: 926.2777910232544\n",
      "step: 6253.0, loss: 925.259464263916\n",
      "step: 6254.0, loss: 925.3601303100586\n",
      "step: 6255.0, loss: 926.2612905502319\n",
      "step: 6256.0, loss: 926.8889045715332\n",
      "step: 6257.0, loss: 926.2816896438599\n",
      "step: 6258.0, loss: 926.823676109314\n",
      "step: 6259.0, loss: 926.6089220046997\n",
      "step: 6260.0, loss: 924.9565420150757\n",
      "step: 6261.0, loss: 922.5433368682861\n",
      "step: 6262.0, loss: 923.7117547988892\n",
      "step: 6263.0, loss: 926.7887687683105\n",
      "step: 6264.0, loss: 923.4214391708374\n",
      "step: 6265.0, loss: 922.9652900695801\n",
      "step: 6266.0, loss: 924.4361095428467\n",
      "step: 6267.0, loss: 925.8144464492798\n",
      "step: 6268.0, loss: 924.8902015686035\n",
      "step: 6269.0, loss: 926.3396863937378\n",
      "step: 6270.0, loss: 924.5343551635742\n",
      "step: 6271.0, loss: 927.6581573486328\n",
      "step: 6272.0, loss: 927.3223819732666\n",
      "step: 6273.0, loss: 925.8179941177368\n",
      "step: 6274.0, loss: 925.620364189148\n",
      "step: 6275.0, loss: 924.8460340499878\n",
      "step: 6276.0, loss: 924.1114349365234\n",
      "step: 6277.0, loss: 926.5363454818726\n",
      "step: 6278.0, loss: 925.513617515564\n",
      "step: 6279.0, loss: 924.2705841064453\n",
      "step: 6280.0, loss: 925.1461896896362\n",
      "step: 6281.0, loss: 928.27867603302\n",
      "step: 6282.0, loss: 925.992974281311\n",
      "step: 6283.0, loss: 925.6850156784058\n",
      "step: 6284.0, loss: 925.8761968612671\n",
      "step: 6285.0, loss: 927.0400075912476\n",
      "step: 6286.0, loss: 926.3009767532349\n",
      "step: 6287.0, loss: 925.6866626739502\n",
      "step: 6288.0, loss: 924.2686491012573\n",
      "step: 6289.0, loss: 924.0365238189697\n",
      "step: 6290.0, loss: 926.5787124633789\n",
      "step: 6291.0, loss: 924.8397760391235\n",
      "step: 6292.0, loss: 924.6502780914307\n",
      "step: 6293.0, loss: 925.8351278305054\n",
      "step: 6294.0, loss: 925.7440547943115\n",
      "step: 6295.0, loss: 925.808931350708\n",
      "step: 6296.0, loss: 924.4613828659058\n",
      "step: 6297.0, loss: 927.2581958770752\n",
      "step: 6298.0, loss: 926.5287990570068\n",
      "step: 6299.0, loss: 926.0845222473145\n",
      "step: 6300.0, loss: 926.2215433120728\n",
      "step: 6301.0, loss: 925.2171726226807\n",
      "step: 6302.0, loss: 924.4835786819458\n",
      "step: 6303.0, loss: 925.9440774917603\n",
      "step: 6304.0, loss: 926.5778112411499\n",
      "step: 6305.0, loss: 924.0494136810303\n",
      "step: 6306.0, loss: 925.3008890151978\n",
      "step: 6307.0, loss: 927.4656267166138\n",
      "step: 6308.0, loss: 925.2844352722168\n",
      "step: 6309.0, loss: 924.9386205673218\n",
      "step: 6310.0, loss: 923.8838338851929\n",
      "step: 6311.0, loss: 925.7617883682251\n",
      "step: 6312.0, loss: 925.4425649642944\n",
      "step: 6313.0, loss: 924.3673658370972\n",
      "step: 6314.0, loss: 925.3515214920044\n",
      "step: 6315.0, loss: 924.3450613021851\n",
      "step: 6316.0, loss: 923.9041738510132\n",
      "step: 6317.0, loss: 925.3873357772827\n",
      "step: 6318.0, loss: 927.1675071716309\n",
      "step: 6319.0, loss: 924.0407466888428\n",
      "step: 6320.0, loss: 924.4683866500854\n",
      "step: 6321.0, loss: 926.4468955993652\n",
      "step: 6322.0, loss: 924.120659828186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 6323.0, loss: 924.8445062637329\n",
      "step: 6324.0, loss: 924.0101919174194\n",
      "step: 6325.0, loss: 925.3958072662354\n",
      "step: 6326.0, loss: 926.2946634292603\n",
      "step: 6327.0, loss: 925.8225975036621\n",
      "step: 6328.0, loss: 925.472785949707\n",
      "step: 6329.0, loss: 923.4855241775513\n",
      "step: 6330.0, loss: 926.9584321975708\n",
      "step: 6331.0, loss: 924.6883115768433\n",
      "step: 6332.0, loss: 926.0530405044556\n",
      "step: 6333.0, loss: 925.0236501693726\n",
      "step: 6334.0, loss: 926.2546768188477\n",
      "step: 6335.0, loss: 925.1619253158569\n",
      "step: 6336.0, loss: 924.6172552108765\n",
      "step: 6337.0, loss: 926.9287233352661\n",
      "step: 6338.0, loss: 923.2123613357544\n",
      "step: 6339.0, loss: 924.8553447723389\n",
      "step: 6340.0, loss: 924.662483215332\n",
      "step: 6341.0, loss: 926.1748561859131\n",
      "step: 6342.0, loss: 927.0042734146118\n",
      "step: 6343.0, loss: 927.3249959945679\n",
      "step: 6344.0, loss: 923.5956106185913\n",
      "step: 6345.0, loss: 925.5728988647461\n",
      "step: 6346.0, loss: 927.9524259567261\n",
      "step: 6347.0, loss: 926.0359382629395\n",
      "step: 6348.0, loss: 925.8848218917847\n",
      "step: 6349.0, loss: 925.8771114349365\n",
      "step: 6350.0, loss: 925.6715536117554\n",
      "step: 6351.0, loss: 925.9995937347412\n",
      "step: 6352.0, loss: 927.4441843032837\n",
      "step: 6353.0, loss: 923.7258348464966\n",
      "step: 6354.0, loss: 926.2074108123779\n",
      "step: 6355.0, loss: 924.03160572052\n",
      "step: 6356.0, loss: 924.4017782211304\n",
      "step: 6357.0, loss: 925.1577978134155\n",
      "step: 6358.0, loss: 926.5453948974609\n",
      "step: 6359.0, loss: 926.0512447357178\n",
      "step: 6360.0, loss: 925.2208843231201\n",
      "step: 6361.0, loss: 926.0462369918823\n",
      "step: 6362.0, loss: 927.236029624939\n",
      "step: 6363.0, loss: 924.9689292907715\n",
      "step: 6364.0, loss: 925.5257606506348\n",
      "step: 6365.0, loss: 927.3980369567871\n",
      "step: 6366.0, loss: 925.7736396789551\n",
      "step: 6367.0, loss: 926.7654418945312\n",
      "step: 6368.0, loss: 926.3725166320801\n",
      "step: 6369.0, loss: 926.146164894104\n",
      "step: 6370.0, loss: 925.6023435592651\n",
      "step: 6371.0, loss: 925.8147354125977\n",
      "step: 6372.0, loss: 925.6318359375\n",
      "step: 6373.0, loss: 926.4687194824219\n",
      "step: 6374.0, loss: 926.7599859237671\n",
      "step: 6375.0, loss: 928.1097612380981\n",
      "step: 6376.0, loss: 926.6215381622314\n",
      "step: 6377.0, loss: 924.7450084686279\n",
      "step: 6378.0, loss: 925.1137094497681\n",
      "step: 6379.0, loss: 924.743691444397\n",
      "step: 6380.0, loss: 926.5110311508179\n",
      "step: 6381.0, loss: 925.2527914047241\n",
      "step: 6382.0, loss: 925.015212059021\n",
      "step: 6383.0, loss: 927.1299476623535\n",
      "step: 6384.0, loss: 923.3215503692627\n",
      "step: 6385.0, loss: 925.8673696517944\n",
      "step: 6386.0, loss: 924.5234308242798\n",
      "step: 6387.0, loss: 927.4237871170044\n",
      "step: 6388.0, loss: 926.4665937423706\n",
      "step: 6389.0, loss: 925.2815370559692\n",
      "step: 6390.0, loss: 923.9981956481934\n",
      "step: 6391.0, loss: 925.1460046768188\n",
      "step: 6392.0, loss: 924.5694465637207\n",
      "step: 6393.0, loss: 926.1367607116699\n",
      "step: 6394.0, loss: 927.86976146698\n",
      "step: 6395.0, loss: 923.4910917282104\n",
      "step: 6396.0, loss: 925.7920989990234\n",
      "step: 6397.0, loss: 924.5933170318604\n",
      "step: 6398.0, loss: 923.6577224731445\n",
      "step: 6399.0, loss: 924.3137397766113\n",
      "step: 6400.0, loss: 924.071005821228\n",
      "step: 6401.0, loss: 924.4653720855713\n",
      "step: 6402.0, loss: 924.9064359664917\n",
      "step: 6403.0, loss: 925.6322937011719\n",
      "step: 6404.0, loss: 925.5289525985718\n",
      "step: 6405.0, loss: 926.4832468032837\n",
      "step: 6406.0, loss: 926.171220779419\n",
      "step: 6407.0, loss: 924.6870679855347\n",
      "step: 6408.0, loss: 925.696102142334\n",
      "step: 6409.0, loss: 925.6322603225708\n",
      "step: 6410.0, loss: 925.2885789871216\n",
      "step: 6411.0, loss: 926.1561374664307\n",
      "step: 6412.0, loss: 926.1177206039429\n",
      "step: 6413.0, loss: 926.5306158065796\n",
      "step: 6414.0, loss: 925.0705757141113\n",
      "step: 6415.0, loss: 922.5511302947998\n",
      "step: 6416.0, loss: 925.5453386306763\n",
      "step: 6417.0, loss: 924.2051820755005\n",
      "step: 6418.0, loss: 924.3067626953125\n",
      "step: 6419.0, loss: 924.4619455337524\n",
      "step: 6420.0, loss: 925.5591993331909\n",
      "step: 6421.0, loss: 926.5646171569824\n",
      "step: 6422.0, loss: 924.8095045089722\n",
      "step: 6423.0, loss: 925.1724462509155\n",
      "step: 6424.0, loss: 924.3305368423462\n",
      "step: 6425.0, loss: 925.518931388855\n",
      "step: 6426.0, loss: 924.9087219238281\n",
      "step: 6427.0, loss: 925.0568132400513\n",
      "step: 6428.0, loss: 927.5895729064941\n",
      "step: 6429.0, loss: 926.3841609954834\n",
      "step: 6430.0, loss: 924.297571182251\n",
      "step: 6431.0, loss: 926.0969858169556\n",
      "step: 6432.0, loss: 926.257342338562\n",
      "step: 6433.0, loss: 924.5299673080444\n",
      "step: 6434.0, loss: 926.8196897506714\n",
      "step: 6435.0, loss: 924.7416658401489\n",
      "step: 6436.0, loss: 927.1145181655884\n",
      "step: 6437.0, loss: 926.4277200698853\n",
      "step: 6438.0, loss: 925.6425189971924\n",
      "step: 6439.0, loss: 925.7742338180542\n",
      "step: 6440.0, loss: 926.2409591674805\n",
      "step: 6441.0, loss: 926.275707244873\n",
      "step: 6442.0, loss: 926.694525718689\n",
      "step: 6443.0, loss: 924.0431394577026\n",
      "step: 6444.0, loss: 924.791314125061\n",
      "step: 6445.0, loss: 926.5399103164673\n",
      "step: 6446.0, loss: 926.2009687423706\n",
      "step: 6447.0, loss: 922.6486692428589\n",
      "step: 6448.0, loss: 924.7862977981567\n",
      "step: 6449.0, loss: 925.50217628479\n",
      "step: 6450.0, loss: 924.5107173919678\n",
      "step: 6451.0, loss: 925.570722579956\n",
      "step: 6452.0, loss: 926.0696067810059\n",
      "step: 6453.0, loss: 924.6017866134644\n",
      "step: 6454.0, loss: 926.0710926055908\n",
      "step: 6455.0, loss: 926.2618455886841\n",
      "step: 6456.0, loss: 923.6210289001465\n",
      "step: 6457.0, loss: 924.6770601272583\n",
      "step: 6458.0, loss: 924.3956546783447\n",
      "step: 6459.0, loss: 925.0579147338867\n",
      "step: 6460.0, loss: 926.9484281539917\n",
      "step: 6461.0, loss: 925.690634727478\n",
      "step: 6462.0, loss: 925.5662288665771\n",
      "step: 6463.0, loss: 925.1781148910522\n",
      "step: 6464.0, loss: 926.0279340744019\n",
      "step: 6465.0, loss: 924.6073026657104\n",
      "step: 6466.0, loss: 925.7165060043335\n",
      "step: 6467.0, loss: 924.5837364196777\n",
      "step: 6468.0, loss: 924.0520963668823\n",
      "step: 6469.0, loss: 926.2922744750977\n",
      "step: 6470.0, loss: 924.6027011871338\n",
      "step: 6471.0, loss: 926.8995914459229\n",
      "step: 6472.0, loss: 928.4680061340332\n",
      "step: 6473.0, loss: 925.9914455413818\n",
      "step: 6474.0, loss: 924.9496421813965\n",
      "step: 6475.0, loss: 926.877194404602\n",
      "step: 6476.0, loss: 924.5181064605713\n",
      "step: 6477.0, loss: 923.3831987380981\n",
      "step: 6478.0, loss: 926.289270401001\n",
      "step: 6479.0, loss: 926.1136350631714\n",
      "step: 6480.0, loss: 923.2818813323975\n",
      "step: 6481.0, loss: 925.0626306533813\n",
      "step: 6482.0, loss: 926.340931892395\n",
      "step: 6483.0, loss: 922.1771669387817\n",
      "step: 6484.0, loss: 926.8316707611084\n",
      "step: 6485.0, loss: 925.207820892334\n",
      "step: 6486.0, loss: 926.8738708496094\n",
      "step: 6487.0, loss: 924.8952045440674\n",
      "step: 6488.0, loss: 925.696307182312\n",
      "step: 6489.0, loss: 924.7279024124146\n",
      "step: 6490.0, loss: 926.6877670288086\n",
      "step: 6491.0, loss: 925.9152183532715\n",
      "step: 6492.0, loss: 925.6216020584106\n",
      "step: 6493.0, loss: 926.1099319458008\n",
      "step: 6494.0, loss: 924.933837890625\n",
      "step: 6495.0, loss: 926.2668104171753\n",
      "step: 6496.0, loss: 924.2884330749512\n",
      "step: 6497.0, loss: 924.6175622940063\n",
      "step: 6498.0, loss: 925.1753845214844\n",
      "step: 6499.0, loss: 923.100866317749\n",
      "step: 6500.0, loss: 923.3573513031006\n",
      "step: 6501.0, loss: 925.2555742263794\n",
      "step: 6502.0, loss: 926.6376333236694\n",
      "step: 6503.0, loss: 927.7647142410278\n",
      "step: 6504.0, loss: 925.2190132141113\n",
      "step: 6505.0, loss: 925.3076858520508\n",
      "step: 6506.0, loss: 925.2976455688477\n",
      "step: 6507.0, loss: 925.3698968887329\n",
      "step: 6508.0, loss: 924.5347890853882\n",
      "step: 6509.0, loss: 925.97718334198\n",
      "step: 6510.0, loss: 925.5005626678467\n",
      "step: 6511.0, loss: 924.2519817352295\n",
      "step: 6512.0, loss: 924.6110248565674\n",
      "step: 6513.0, loss: 924.384614944458\n",
      "step: 6514.0, loss: 925.4764499664307\n",
      "step: 6515.0, loss: 923.5824308395386\n",
      "step: 6516.0, loss: 925.5163841247559\n",
      "step: 6517.0, loss: 927.1084928512573\n",
      "step: 6518.0, loss: 926.2074337005615\n",
      "step: 6519.0, loss: 926.5367374420166\n",
      "step: 6520.0, loss: 925.2004833221436\n",
      "step: 6521.0, loss: 924.3420972824097\n",
      "step: 6522.0, loss: 923.8508415222168\n",
      "step: 6523.0, loss: 925.4967966079712\n",
      "step: 6524.0, loss: 924.8365106582642\n",
      "step: 6525.0, loss: 926.1635084152222\n",
      "step: 6526.0, loss: 926.3248777389526\n",
      "step: 6527.0, loss: 925.3062200546265\n",
      "step: 6528.0, loss: 926.2090187072754\n",
      "step: 6529.0, loss: 926.492259979248\n",
      "step: 6530.0, loss: 925.3824834823608\n",
      "step: 6531.0, loss: 924.223126411438\n",
      "step: 6532.0, loss: 924.896990776062\n",
      "step: 6533.0, loss: 925.0264320373535\n",
      "step: 6534.0, loss: 925.0457172393799\n",
      "step: 6535.0, loss: 925.7418766021729\n",
      "step: 6536.0, loss: 926.758677482605\n",
      "step: 6537.0, loss: 923.9159440994263\n",
      "step: 6538.0, loss: 927.2770709991455\n",
      "step: 6539.0, loss: 926.5758218765259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 6540.0, loss: 923.6131496429443\n",
      "step: 6541.0, loss: 924.2903347015381\n",
      "step: 6542.0, loss: 923.9341106414795\n",
      "step: 6543.0, loss: 923.8497905731201\n",
      "step: 6544.0, loss: 924.2965421676636\n",
      "step: 6545.0, loss: 926.9124412536621\n",
      "step: 6546.0, loss: 924.3603363037109\n",
      "step: 6547.0, loss: 924.863881111145\n",
      "step: 6548.0, loss: 925.25794506073\n",
      "step: 6549.0, loss: 924.8901948928833\n",
      "step: 6550.0, loss: 922.8823041915894\n",
      "step: 6551.0, loss: 925.2858572006226\n",
      "step: 6552.0, loss: 926.4309740066528\n",
      "step: 6553.0, loss: 925.4223041534424\n",
      "step: 6554.0, loss: 924.252537727356\n",
      "step: 6555.0, loss: 924.1106719970703\n",
      "step: 6556.0, loss: 926.1634435653687\n",
      "step: 6557.0, loss: 925.482494354248\n",
      "step: 6558.0, loss: 926.230749130249\n",
      "step: 6559.0, loss: 924.9860992431641\n",
      "step: 6560.0, loss: 924.9309206008911\n",
      "step: 6561.0, loss: 924.4993762969971\n",
      "step: 6562.0, loss: 925.7169933319092\n",
      "step: 6563.0, loss: 925.8278312683105\n",
      "step: 6564.0, loss: 925.582986831665\n",
      "step: 6565.0, loss: 925.0573329925537\n",
      "step: 6566.0, loss: 926.2137975692749\n",
      "step: 6567.0, loss: 926.5339603424072\n",
      "step: 6568.0, loss: 925.6232900619507\n",
      "step: 6569.0, loss: 926.9225740432739\n",
      "step: 6570.0, loss: 924.7052011489868\n",
      "step: 6571.0, loss: 923.2409906387329\n",
      "step: 6572.0, loss: 926.5953359603882\n",
      "step: 6573.0, loss: 925.871506690979\n",
      "step: 6574.0, loss: 925.9012660980225\n",
      "step: 6575.0, loss: 925.4231843948364\n",
      "step: 6576.0, loss: 925.0639181137085\n",
      "step: 6577.0, loss: 925.5359745025635\n",
      "step: 6578.0, loss: 925.3148927688599\n",
      "step: 6579.0, loss: 926.042423248291\n",
      "step: 6580.0, loss: 924.0392303466797\n",
      "step: 6581.0, loss: 925.4554357528687\n",
      "step: 6582.0, loss: 922.7028636932373\n",
      "step: 6583.0, loss: 925.328272819519\n",
      "step: 6584.0, loss: 926.36780834198\n",
      "step: 6585.0, loss: 926.4930229187012\n",
      "step: 6586.0, loss: 925.5483551025391\n",
      "step: 6587.0, loss: 924.379264831543\n",
      "step: 6588.0, loss: 926.5913934707642\n",
      "step: 6589.0, loss: 925.7670383453369\n",
      "step: 6590.0, loss: 924.4595804214478\n",
      "step: 6591.0, loss: 925.082878112793\n",
      "step: 6592.0, loss: 927.3485946655273\n",
      "step: 6593.0, loss: 922.6550235748291\n",
      "step: 6594.0, loss: 925.7886066436768\n",
      "step: 6595.0, loss: 924.7066984176636\n",
      "step: 6596.0, loss: 925.4501142501831\n",
      "step: 6597.0, loss: 924.5289068222046\n",
      "step: 6598.0, loss: 925.098804473877\n",
      "step: 6599.0, loss: 924.0595245361328\n",
      "step: 6600.0, loss: 923.9307098388672\n",
      "step: 6601.0, loss: 924.4745969772339\n",
      "step: 6602.0, loss: 925.2866716384888\n",
      "step: 6603.0, loss: 924.1367921829224\n",
      "step: 6604.0, loss: 924.7870178222656\n",
      "step: 6605.0, loss: 924.7470293045044\n",
      "step: 6606.0, loss: 924.8803110122681\n",
      "step: 6607.0, loss: 923.0960502624512\n",
      "step: 6608.0, loss: 923.3101119995117\n",
      "step: 6609.0, loss: 924.2328863143921\n",
      "step: 6610.0, loss: 925.3071403503418\n",
      "step: 6611.0, loss: 924.9155464172363\n",
      "step: 6612.0, loss: 923.7890367507935\n",
      "step: 6613.0, loss: 925.9019174575806\n",
      "step: 6614.0, loss: 924.9382658004761\n",
      "step: 6615.0, loss: 926.8546323776245\n",
      "step: 6616.0, loss: 926.6544189453125\n",
      "step: 6617.0, loss: 925.2102899551392\n",
      "step: 6618.0, loss: 925.8900270462036\n",
      "step: 6619.0, loss: 922.851185798645\n",
      "step: 6620.0, loss: 926.0371942520142\n",
      "step: 6621.0, loss: 925.4221315383911\n",
      "step: 6622.0, loss: 924.3008661270142\n",
      "step: 6623.0, loss: 927.3202753067017\n",
      "step: 6624.0, loss: 924.2643661499023\n",
      "step: 6625.0, loss: 923.8884372711182\n",
      "step: 6626.0, loss: 925.5775060653687\n",
      "step: 6627.0, loss: 925.0303373336792\n",
      "step: 6628.0, loss: 925.8893117904663\n",
      "step: 6629.0, loss: 924.9584035873413\n",
      "step: 6630.0, loss: 923.5716075897217\n",
      "step: 6631.0, loss: 925.6377630233765\n",
      "step: 6632.0, loss: 923.895378112793\n",
      "step: 6633.0, loss: 923.4409294128418\n",
      "step: 6634.0, loss: 924.5485124588013\n",
      "step: 6635.0, loss: 925.0691528320312\n",
      "step: 6636.0, loss: 924.9136934280396\n",
      "step: 6637.0, loss: 924.8151741027832\n",
      "step: 6638.0, loss: 928.9680500030518\n",
      "step: 6639.0, loss: 927.5769243240356\n",
      "step: 6640.0, loss: 924.9444923400879\n",
      "step: 6641.0, loss: 924.3110179901123\n",
      "step: 6642.0, loss: 924.527530670166\n",
      "step: 6643.0, loss: 923.2025680541992\n",
      "step: 6644.0, loss: 925.024001121521\n",
      "step: 6645.0, loss: 924.5442008972168\n",
      "step: 6646.0, loss: 925.4640235900879\n",
      "step: 6647.0, loss: 924.679708480835\n",
      "step: 6648.0, loss: 924.9368371963501\n",
      "step: 6649.0, loss: 924.7200546264648\n",
      "step: 6650.0, loss: 925.6153001785278\n",
      "step: 6651.0, loss: 924.4403162002563\n",
      "step: 6652.0, loss: 924.624077796936\n",
      "step: 6653.0, loss: 924.0162658691406\n",
      "step: 6654.0, loss: 925.2347602844238\n",
      "step: 6655.0, loss: 923.0379991531372\n",
      "step: 6656.0, loss: 924.3047752380371\n",
      "step: 6657.0, loss: 925.0256299972534\n",
      "step: 6658.0, loss: 926.46937084198\n",
      "step: 6659.0, loss: 924.7145709991455\n",
      "step: 6660.0, loss: 924.4874610900879\n",
      "step: 6661.0, loss: 924.9043388366699\n",
      "step: 6662.0, loss: 926.394534111023\n",
      "step: 6663.0, loss: 924.5514888763428\n",
      "step: 6664.0, loss: 925.7040719985962\n",
      "step: 6665.0, loss: 924.6401205062866\n",
      "step: 6666.0, loss: 926.1264038085938\n",
      "step: 6667.0, loss: 923.7665672302246\n",
      "step: 6668.0, loss: 927.2582626342773\n",
      "step: 6669.0, loss: 923.464056968689\n",
      "step: 6670.0, loss: 924.7280893325806\n",
      "step: 6671.0, loss: 924.0447883605957\n",
      "step: 6672.0, loss: 925.6186819076538\n",
      "step: 6673.0, loss: 927.1049575805664\n",
      "step: 6674.0, loss: 923.7424488067627\n",
      "step: 6675.0, loss: 925.765154838562\n",
      "step: 6676.0, loss: 924.3089742660522\n",
      "step: 6677.0, loss: 924.6243591308594\n",
      "step: 6678.0, loss: 926.1579027175903\n",
      "step: 6679.0, loss: 925.0287714004517\n",
      "step: 6680.0, loss: 925.2771806716919\n",
      "step: 6681.0, loss: 924.6770353317261\n",
      "step: 6682.0, loss: 925.4476022720337\n",
      "step: 6683.0, loss: 925.8858995437622\n",
      "step: 6684.0, loss: 923.8741140365601\n",
      "step: 6685.0, loss: 924.1941499710083\n",
      "step: 6686.0, loss: 924.4560108184814\n",
      "step: 6687.0, loss: 926.6871433258057\n",
      "step: 6688.0, loss: 924.7353200912476\n",
      "step: 6689.0, loss: 925.8275947570801\n",
      "step: 6690.0, loss: 923.9073295593262\n",
      "step: 6691.0, loss: 924.6734313964844\n",
      "step: 6692.0, loss: 924.7612552642822\n",
      "step: 6693.0, loss: 924.4797191619873\n",
      "step: 6694.0, loss: 925.5036468505859\n",
      "step: 6695.0, loss: 925.0143451690674\n",
      "step: 6696.0, loss: 924.7539739608765\n",
      "step: 6697.0, loss: 925.579213142395\n",
      "step: 6698.0, loss: 924.7161283493042\n",
      "step: 6699.0, loss: 923.595458984375\n",
      "step: 6700.0, loss: 925.1933059692383\n",
      "step: 6701.0, loss: 924.372392654419\n",
      "step: 6702.0, loss: 925.3742351531982\n",
      "step: 6703.0, loss: 925.8265581130981\n",
      "step: 6704.0, loss: 924.9267997741699\n",
      "step: 6705.0, loss: 923.96071434021\n",
      "step: 6706.0, loss: 925.1881256103516\n",
      "step: 6707.0, loss: 924.1747903823853\n",
      "step: 6708.0, loss: 925.917405128479\n",
      "step: 6709.0, loss: 927.044490814209\n",
      "step: 6710.0, loss: 923.7869901657104\n",
      "step: 6711.0, loss: 923.857889175415\n",
      "step: 6712.0, loss: 925.1447629928589\n",
      "step: 6713.0, loss: 924.6841373443604\n",
      "step: 6714.0, loss: 924.8295345306396\n",
      "step: 6715.0, loss: 926.766396522522\n",
      "step: 6716.0, loss: 925.8176736831665\n",
      "step: 6717.0, loss: 924.8472299575806\n",
      "step: 6718.0, loss: 924.7511930465698\n",
      "step: 6719.0, loss: 922.3995122909546\n",
      "step: 6720.0, loss: 924.1693906784058\n",
      "step: 6721.0, loss: 923.9644021987915\n",
      "step: 6722.0, loss: 925.8117094039917\n",
      "step: 6723.0, loss: 925.8099212646484\n",
      "step: 6724.0, loss: 924.7189283370972\n",
      "step: 6725.0, loss: 924.409649848938\n",
      "step: 6726.0, loss: 925.4544258117676\n",
      "step: 6727.0, loss: 923.4697751998901\n",
      "step: 6728.0, loss: 926.8919296264648\n",
      "step: 6729.0, loss: 924.9444742202759\n",
      "step: 6730.0, loss: 923.683201789856\n",
      "step: 6731.0, loss: 924.3121881484985\n",
      "step: 6732.0, loss: 925.8996486663818\n",
      "step: 6733.0, loss: 923.86878490448\n",
      "step: 6734.0, loss: 923.3938322067261\n",
      "step: 6735.0, loss: 924.1432237625122\n",
      "step: 6736.0, loss: 924.4386606216431\n",
      "step: 6737.0, loss: 924.3269920349121\n",
      "step: 6738.0, loss: 922.8309268951416\n",
      "step: 6739.0, loss: 925.585126876831\n",
      "step: 6740.0, loss: 925.3464889526367\n",
      "step: 6741.0, loss: 926.6355304718018\n",
      "step: 6742.0, loss: 924.0960683822632\n",
      "step: 6743.0, loss: 925.393648147583\n",
      "step: 6744.0, loss: 926.3478260040283\n",
      "step: 6745.0, loss: 924.4085912704468\n",
      "step: 6746.0, loss: 926.6268529891968\n",
      "step: 6747.0, loss: 922.5732517242432\n",
      "step: 6748.0, loss: 924.8726787567139\n",
      "step: 6749.0, loss: 925.0843715667725\n",
      "step: 6750.0, loss: 924.8032541275024\n",
      "step: 6751.0, loss: 923.1151523590088\n",
      "step: 6752.0, loss: 926.2459287643433\n",
      "step: 6753.0, loss: 924.1525421142578\n",
      "step: 6754.0, loss: 925.6658954620361\n",
      "step: 6755.0, loss: 923.6049919128418\n",
      "step: 6756.0, loss: 925.5843963623047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 6757.0, loss: 925.7204103469849\n",
      "step: 6758.0, loss: 924.2942247390747\n",
      "step: 6759.0, loss: 924.9566287994385\n",
      "step: 6760.0, loss: 924.173921585083\n",
      "step: 6761.0, loss: 924.9002656936646\n",
      "step: 6762.0, loss: 925.0846967697144\n",
      "step: 6763.0, loss: 925.2010707855225\n",
      "step: 6764.0, loss: 924.8735752105713\n",
      "step: 6765.0, loss: 925.413984298706\n",
      "step: 6766.0, loss: 924.5813302993774\n",
      "step: 6767.0, loss: 926.2609348297119\n",
      "step: 6768.0, loss: 922.7355518341064\n",
      "step: 6769.0, loss: 927.044979095459\n",
      "step: 6770.0, loss: 926.0031261444092\n",
      "step: 6771.0, loss: 924.3677520751953\n",
      "step: 6772.0, loss: 925.2375450134277\n",
      "step: 6773.0, loss: 924.8982877731323\n",
      "step: 6774.0, loss: 925.1755485534668\n",
      "step: 6775.0, loss: 923.8886451721191\n",
      "step: 6776.0, loss: 926.6180381774902\n",
      "step: 6777.0, loss: 925.1117076873779\n",
      "step: 6778.0, loss: 926.1805448532104\n",
      "step: 6779.0, loss: 924.3604021072388\n",
      "step: 6780.0, loss: 925.2612199783325\n",
      "step: 6781.0, loss: 923.9717206954956\n",
      "step: 6782.0, loss: 924.0715637207031\n",
      "step: 6783.0, loss: 927.8040437698364\n",
      "step: 6784.0, loss: 922.8962125778198\n",
      "step: 6785.0, loss: 924.2128219604492\n",
      "step: 6786.0, loss: 925.3278379440308\n",
      "step: 6787.0, loss: 925.878978729248\n",
      "step: 6788.0, loss: 925.0389213562012\n",
      "step: 6789.0, loss: 924.528603553772\n",
      "step: 6790.0, loss: 925.2220182418823\n",
      "step: 6791.0, loss: 924.8907117843628\n",
      "step: 6792.0, loss: 924.9159908294678\n",
      "step: 6793.0, loss: 925.440936088562\n",
      "step: 6794.0, loss: 924.0469627380371\n",
      "step: 6795.0, loss: 924.4591341018677\n",
      "step: 6796.0, loss: 924.8685646057129\n",
      "step: 6797.0, loss: 926.2232294082642\n",
      "step: 6798.0, loss: 926.630542755127\n",
      "step: 6799.0, loss: 924.2944946289062\n",
      "step: 6800.0, loss: 924.7351503372192\n",
      "step: 6801.0, loss: 924.4325733184814\n",
      "step: 6802.0, loss: 926.0957937240601\n",
      "step: 6803.0, loss: 923.7160768508911\n",
      "step: 6804.0, loss: 925.0086545944214\n",
      "step: 6805.0, loss: 928.0292444229126\n",
      "step: 6806.0, loss: 925.4317169189453\n",
      "step: 6807.0, loss: 923.9169254302979\n",
      "step: 6808.0, loss: 924.3821153640747\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "min_token_freq = 5\n",
    "epochs = 1\n",
    "embedding_size = 50\n",
    "learning_rate=5e-5\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "model = Word2Vec(len(corpus.word_counts), embedding_size)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_loader = DataLoader(dataset=training_data, batch_size=batch_size, shuffle=True)\n",
    "writer = SummaryWriter()\n",
    "# HINT: wrapping the epoch/step loops in nested tqdm calls is a great way\n",
    "# to keep track of how fast things are and how much longer training will take\n",
    "\n",
    "min_loss = 2000\n",
    "patience =30\n",
    "trigger_times = 0\n",
    "stop_model=False\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    loss_sum = 0.0\n",
    "    \n",
    "    # TODO: use your DataLoader to iterate over the data\n",
    "    for step, data in enumerate(tqdm(train_loader)):\n",
    "        \n",
    "        if stop_model:\n",
    "            break\n",
    "        \n",
    "        # NOTE: since you created the data as a tuple of three np.array instances,\n",
    "        # these have now been converted to Tensor objects for us\n",
    "        target_ids, context_ids, labels = data    \n",
    "        # TODO: Fill in all the training details here\n",
    "        \n",
    "        # TODO: Based on the details in the Homework PDF, periodically\n",
    "        # report the running-sum of the loss to tensorboard. Be sure\n",
    "        # to reset the running sum after reporting it.\n",
    "        \n",
    "        # TODO: it can be helpful to add some early stopping here after\n",
    "        # a fixed number of steps (e.g., if step > max_steps)\n",
    "        optimizer.zero_grad()\n",
    "        y = model(target_ids, context_ids).view(len(target_ids), -1)\n",
    "        \n",
    "        # bias evaulation every 20 steps\n",
    "        if step%20==0:\n",
    "            loss = loss_func(y, labels.float())+bias_evaluation(model)\n",
    "        else:\n",
    "            loss = loss_func(y, labels.float())\n",
    "        \n",
    "#         loss = loss_func(y, labels.float())+bias_evaluation(model)\n",
    "        loss_sum += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if step % 100 == 99:\n",
    "            writer.add_scalar(\"Loss/train\", loss_sum, (step+1)/100)\n",
    "            print(\"step: {}, loss: {}\".format((step+1)/100, loss_sum))\n",
    "\n",
    "            # Early Stopping\n",
    "#             if loss_sum > min_loss:\n",
    "#                 trigger_times += 1\n",
    "#                 print('trigger times:', trigger_times)\n",
    "#                 if trigger_times >= patience:\n",
    "#                     print('Early stopping!\\nStart to test process.')\n",
    "#                     writer.close()\n",
    "#                     model.eval()\n",
    "#                     stop_model=True\n",
    "#             else:\n",
    "#                 print('trigger times: 0')\n",
    "#                 trigger_times = 0\n",
    "#             min_loss = min(loss_sum,min_loss)\n",
    "            loss_sum = 0\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "if not stop_model:          \n",
    "    writer.close()\n",
    "# once you finish training, it's good practice to switch to eval.\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify things are working\n",
    "\n",
    "Once you have an initial model trained, try using the following code to query the model for what are the nearest neighbor of a word. This code is intended to help you debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b16b3cf3f3d41088e9790d71c613c0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/110331 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'word': 'june', 'score': 0.999844491481781},\n",
       " {'word': 'february', 'score': 0.9997945427894592},\n",
       " {'word': 'november', 'score': 0.9997267723083496},\n",
       " {'word': 'july', 'score': 0.9996899962425232},\n",
       " {'word': 'march', 'score': 0.9996562004089355},\n",
       " {'word': 'september', 'score': 0.9995689392089844},\n",
       " {'word': 'august', 'score': 0.9995659589767456},\n",
       " {'word': 'october', 'score': 0.999500036239624},\n",
       " {'word': 'april', 'score': 0.9994309544563293},\n",
       " {'word': 'december', 'score': 0.9990468621253967},\n",
       " {'word': '30', 'score': 0.9873328804969788},\n",
       " {'word': 'aged', 'score': 0.9869542121887207},\n",
       " {'word': '28', 'score': 0.9868108034133911},\n",
       " {'word': '26', 'score': 0.9867387413978577},\n",
       " {'word': 'windsor_park', 'score': 0.9863975048065186},\n",
       " {'word': 'dec', 'score': 0.9863268733024597},\n",
       " {'word': 'pandemic', 'score': 0.9861127734184265},\n",
       " {'word': '86', 'score': 0.9848241806030273},\n",
       " {'word': '27', 'score': 0.9846280217170715},\n",
       " {'word': '84', 'score': 0.9844803810119629},\n",
       " {'word': '91', 'score': 0.9843347072601318},\n",
       " {'word': '87', 'score': 0.9841465950012207},\n",
       " {'word': '79', 'score': 0.9840441346168518},\n",
       " {'word': '88', 'score': 0.9840255379676819},\n",
       " {'word': '83', 'score': 0.984010636806488},\n",
       " {'word': '24', 'score': 0.9838200807571411},\n",
       " {'word': '92', 'score': 0.9831346273422241},\n",
       " {'word': '22', 'score': 0.9829670786857605},\n",
       " {'word': '23', 'score': 0.9829275608062744},\n",
       " {'word': '29', 'score': 0.9827406406402588},\n",
       " {'word': '25', 'score': 0.9827284216880798},\n",
       " {'word': '95', 'score': 0.9822710156440735},\n",
       " {'word': 'stade', 'score': 0.982133150100708},\n",
       " {'word': '85', 'score': 0.9817718267440796},\n",
       " {'word': '93', 'score': 0.981403112411499},\n",
       " {'word': '89', 'score': 0.9812034368515015},\n",
       " {'word': 'boston_bruins', 'score': 0.9810740351676941},\n",
       " {'word': '96', 'score': 0.9807950258255005},\n",
       " {'word': '94', 'score': 0.9807823896408081},\n",
       " {'word': '31', 'score': 0.9807363748550415},\n",
       " {'word': '81', 'score': 0.9803645014762878},\n",
       " {'word': 'hampden_park', 'score': 0.9801604151725769},\n",
       " {'word': '76', 'score': 0.9800557494163513},\n",
       " {'word': 'lada', 'score': 0.9795682430267334},\n",
       " {'word': 'aff_championship', 'score': 0.9794843792915344},\n",
       " {'word': 'sept', 'score': 0.9792231917381287},\n",
       " {'word': 'mighty_ducks_of_anaheim', 'score': 0.9791586399078369},\n",
       " {'word': '15', 'score': 0.978908896446228},\n",
       " {'word': '907', 'score': 0.9788432717323303},\n",
       " {'word': 'toronto_maple_leafs', 'score': 0.9788358807563782},\n",
       " {'word': 'pittsburgh_penguins', 'score': 0.9787930250167847},\n",
       " {'word': 'inches', 'score': 0.9787262082099915},\n",
       " {'word': '75', 'score': 0.9782024025917053},\n",
       " {'word': 'washington_capitals', 'score': 0.977716326713562},\n",
       " {'word': '82', 'score': 0.9776532053947449},\n",
       " {'word': '896', 'score': 0.9776294827461243},\n",
       " {'word': 'new_york_rangers', 'score': 0.9775741696357727},\n",
       " {'word': 'los_angeles_kings', 'score': 0.9775136709213257},\n",
       " {'word': '99', 'score': 0.977509617805481},\n",
       " {'word': 'montreal_canadiens', 'score': 0.9774426817893982},\n",
       " {'word': 'ottawa_senators', 'score': 0.9773561358451843},\n",
       " {'word': 'newlands', 'score': 0.9773462414741516},\n",
       " {'word': '78', 'score': 0.977330207824707},\n",
       " {'word': '18', 'score': 0.9772223830223083},\n",
       " {'word': 'mestis', 'score': 0.9772147536277771},\n",
       " {'word': '77', 'score': 0.977166473865509},\n",
       " {'word': 'philadelphia_flyers', 'score': 0.9768895506858826},\n",
       " {'word': '74', 'score': 0.9768800735473633},\n",
       " {'word': 'rbis', 'score': 0.9767765402793884},\n",
       " {'word': '17', 'score': 0.9767665863037109},\n",
       " {'word': '63', 'score': 0.9767473340034485},\n",
       " {'word': '73', 'score': 0.9763333201408386},\n",
       " {'word': 'dal', 'score': 0.9761463403701782},\n",
       " {'word': '69', 'score': 0.9760936498641968},\n",
       " {'word': '14', 'score': 0.9759873747825623},\n",
       " {'word': '65', 'score': 0.9759098291397095},\n",
       " {'word': 'paulista', 'score': 0.9757607579231262},\n",
       " {'word': '794', 'score': 0.975646436214447},\n",
       " {'word': 'f12', 'score': 0.9755913019180298},\n",
       " {'word': 'montreal_maroons', 'score': 0.9754459261894226},\n",
       " {'word': 'saturday', 'score': 0.9753857851028442},\n",
       " {'word': 'vc', 'score': 0.9753682017326355},\n",
       " {'word': '21', 'score': 0.975162923336029},\n",
       " {'word': 'pless', 'score': 0.9749720692634583},\n",
       " {'word': '68', 'score': 0.9749494194984436},\n",
       " {'word': 'armenian_premier_league', 'score': 0.9749332666397095},\n",
       " {'word': '358', 'score': 0.9748275279998779},\n",
       " {'word': 'db', 'score': 0.9744647741317749},\n",
       " {'word': 'rus', 'score': 0.9744197726249695},\n",
       " {'word': '70', 'score': 0.974348247051239},\n",
       " {'word': '137', 'score': 0.9743388295173645},\n",
       " {'word': '98', 'score': 0.9743284583091736},\n",
       " {'word': '66', 'score': 0.9740756154060364},\n",
       " {'word': 'anaheim_ducks', 'score': 0.9740431308746338},\n",
       " {'word': 'ekstraklasa', 'score': 0.9740043878555298},\n",
       " {'word': 'globalport', 'score': 0.9739488363265991},\n",
       " {'word': '895', 'score': 0.9739117622375488},\n",
       " {'word': 'suning', 'score': 0.9737576246261597},\n",
       " {'word': '687', 'score': 0.9736379384994507},\n",
       " {'word': 'new_jersey_devils', 'score': 0.9732950329780579},\n",
       " {'word': '72', 'score': 0.9732640981674194},\n",
       " {'word': '61', 'score': 0.9732521176338196},\n",
       " {'word': '16', 'score': 0.9731684327125549},\n",
       " {'word': 'china_league_one', 'score': 0.973059892654419},\n",
       " {'word': '55', 'score': 0.97303307056427},\n",
       " {'word': '782', 'score': 0.9728938341140747},\n",
       " {'word': '359', 'score': 0.9728814363479614},\n",
       " {'word': 'africa_cup_of_nations', 'score': 0.9727840423583984},\n",
       " {'word': '163', 'score': 0.9727023839950562},\n",
       " {'word': '80', 'score': 0.9726762175559998},\n",
       " {'word': '19', 'score': 0.972463071346283},\n",
       " {'word': 'hamburg_freezers', 'score': 0.9724236130714417},\n",
       " {'word': 'rbi', 'score': 0.9721856117248535},\n",
       " {'word': '64', 'score': 0.9721523523330688},\n",
       " {'word': 'lola', 'score': 0.9721139669418335},\n",
       " {'word': 'winnipeg_jets', 'score': 0.9720678925514221},\n",
       " {'word': '20', 'score': 0.9720346927642822},\n",
       " {'word': 'del', 'score': 0.9720212817192078},\n",
       " {'word': 'tsv', 'score': 0.9719207882881165},\n",
       " {'word': 'u22', 'score': 0.971904456615448},\n",
       " {'word': '928', 'score': 0.971866250038147},\n",
       " {'word': '428', 'score': 0.971733570098877},\n",
       " {'word': 'aerials', 'score': 0.9716918468475342},\n",
       " {'word': 'pabl', 'score': 0.9715787172317505},\n",
       " {'word': '97', 'score': 0.9715765714645386},\n",
       " {'word': '147', 'score': 0.9715250134468079},\n",
       " {'word': '295', 'score': 0.9714515209197998},\n",
       " {'word': '54', 'score': 0.97139972448349},\n",
       " {'word': 'stolen_bases', 'score': 0.9713553190231323},\n",
       " {'word': '643', 'score': 0.9712783098220825},\n",
       " {'word': 'afc_asian_cup', 'score': 0.9711724519729614},\n",
       " {'word': '536', 'score': 0.9710890054702759},\n",
       " {'word': '71', 'score': 0.9710158109664917},\n",
       " {'word': '13', 'score': 0.970912516117096},\n",
       " {'word': 'voltigeurs', 'score': 0.9707739949226379},\n",
       " {'word': '05', 'score': 0.9706549644470215},\n",
       " {'word': 'buffalo_sabres', 'score': 0.9705939888954163},\n",
       " {'word': '11', 'score': 0.9705935716629028},\n",
       " {'word': '62', 'score': 0.9705837965011597},\n",
       " {'word': 'khimik', 'score': 0.9705401659011841},\n",
       " {'word': 'dallas_stars', 'score': 0.9705201387405396},\n",
       " {'word': '67', 'score': 0.9704563021659851},\n",
       " {'word': 'olds', 'score': 0.9703440070152283},\n",
       " {'word': '304', 'score': 0.9701871871948242},\n",
       " {'word': '176', 'score': 0.9701852202415466},\n",
       " {'word': 'olmpico', 'score': 0.9701831936836243},\n",
       " {'word': 'indianapolis_racers', 'score': 0.9700664281845093},\n",
       " {'word': '787', 'score': 0.9698889255523682},\n",
       " {'word': '04', 'score': 0.9698842763900757},\n",
       " {'word': '861', 'score': 0.9698519706726074},\n",
       " {'word': '12', 'score': 0.9698518514633179},\n",
       " {'word': 'ev', 'score': 0.9697579741477966},\n",
       " {'word': 'fifa_world_cup_qualification', 'score': 0.9696835875511169},\n",
       " {'word': '122', 'score': 0.969619870185852},\n",
       " {'word': 'wc', 'score': 0.9696038365364075},\n",
       " {'word': '161', 'score': 0.9695987701416016},\n",
       " {'word': '865', 'score': 0.9695421457290649},\n",
       " {'word': '103', 'score': 0.9693087935447693},\n",
       " {'word': '618', 'score': 0.9692447781562805},\n",
       " {'word': '721', 'score': 0.9691843390464783},\n",
       " {'word': '162', 'score': 0.9690878987312317},\n",
       " {'word': '138', 'score': 0.969014048576355},\n",
       " {'word': 'columbus_blue_jackets', 'score': 0.9688946008682251},\n",
       " {'word': 'wphl', 'score': 0.9688516855239868},\n",
       " {'word': '713', 'score': 0.9687430262565613},\n",
       " {'word': 'may', 'score': 0.9687126874923706},\n",
       " {'word': '166', 'score': 0.9686893224716187},\n",
       " {'word': 'the_new_york_times_magazine', 'score': 0.9685307741165161},\n",
       " {'word': '915', 'score': 0.968524158000946},\n",
       " {'word': 'primera_b_nacional', 'score': 0.9685239195823669},\n",
       " {'word': '313', 'score': 0.9684917330741882},\n",
       " {'word': '06', 'score': 0.9684725403785706},\n",
       " {'word': 'tampa_bay_lightning', 'score': 0.9684646725654602},\n",
       " {'word': 'new_york_islanders', 'score': 0.9683327078819275},\n",
       " {'word': 'arizona_coyotes', 'score': 0.9681016802787781},\n",
       " {'word': '741', 'score': 0.9679365158081055},\n",
       " {'word': '174', 'score': 0.9678923487663269},\n",
       " {'word': '703', 'score': 0.9678893685340881},\n",
       " {'word': '107', 'score': 0.9676457047462463},\n",
       " {'word': 'daejeon_citizen', 'score': 0.967644214630127},\n",
       " {'word': '57', 'score': 0.9676204323768616},\n",
       " {'word': 'proximus', 'score': 0.9674994349479675},\n",
       " {'word': '757', 'score': 0.9674311280250549},\n",
       " {'word': 'elmira_jackals', 'score': 0.9673409461975098},\n",
       " {'word': 'worcester_sharks', 'score': 0.9672952890396118},\n",
       " {'word': 'kitchener_rangers', 'score': 0.9671510457992554},\n",
       " {'word': '208', 'score': 0.9671148657798767},\n",
       " {'word': '02', 'score': 0.9670361876487732},\n",
       " {'word': 'appleyard', 'score': 0.9669082760810852},\n",
       " {'word': '178', 'score': 0.966892659664154},\n",
       " {'word': 'bruguera', 'score': 0.9668609499931335},\n",
       " {'word': 'feb', 'score': 0.9668324589729309},\n",
       " {'word': 'cypriot_first_division', 'score': 0.966804027557373},\n",
       " {'word': '867', 'score': 0.966729462146759},\n",
       " {'word': 'cc', 'score': 0.9667278528213501},\n",
       " {'word': '228', 'score': 0.9666977524757385},\n",
       " {'word': '763', 'score': 0.9666435122489929},\n",
       " {'word': '53', 'score': 0.9666123986244202},\n",
       " {'word': '505', 'score': 0.9666121006011963},\n",
       " {'word': 'ne', 'score': 0.9665344953536987},\n",
       " {'word': '691', 'score': 0.9664939641952515},\n",
       " {'word': '105', 'score': 0.9664762020111084},\n",
       " {'word': 'pm', 'score': 0.9664721488952637},\n",
       " {'word': 'rouyn', 'score': 0.9664719104766846},\n",
       " {'word': '226', 'score': 0.9664710164070129},\n",
       " {'word': 'og', 'score': 0.9664355516433716},\n",
       " {'word': 'reading_royals', 'score': 0.9664102792739868},\n",
       " {'word': '305', 'score': 0.966243326663971},\n",
       " {'word': 'jun', 'score': 0.9661896228790283},\n",
       " {'word': '524', 'score': 0.9660793542861938},\n",
       " {'word': '805', 'score': 0.9660472869873047},\n",
       " {'word': '664', 'score': 0.9660142660140991},\n",
       " {'word': '133', 'score': 0.9659597873687744},\n",
       " {'word': '149', 'score': 0.9658849835395813},\n",
       " {'word': 'abbotsford_heat', 'score': 0.9658311009407043},\n",
       " {'word': 'kora', 'score': 0.9658142328262329},\n",
       " {'word': '246', 'score': 0.9657125473022461},\n",
       " {'word': '109', 'score': 0.9656499624252319},\n",
       " {'word': '323', 'score': 0.965645968914032},\n",
       " {'word': '216', 'score': 0.9656230807304382},\n",
       " {'word': 'estdio', 'score': 0.9655212163925171},\n",
       " {'word': '45', 'score': 0.9654715061187744},\n",
       " {'word': '925', 'score': 0.9654682278633118},\n",
       " {'word': 'calgary_flames', 'score': 0.9654342532157898},\n",
       " {'word': 'campeonato', 'score': 0.9653683304786682},\n",
       " {'word': '128', 'score': 0.9653680920600891},\n",
       " {'word': '498', 'score': 0.9653409719467163},\n",
       " {'word': 'colorado_avalanche', 'score': 0.9653149247169495},\n",
       " {'word': '197', 'score': 0.96530681848526},\n",
       " {'word': 'zammit', 'score': 0.9652469754219055},\n",
       " {'word': '652', 'score': 0.9651152491569519},\n",
       " {'word': '251', 'score': 0.9650892615318298},\n",
       " {'word': '257', 'score': 0.9650136232376099},\n",
       " {'word': 'russian_premier_league', 'score': 0.9650036096572876},\n",
       " {'word': '180', 'score': 0.9649733901023865},\n",
       " {'word': 'hc_lugano', 'score': 0.9649407267570496},\n",
       " {'word': 'roadrunners', 'score': 0.9648648500442505},\n",
       " {'word': '07', 'score': 0.9647938013076782},\n",
       " {'word': '58', 'score': 0.9647058248519897},\n",
       " {'word': 'ec_red_bull_salzburg', 'score': 0.9645146727561951},\n",
       " {'word': 'sper', 'score': 0.9644850492477417},\n",
       " {'word': '752', 'score': 0.9644314050674438},\n",
       " {'word': '47', 'score': 0.9643129706382751},\n",
       " {'word': '131', 'score': 0.9643056988716125},\n",
       " {'word': 'providence_reds', 'score': 0.9642345309257507},\n",
       " {'word': '10', 'score': 0.9642261862754822},\n",
       " {'word': '59', 'score': 0.9641941785812378},\n",
       " {'word': 'yamaha', 'score': 0.9641289710998535},\n",
       " {'word': '474', 'score': 0.9640980958938599},\n",
       " {'word': '114', 'score': 0.964053213596344},\n",
       " {'word': 'hc_sochi', 'score': 0.9639143943786621},\n",
       " {'word': '071', 'score': 0.9639062285423279},\n",
       " {'word': 'thani', 'score': 0.9638729095458984},\n",
       " {'word': '116', 'score': 0.9638713002204895},\n",
       " {'word': 'motorsport', 'score': 0.9638090133666992},\n",
       " {'word': 'oberliga', 'score': 0.9636442065238953},\n",
       " {'word': '56', 'score': 0.9635655283927917},\n",
       " {'word': 'bayernliga', 'score': 0.9635524749755859},\n",
       " {'word': '126', 'score': 0.9634934663772583},\n",
       " {'word': '158', 'score': 0.9634910225868225},\n",
       " {'word': 'gretzmacher', 'score': 0.963488757610321},\n",
       " {'word': 'earned_run_average', 'score': 0.9633885622024536},\n",
       " {'word': 'lipsky', 'score': 0.96336430311203},\n",
       " {'word': 'primera', 'score': 0.963320791721344},\n",
       " {'word': 'chevrolet', 'score': 0.9633169770240784},\n",
       " {'word': '743', 'score': 0.9633117914199829},\n",
       " {'word': '583', 'score': 0.9632251262664795},\n",
       " {'word': '209', 'score': 0.96322101354599},\n",
       " {'word': '187', 'score': 0.9631817936897278},\n",
       " {'word': 'chinese_super_league', 'score': 0.963111937046051},\n",
       " {'word': 'strikeouts', 'score': 0.9630956053733826},\n",
       " {'word': '164', 'score': 0.9630215764045715},\n",
       " {'word': '146', 'score': 0.9629690647125244},\n",
       " {'word': 'vol', 'score': 0.9628778100013733},\n",
       " {'word': 'wsl', 'score': 0.9627668261528015},\n",
       " {'word': 'albufeira', 'score': 0.96273273229599},\n",
       " {'word': '08', 'score': 0.9626927375793457},\n",
       " {'word': 'phoenix_coyotes', 'score': 0.9626854062080383},\n",
       " {'word': 'prema_powerteam', 'score': 0.9626756906509399},\n",
       " {'word': 'vancouver_canucks', 'score': 0.9626742601394653},\n",
       " {'word': 'cecafa', 'score': 0.9625587463378906},\n",
       " {'word': '496', 'score': 0.9625532627105713},\n",
       " {'word': '682', 'score': 0.9625339508056641},\n",
       " {'word': 'dkw', 'score': 0.9625080227851868},\n",
       " {'word': '157', 'score': 0.9625076651573181},\n",
       " {'word': '563', 'score': 0.9623889923095703},\n",
       " {'word': '09', 'score': 0.9623521566390991},\n",
       " {'word': 'london_knights', 'score': 0.9623201489448547},\n",
       " {'word': '195', 'score': 0.9622783064842224},\n",
       " {'word': 'juhsz', 'score': 0.962151825428009},\n",
       " {'word': 'larnaca', 'score': 0.962144136428833},\n",
       " {'word': 'eredivisie', 'score': 0.9620764851570129},\n",
       " {'word': '831', 'score': 0.9620601534843445},\n",
       " {'word': 'draw', 'score': 0.9620419144630432},\n",
       " {'word': '192', 'score': 0.9620017409324646},\n",
       " {'word': '609', 'score': 0.9619269967079163},\n",
       " {'word': 'liiga', 'score': 0.9619224071502686},\n",
       " {'word': '48', 'score': 0.9619051814079285},\n",
       " {'word': '076', 'score': 0.9618558287620544},\n",
       " {'word': '139', 'score': 0.9618386626243591},\n",
       " {'word': '37', 'score': 0.9618203043937683},\n",
       " {'word': '184', 'score': 0.9617259502410889},\n",
       " {'word': 'formula_renault', 'score': 0.9617068767547607},\n",
       " {'word': '60', 'score': 0.9616812467575073},\n",
       " {'word': '944', 'score': 0.9616271257400513},\n",
       " {'word': 'scottish_premiership', 'score': 0.9615839123725891},\n",
       " {'word': '263', 'score': 0.9615451097488403},\n",
       " {'word': '49', 'score': 0.9615084528923035},\n",
       " {'word': 'fssen', 'score': 0.9615080952644348},\n",
       " {'word': 'austrian_bundesliga', 'score': 0.9614906311035156},\n",
       " {'word': '111', 'score': 0.961488664150238},\n",
       " {'word': 'ligapro', 'score': 0.9614872932434082},\n",
       " {'word': '888', 'score': 0.9614418745040894},\n",
       " {'word': 'sm', 'score': 0.9614377617835999},\n",
       " {'word': 'moose_jaw_warriors', 'score': 0.9613636136054993},\n",
       " {'word': 'toyota', 'score': 0.9613518714904785},\n",
       " {'word': 'truesports', 'score': 0.9613401293754578},\n",
       " {'word': '704', 'score': 0.9613112211227417},\n",
       " {'word': '196', 'score': 0.9612823128700256},\n",
       " {'word': '298', 'score': 0.961205244064331},\n",
       " {'word': '41', 'score': 0.9611283540725708},\n",
       " {'word': 'copa', 'score': 0.9610502123832703},\n",
       " {'word': '345', 'score': 0.9610450267791748},\n",
       " {'word': '287', 'score': 0.9610353112220764},\n",
       " {'word': 'scottish_championship', 'score': 0.9610247015953064},\n",
       " {'word': '332', 'score': 0.9610199332237244},\n",
       " {'word': 'masika', 'score': 0.9610162377357483},\n",
       " {'word': '101', 'score': 0.9610100388526917},\n",
       " {'word': '255', 'score': 0.9610089659690857},\n",
       " {'word': '717', 'score': 0.9609835147857666},\n",
       " {'word': '51', 'score': 0.9609588980674744},\n",
       " {'word': '115', 'score': 0.9609554409980774},\n",
       " {'word': 'perales', 'score': 0.9609513878822327},\n",
       " {'word': 'copa_do_brasil', 'score': 0.960922360420227},\n",
       " {'word': '307', 'score': 0.9609103202819824},\n",
       " {'word': '562', 'score': 0.9608755707740784},\n",
       " {'word': 'phi', 'score': 0.9607778787612915},\n",
       " {'word': '127', 'score': 0.960737407207489},\n",
       " {'word': 'amsterdam_arena', 'score': 0.960715115070343},\n",
       " {'word': '124', 'score': 0.9606258869171143},\n",
       " {'word': 'liga_mx', 'score': 0.9606046676635742},\n",
       " {'word': '671', 'score': 0.9606040120124817},\n",
       " {'word': '351', 'score': 0.9605700969696045},\n",
       " {'word': '159', 'score': 0.960552453994751},\n",
       " {'word': '375', 'score': 0.960532009601593},\n",
       " {'word': 'ashl', 'score': 0.9604945182800293},\n",
       " {'word': 'slovenian_prvaliga', 'score': 0.9604930877685547},\n",
       " {'word': '566', 'score': 0.9604832530021667},\n",
       " {'word': '165', 'score': 0.9604320526123047},\n",
       " {'word': 'habsi', 'score': 0.9603303670883179},\n",
       " {'word': '01', 'score': 0.960330069065094},\n",
       " {'word': 'replacing', 'score': 0.960193395614624},\n",
       " {'word': 'covid', 'score': 0.9601410627365112},\n",
       " {'word': '329', 'score': 0.9601038098335266},\n",
       " {'word': 'saudi_professional_league', 'score': 0.96007239818573},\n",
       " {'word': '221', 'score': 0.9600485563278198},\n",
       " {'word': 'efl_league_two', 'score': 0.960037887096405},\n",
       " {'word': '132', 'score': 0.9599899649620056},\n",
       " {'word': '267', 'score': 0.9599565863609314},\n",
       " {'word': 'gwinnett_gladiators', 'score': 0.959942638874054},\n",
       " {'word': 'gottron', 'score': 0.959937334060669},\n",
       " {'word': '777', 'score': 0.9598904252052307},\n",
       " {'word': '242', 'score': 0.959869921207428},\n",
       " {'word': 'regina_pats', 'score': 0.9598435163497925},\n",
       " {'word': 'tm', 'score': 0.9597699642181396},\n",
       " {'word': 'ehime', 'score': 0.9597556591033936},\n",
       " {'word': 'jtr', 'score': 0.9597357511520386},\n",
       " {'word': '179', 'score': 0.9596869349479675},\n",
       " {'word': 'nov', 'score': 0.9596858620643616},\n",
       " {'word': '709', 'score': 0.9596568942070007},\n",
       " {'word': '596', 'score': 0.9596386551856995},\n",
       " {'word': 'chicago_wolves', 'score': 0.9596030116081238},\n",
       " {'word': 'chicago_blackhawks', 'score': 0.9595596790313721},\n",
       " {'word': '104', 'score': 0.959514856338501},\n",
       " {'word': '220', 'score': 0.9594653844833374},\n",
       " {'word': '119', 'score': 0.9594584107398987},\n",
       " {'word': '897', 'score': 0.9594239592552185},\n",
       " {'word': '319', 'score': 0.9594148397445679},\n",
       " {'word': '145', 'score': 0.9594118595123291},\n",
       " {'word': '129', 'score': 0.9593676924705505},\n",
       " {'word': 'andreev', 'score': 0.9593066573143005},\n",
       " {'word': 'wilkes', 'score': 0.9592394828796387},\n",
       " {'word': '278', 'score': 0.9592353701591492},\n",
       " {'word': 'mats_wilander', 'score': 0.9592268466949463},\n",
       " {'word': '38', 'score': 0.9592042565345764},\n",
       " {'word': 'cska_moscow', 'score': 0.9591471552848816},\n",
       " {'word': 'montego_bay', 'score': 0.9590552449226379},\n",
       " {'word': '881', 'score': 0.9590489268302917},\n",
       " {'word': '35', 'score': 0.9590131044387817},\n",
       " {'word': 'high1', 'score': 0.9590006470680237},\n",
       " {'word': '904', 'score': 0.9589590430259705},\n",
       " {'word': 'westfalen', 'score': 0.9589474201202393},\n",
       " {'word': 'bk', 'score': 0.9588977098464966},\n",
       " {'word': '822', 'score': 0.9588887691497803},\n",
       " {'word': '586', 'score': 0.9588608741760254},\n",
       " {'word': 'kapaz', 'score': 0.9587835073471069},\n",
       " {'word': 'percent', 'score': 0.9587299823760986},\n",
       " {'word': 'home_runs', 'score': 0.9587234258651733},\n",
       " {'word': '280', 'score': 0.9586953520774841},\n",
       " {'word': 'iwate', 'score': 0.9585347175598145},\n",
       " {'word': '113', 'score': 0.958500325679779},\n",
       " {'word': 'icemen', 'score': 0.9584692120552063},\n",
       " {'word': '134', 'score': 0.958457350730896},\n",
       " {'word': 'taraz', 'score': 0.9584478735923767},\n",
       " {'word': 'albanian_superliga', 'score': 0.9583977460861206},\n",
       " {'word': 'olhovskiy', 'score': 0.9583930969238281},\n",
       " {'word': 'liga', 'score': 0.9583551287651062},\n",
       " {'word': 'detroit_red_wings', 'score': 0.958345890045166},\n",
       " {'word': '746', 'score': 0.9583204388618469},\n",
       " {'word': '153', 'score': 0.9582993388175964},\n",
       " {'word': '136', 'score': 0.9582086205482483},\n",
       " {'word': '533', 'score': 0.9581106305122375},\n",
       " {'word': 'persija_jakarta', 'score': 0.9580457210540771},\n",
       " {'word': '261', 'score': 0.9580109119415283},\n",
       " {'word': '756', 'score': 0.9579792022705078},\n",
       " {'word': 'friendly_match', 'score': 0.9579006433486938},\n",
       " {'word': '33', 'score': 0.9578645825386047},\n",
       " {'word': 'uae_pro_league', 'score': 0.9578513503074646},\n",
       " {'word': 'austrian_football_bundesliga', 'score': 0.9578285813331604},\n",
       " {'word': '430', 'score': 0.9577921032905579},\n",
       " {'word': '117', 'score': 0.957757830619812},\n",
       " {'word': 'karaganda', 'score': 0.9577165246009827},\n",
       " {'word': '394', 'score': 0.9577011466026306},\n",
       " {'word': '39', 'score': 0.9576912522315979},\n",
       " {'word': '171', 'score': 0.9575481414794922},\n",
       " {'word': '123', 'score': 0.9575114846229553},\n",
       " {'word': 'eisbren', 'score': 0.9574660062789917},\n",
       " {'word': 'pfeffer', 'score': 0.9574564695358276},\n",
       " {'word': 'conference_south', 'score': 0.957410454750061},\n",
       " {'word': 'heather_watson', 'score': 0.9574048519134521},\n",
       " {'word': '186', 'score': 0.9573889970779419},\n",
       " {'word': '484', 'score': 0.957365870475769},\n",
       " {'word': 'pacific_games', 'score': 0.957329273223877},\n",
       " {'word': 'amrica', 'score': 0.9572842121124268},\n",
       " {'word': '44', 'score': 0.9572412371635437},\n",
       " {'word': '183', 'score': 0.9572018384933472},\n",
       " {'word': 'nchc', 'score': 0.9572000503540039},\n",
       " {'word': 'fsv', 'score': 0.9571759104728699},\n",
       " {'word': 'edmonton_oilers', 'score': 0.9571101069450378},\n",
       " {'word': '425', 'score': 0.9570819139480591},\n",
       " {'word': '792', 'score': 0.9570770263671875},\n",
       " {'word': 'alashkert', 'score': 0.9570518136024475},\n",
       " {'word': 'wg', 'score': 0.957027018070221},\n",
       " {'word': '112', 'score': 0.9568740129470825},\n",
       " {'word': '42', 'score': 0.9568456411361694},\n",
       " {'word': '03', 'score': 0.9567931890487671},\n",
       " {'word': 'sd', 'score': 0.9567474722862244},\n",
       " {'word': 'vols', 'score': 0.9567241072654724},\n",
       " {'word': 'averaging', 'score': 0.9566694498062134},\n",
       " {'word': 'bacolod', 'score': 0.9566141366958618},\n",
       " {'word': '211', 'score': 0.9565656781196594},\n",
       " {'word': 'tlzer', 'score': 0.956529438495636},\n",
       " {'word': '154', 'score': 0.9564821720123291},\n",
       " {'word': '46', 'score': 0.9564412832260132},\n",
       " {'word': 'jul', 'score': 0.9563856720924377},\n",
       " {'word': '32', 'score': 0.9563791751861572},\n",
       " {'word': 'age', 'score': 0.9563409090042114},\n",
       " {'word': 'ejrhl', 'score': 0.9562982320785522},\n",
       " {'word': '218', 'score': 0.9562976956367493},\n",
       " {'word': '156', 'score': 0.9562827944755554},\n",
       " {'word': 'gilas', 'score': 0.9562640190124512},\n",
       " {'word': '504', 'score': 0.956182062625885},\n",
       " {'word': '318', 'score': 0.9561440348625183},\n",
       " {'word': 'serbian_superliga', 'score': 0.956078290939331},\n",
       " {'word': '234', 'score': 0.9560198783874512},\n",
       " {'word': '674', 'score': 0.9560157656669617},\n",
       " {'word': 'gb', 'score': 0.9558361768722534},\n",
       " {'word': 'kakkonen', 'score': 0.9557276964187622},\n",
       " {'word': '248', 'score': 0.9556755423545837},\n",
       " {'word': 'leitner', 'score': 0.9555496573448181},\n",
       " {'word': 'etar', 'score': 0.9555076956748962},\n",
       " {'word': 'netherlands', 'score': 0.9554376602172852},\n",
       " {'word': '953', 'score': 0.9553748369216919},\n",
       " {'word': 'primera_b_metropolitana', 'score': 0.9553147554397583},\n",
       " {'word': '414', 'score': 0.9553142189979553},\n",
       " {'word': '217', 'score': 0.9553009867668152},\n",
       " {'word': '144', 'score': 0.9552995562553406},\n",
       " {'word': '773', 'score': 0.9552712440490723},\n",
       " {'word': 'scottish_premier_league', 'score': 0.9551313519477844},\n",
       " {'word': 'grass', 'score': 0.9550761580467224},\n",
       " {'word': 'cardiff_devils', 'score': 0.9550431370735168},\n",
       " {'word': '885', 'score': 0.9550206065177917},\n",
       " {'word': 'ec', 'score': 0.954983115196228},\n",
       " {'word': '270', 'score': 0.9549655318260193},\n",
       " {'word': '148', 'score': 0.9549459218978882},\n",
       " {'word': 'innings_pitched', 'score': 0.9548691511154175},\n",
       " {'word': 'heracles_almelo', 'score': 0.9548363089561462},\n",
       " {'word': '361', 'score': 0.9547701478004456},\n",
       " {'word': 'cm', 'score': 0.9547349214553833},\n",
       " {'word': '173', 'score': 0.954716145992279},\n",
       " {'word': 'macau_grand_prix', 'score': 0.954698383808136},\n",
       " {'word': 'earing', 'score': 0.9546934962272644},\n",
       " {'word': 'upko', 'score': 0.9546799063682556},\n",
       " {'word': 'mazda', 'score': 0.9545878171920776},\n",
       " {'word': 'ford', 'score': 0.9545844197273254},\n",
       " {'word': 'metres', 'score': 0.954529345035553},\n",
       " {'word': '203', 'score': 0.95452481508255},\n",
       " {'word': 'stadion', 'score': 0.9545173048973083},\n",
       " {'word': 'lc', 'score': 0.9544994831085205},\n",
       " {'word': '804', 'score': 0.9543848633766174},\n",
       " {'word': '281', 'score': 0.9543814659118652},\n",
       " {'word': 'atl9', 'score': 0.9543638825416565},\n",
       " {'word': '160', 'score': 0.9543595314025879},\n",
       " {'word': '189', 'score': 0.9543560147285461},\n",
       " {'word': '36', 'score': 0.9543544054031372},\n",
       " {'word': 'karlsson', 'score': 0.9543420672416687},\n",
       " {'word': 'aeros', 'score': 0.9543331861495972},\n",
       " {'word': '219', 'score': 0.9543180465698242},\n",
       " {'word': 'pim', 'score': 0.95418381690979},\n",
       " {'word': '342', 'score': 0.9541454315185547},\n",
       " {'word': '285', 'score': 0.9541258215904236},\n",
       " {'word': '102', 'score': 0.9540008306503296},\n",
       " {'word': '839', 'score': 0.9539968967437744},\n",
       " {'word': 'liefering', 'score': 0.9539836049079895},\n",
       " {'word': '512', 'score': 0.9539452791213989},\n",
       " {'word': 'latte', 'score': 0.9538898468017578},\n",
       " {'word': 'lapentti', 'score': 0.9538882374763489},\n",
       " {'word': '169', 'score': 0.9538711905479431},\n",
       " {'word': 'il', 'score': 0.9538224339485168},\n",
       " {'word': '125', 'score': 0.9537729620933533},\n",
       " {'word': 'league_two', 'score': 0.9537562727928162},\n",
       " {'word': 'kallang', 'score': 0.9537500143051147},\n",
       " {'word': '873', 'score': 0.9537495374679565},\n",
       " {'word': '325', 'score': 0.9537334442138672},\n",
       " {'word': '135', 'score': 0.9536687731742859},\n",
       " {'word': 'guelph_storm', 'score': 0.9536468982696533},\n",
       " {'word': 'rebounds', 'score': 0.9536235928535461},\n",
       " {'word': 'shenxin', 'score': 0.9535849690437317},\n",
       " {'word': 'dodge', 'score': 0.9535481929779053},\n",
       " {'word': '243', 'score': 0.9535382390022278},\n",
       " {'word': 'kitzbhel', 'score': 0.953488826751709},\n",
       " {'word': '258', 'score': 0.95348060131073},\n",
       " {'word': '698', 'score': 0.9534742832183838},\n",
       " {'word': 'eurocup', 'score': 0.9534474015235901},\n",
       " {'word': 'busan_ipark', 'score': 0.9534270763397217},\n",
       " {'word': 'saff', 'score': 0.9533253908157349},\n",
       " {'word': 'persib_bandung', 'score': 0.9532675743103027},\n",
       " {'word': '390', 'score': 0.9532510042190552},\n",
       " {'word': '621', 'score': 0.953246533870697},\n",
       " {'word': 'mike_bryan', 'score': 0.9532096982002258},\n",
       " {'word': '315', 'score': 0.9531832933425903},\n",
       " {'word': '392', 'score': 0.9531502723693848},\n",
       " {'word': 'litres', 'score': 0.9531358480453491},\n",
       " {'word': 'woodhead', 'score': 0.9531286954879761},\n",
       " {'word': '926', 'score': 0.9531260132789612},\n",
       " {'word': '108', 'score': 0.953122615814209},\n",
       " {'word': 'changchun_yatai', 'score': 0.9530182480812073},\n",
       " {'word': 'fehrvr', 'score': 0.9529175758361816},\n",
       " {'word': '310', 'score': 0.9529068470001221},\n",
       " {'word': 'lausanne_hc', 'score': 0.9528926610946655},\n",
       " {'word': 'isthmian_league_premier_division', 'score': 0.9528719186782837},\n",
       " {'word': '206', 'score': 0.9528523087501526},\n",
       " {'word': 'ontario_reign', 'score': 0.9528401494026184},\n",
       " {'word': '764', 'score': 0.952833890914917},\n",
       " {'word': '595', 'score': 0.9527198076248169},\n",
       " {'word': 'rebounds_per_game', 'score': 0.952633798122406},\n",
       " {'word': '853', 'score': 0.9526278376579285},\n",
       " {'word': 'liaoning_whowin', 'score': 0.9525781869888306},\n",
       " {'word': '890', 'score': 0.9525490403175354},\n",
       " {'word': 'nizhnekamsk', 'score': 0.9525472521781921},\n",
       " {'word': '175', 'score': 0.9524897336959839},\n",
       " {'word': '43', 'score': 0.9523916244506836},\n",
       " {'word': 'sigulda', 'score': 0.9523012042045593},\n",
       " {'word': '339', 'score': 0.9522596001625061},\n",
       " {'word': '892', 'score': 0.9522417187690735},\n",
       " {'word': 'ocanic', 'score': 0.952214777469635},\n",
       " {'word': '193', 'score': 0.9520469307899475},\n",
       " {'word': '517', 'score': 0.9520291686058044},\n",
       " {'word': '330', 'score': 0.9518691897392273},\n",
       " {'word': '081', 'score': 0.9518675804138184},\n",
       " {'word': 'thrashed', 'score': 0.9516429305076599},\n",
       " {'word': 'primera_c_metropolitana', 'score': 0.9516162276268005},\n",
       " {'word': 'jakobsen', 'score': 0.951579749584198},\n",
       " {'word': '744', 'score': 0.9515446424484253},\n",
       " {'word': '272', 'score': 0.9515100717544556},\n",
       " {'word': '118', 'score': 0.9515059590339661},\n",
       " {'word': 'atlanta_thrashers', 'score': 0.9514905214309692},\n",
       " {'word': '628', 'score': 0.9514732956886292},\n",
       " {'word': 'manitoba_moose', 'score': 0.9514446258544922},\n",
       " {'word': '202', 'score': 0.9514250755310059},\n",
       " {'word': '032', 'score': 0.9514250159263611},\n",
       " {'word': 'honiara', 'score': 0.9513700604438782},\n",
       " {'word': 'divisin', 'score': 0.9513384699821472},\n",
       " {'word': 'v1', 'score': 0.9513338208198547},\n",
       " {'word': '215', 'score': 0.9512648582458496},\n",
       " {'word': 'regionalliga', 'score': 0.9512645602226257},\n",
       " {'word': 'chl', 'score': 0.951166570186615},\n",
       " {'word': '168', 'score': 0.9509832262992859},\n",
       " {'word': '832', 'score': 0.9509674310684204},\n",
       " {'word': 'kuwait_city', 'score': 0.950839638710022},\n",
       " {'word': '239', 'score': 0.9507380723953247},\n",
       " {'word': 'yevgeny_kafelnikov', 'score': 0.9506694674491882},\n",
       " {'word': 'carlin_motorsport', 'score': 0.9506357312202454},\n",
       " {'word': 'league_of_ireland_first_division', 'score': 0.950631320476532},\n",
       " {'word': 'gambrinus_liga', 'score': 0.9505720734596252},\n",
       " {'word': 'caribbean_cup', 'score': 0.9505662322044373},\n",
       " {'word': 'passes', 'score': 0.9505169987678528},\n",
       " {'word': 'receptions', 'score': 0.9504614472389221},\n",
       " {'word': '548', 'score': 0.9504410624504089},\n",
       " {'word': '382', 'score': 0.950432538986206},\n",
       " {'word': '328', 'score': 0.950428307056427},\n",
       " {'word': 'schenley', 'score': 0.9503430128097534},\n",
       " {'word': 'na', 'score': 0.9503355622291565},\n",
       " {'word': '121', 'score': 0.9503107666969299},\n",
       " {'word': 'uncaf', 'score': 0.9502993822097778},\n",
       " {'word': 'catches', 'score': 0.9502950310707092},\n",
       " {'word': 'league_of_ireland_premier_division', 'score': 0.9502483606338501},\n",
       " {'word': 'sahiti', 'score': 0.9501764178276062},\n",
       " {'word': '349', 'score': 0.9501740336418152},\n",
       " {'word': 'fc_kuban_krasnodar', 'score': 0.950089693069458},\n",
       " {'word': 'new_england_whalers', 'score': 0.9500772953033447},\n",
       " {'word': '269', 'score': 0.9500635862350464},\n",
       " {'word': '440', 'score': 0.9500225782394409},\n",
       " {'word': 'singapore_premier_league', 'score': 0.9499801397323608},\n",
       " {'word': '106', 'score': 0.9499733448028564},\n",
       " {'word': '553', 'score': 0.9499537944793701},\n",
       " {'word': 'league_one', 'score': 0.9499338865280151},\n",
       " {'word': 'gabala', 'score': 0.9499337077140808},\n",
       " {'word': '510', 'score': 0.9499167799949646},\n",
       " {'word': 'tt_pro_league', 'score': 0.9499164819717407},\n",
       " {'word': '383', 'score': 0.9498429894447327},\n",
       " {'word': 'wjc', 'score': 0.9498409032821655},\n",
       " {'word': '1994', 'score': 0.9497037529945374},\n",
       " {'word': '1997', 'score': 0.9496209025382996},\n",
       " {'word': '2018', 'score': 0.9495940804481506},\n",
       " {'word': 'anastasia_rodionova', 'score': 0.9495615363121033},\n",
       " {'word': 'racing', 'score': 0.949495792388916},\n",
       " {'word': '847', 'score': 0.9494832754135132},\n",
       " {'word': 'acassuso', 'score': 0.9494456052780151},\n",
       " {'word': 'nissan', 'score': 0.9494156837463379},\n",
       " {'word': 'barchfeld', 'score': 0.9493831992149353},\n",
       " {'word': 'div', 'score': 0.9493516087532043},\n",
       " {'word': '514', 'score': 0.9493486285209656},\n",
       " {'word': 'gauloises', 'score': 0.949314296245575},\n",
       " {'word': '324', 'score': 0.9492958188056946},\n",
       " {'word': 'rochefoucauld', 'score': 0.9492942094802856},\n",
       " {'word': '249', 'score': 0.9492218494415283},\n",
       " {'word': '265', 'score': 0.9491910338401794},\n",
       " {'word': 'hertha_bsc_ii', 'score': 0.949177086353302},\n",
       " {'word': 'meters', 'score': 0.9491305947303772},\n",
       " {'word': '769', 'score': 0.9491155743598938},\n",
       " {'word': '167', 'score': 0.9490748047828674},\n",
       " {'word': 'k_league', 'score': 0.9490519762039185},\n",
       " {'word': 'etisalat', 'score': 0.9489795565605164},\n",
       " {'word': '625', 'score': 0.9489789605140686},\n",
       " {'word': 'carolina_hurricanes', 'score': 0.9487965703010559},\n",
       " {'word': '34', 'score': 0.9487310647964478},\n",
       " {'word': '884', 'score': 0.9486500024795532},\n",
       " {'word': 'fc_tokyo', 'score': 0.9486114978790283},\n",
       " {'word': 'vs', 'score': 0.9485525488853455},\n",
       " {'word': 'triples', 'score': 0.9484735131263733},\n",
       " {'word': 'defensores', 'score': 0.9483935236930847},\n",
       " {'word': '378', 'score': 0.9483352303504944},\n",
       " {'word': '1982', 'score': 0.9483245611190796},\n",
       " {'word': 'gtzis', 'score': 0.9483035206794739},\n",
       " {'word': 'lire', 'score': 0.9482839107513428},\n",
       " {'word': 'czech_first_league', 'score': 0.9482825994491577},\n",
       " {'word': '415', 'score': 0.9481761455535889},\n",
       " {'word': '244', 'score': 0.9481081366539001},\n",
       " {'word': '141', 'score': 0.9480932950973511},\n",
       " {'word': '143', 'score': 0.9480768442153931},\n",
       " {'word': 'zhongji', 'score': 0.9480473399162292},\n",
       " {'word': '708', 'score': 0.9480214715003967},\n",
       " {'word': 'sandar', 'score': 0.9480034708976746},\n",
       " {'word': '619', 'score': 0.9478931427001953},\n",
       " {'word': 'i_liga', 'score': 0.9478843808174133},\n",
       " {'word': 'swings', 'score': 0.9478598833084106},\n",
       " {'word': 'tps', 'score': 0.9478588104248047},\n",
       " {'word': '201', 'score': 0.9478471279144287},\n",
       " {'word': '2000', 'score': 0.9478384256362915},\n",
       " {'word': '401', 'score': 0.9478065371513367},\n",
       " {'word': 'maineiacs', 'score': 0.9477847218513489},\n",
       " {'word': 'bc', 'score': 0.9477487206459045},\n",
       " {'word': '185', 'score': 0.9477320909500122},\n",
       " {'word': '943', 'score': 0.9476408958435059},\n",
       " {'word': '526', 'score': 0.9476224780082703},\n",
       " {'word': '297', 'score': 0.9476116299629211},\n",
       " {'word': 'segunda', 'score': 0.9475147128105164},\n",
       " {'word': '181', 'score': 0.9474976658821106},\n",
       " {'word': '360', 'score': 0.9474744200706482},\n",
       " {'word': 'fortec', 'score': 0.9474688172340393},\n",
       " {'word': '2019', 'score': 0.9473958611488342},\n",
       " {'word': '882', 'score': 0.9473828673362732},\n",
       " {'word': '669', 'score': 0.9473665356636047},\n",
       " {'word': 'belleville_bulls', 'score': 0.9472576975822449},\n",
       " {'word': '320', 'score': 0.9472180008888245},\n",
       " {'word': 'mal4', 'score': 0.9471864104270935},\n",
       " {'word': '864', 'score': 0.9471134543418884},\n",
       " {'word': 'tallying', 'score': 0.9471033811569214},\n",
       " {'word': '386', 'score': 0.9470670819282532},\n",
       " {'word': '689', 'score': 0.9470452666282654},\n",
       " {'word': '292', 'score': 0.9470224380493164},\n",
       " {'word': '791', 'score': 0.9470093846321106},\n",
       " {'word': 'seattle_thunderbirds', 'score': 0.9469879269599915},\n",
       " {'word': '1943', 'score': 0.9469814896583557},\n",
       " {'word': 'avtomobilist_yekaterinburg', 'score': 0.9469767212867737},\n",
       " {'word': 'ratingen', 'score': 0.9469226598739624},\n",
       " {'word': '290', 'score': 0.9468965530395508},\n",
       " {'word': '591', 'score': 0.9468778371810913},\n",
       " {'word': '1998', 'score': 0.9468715786933899},\n",
       " {'word': 'pustertal', 'score': 0.9468013644218445},\n",
       " {'word': '306', 'score': 0.9466437101364136},\n",
       " {'word': 'ls', 'score': 0.9466099143028259},\n",
       " {'word': 'elady', 'score': 0.9466026425361633},\n",
       " {'word': '110', 'score': 0.9465881586074829},\n",
       " {'word': 'defeat', 'score': 0.94656902551651},\n",
       " {'word': 'pokal', 'score': 0.9465041160583496},\n",
       " {'word': '022', 'score': 0.946344792842865},\n",
       " {'word': '191', 'score': 0.9462205171585083},\n",
       " {'word': 'rgle', 'score': 0.9461595416069031},\n",
       " {'word': 'jfl', 'score': 0.9461129307746887},\n",
       " {'word': '326', 'score': 0.9460585117340088},\n",
       " {'word': '856', 'score': 0.9458932280540466},\n",
       " {'word': '644', 'score': 0.9458754658699036},\n",
       " {'word': '194', 'score': 0.9458500146865845},\n",
       " {'word': 'carrera', 'score': 0.9457818269729614},\n",
       " {'word': '714', 'score': 0.9457377791404724},\n",
       " {'word': 'lake_erie_monsters', 'score': 0.9456821084022522},\n",
       " {'word': 'donji', 'score': 0.9456537365913391},\n",
       " {'word': 'ost', 'score': 0.9456377029418945},\n",
       " {'word': 'sep', 'score': 0.9456113576889038},\n",
       " {'word': '501', 'score': 0.9455340504646301},\n",
       " {'word': 'china_league_two', 'score': 0.9454595446586609},\n",
       " {'word': '90', 'score': 0.9454384446144104},\n",
       " {'word': '120', 'score': 0.9454342722892761},\n",
       " {'word': '276', 'score': 0.9453946948051453},\n",
       " {'word': 'cent', 'score': 0.9453713893890381},\n",
       " {'word': '302', 'score': 0.9452899694442749},\n",
       " {'word': '1999', 'score': 0.9452806115150452},\n",
       " {'word': 'swiss_challenge_league', 'score': 0.9452699422836304},\n",
       " {'word': '519', 'score': 0.9452323317527771},\n",
       " {'word': '812', 'score': 0.9452190399169922},\n",
       " {'word': 'vetch', 'score': 0.9452175498008728},\n",
       " {'word': '284', 'score': 0.945209801197052},\n",
       " {'word': 'husrov', 'score': 0.9452050924301147},\n",
       " {'word': 'ofc_nations_cup', 'score': 0.9451631903648376},\n",
       " {'word': 'belgian_pro_league', 'score': 0.9450766444206238},\n",
       " {'word': 'waregem', 'score': 0.9448964595794678},\n",
       " {'word': 'pioline', 'score': 0.9448622465133667},\n",
       " {'word': '155', 'score': 0.9448307752609253},\n",
       " {'word': 'karlovi', 'score': 0.9446942210197449},\n",
       " {'word': 'dmitry_tursunov', 'score': 0.9446861147880554},\n",
       " {'word': 'naional', 'score': 0.9446454048156738},\n",
       " {'word': 'cska', 'score': 0.9446051120758057},\n",
       " {'word': 'pfc_spartak_nalchik', 'score': 0.9445188045501709},\n",
       " {'word': '1988', 'score': 0.9444758892059326},\n",
       " {'word': 'csl', 'score': 0.9444673657417297},\n",
       " {'word': 'volta_a_catalunya', 'score': 0.944460928440094},\n",
       " {'word': 'averaged', 'score': 0.9444363117218018},\n",
       " {'word': 'fminine', 'score': 0.9444069862365723},\n",
       " {'word': '327', 'score': 0.9443864226341248},\n",
       " {'word': 'joe_gibbs_racing', 'score': 0.9443526864051819},\n",
       " {'word': '582', 'score': 0.9443327188491821},\n",
       " {'word': '894', 'score': 0.9443044066429138},\n",
       " {'word': 'atp_world_tour', 'score': 0.9443003535270691},\n",
       " {'word': 'azadegan_league', 'score': 0.9441784620285034},\n",
       " {'word': 'paykan', 'score': 0.9441422820091248},\n",
       " {'word': 'bests', 'score': 0.944100022315979},\n",
       " {'word': '750', 'score': 0.9440735578536987},\n",
       " {'word': '918', 'score': 0.94404137134552},\n",
       " {'word': '188', 'score': 0.9440193176269531},\n",
       " {'word': 'mtech', 'score': 0.9438959956169128},\n",
       " {'word': 'vfl_bochum', 'score': 0.9438830018043518},\n",
       " {'word': 'adapazar', 'score': 0.9438677430152893},\n",
       " {'word': 'reimer', 'score': 0.9438655972480774},\n",
       " {'word': '761', 'score': 0.9437925815582275},\n",
       " {'word': 'ft', 'score': 0.9437616467475891},\n",
       " {'word': 'joest', 'score': 0.9437285661697388},\n",
       " {'word': '754', 'score': 0.9436600804328918},\n",
       " {'word': '004', 'score': 0.9435917735099792},\n",
       " {'word': 'torneo_argentino_a', 'score': 0.9435767531394958},\n",
       " {'word': '855', 'score': 0.9435142874717712},\n",
       " {'word': 'torneo_argentino_b', 'score': 0.9434908032417297},\n",
       " {'word': 'indy_eleven', 'score': 0.943478524684906},\n",
       " {'word': '170', 'score': 0.9434112310409546},\n",
       " {'word': 'fcsb', 'score': 0.9434105157852173},\n",
       " {'word': '1993', 'score': 0.943381130695343},\n",
       " {'word': 'steals', 'score': 0.9433512687683105},\n",
       " {'word': '836', 'score': 0.9432746171951294},\n",
       " {'word': '142', 'score': 0.9432516694068909},\n",
       " {'word': '408', 'score': 0.9432220458984375},\n",
       " {'word': '824', 'score': 0.9431760907173157},\n",
       " {'word': '913', 'score': 0.9431630373001099},\n",
       " {'word': 'krpt', 'score': 0.943045973777771},\n",
       " {'word': 'sane', 'score': 0.942993700504303},\n",
       " {'word': 'renhe', 'score': 0.9429547786712646},\n",
       " {'word': 'oct', 'score': 0.9429050087928772},\n",
       " {'word': 'financial_post', 'score': 0.94287109375},\n",
       " {'word': 'independiente_rivadavia', 'score': 0.9428198337554932},\n",
       " {'word': 'hk', 'score': 0.9428074359893799},\n",
       " {'word': 'karaiskakis', 'score': 0.9427466988563538},\n",
       " {'word': 'hc_davos', 'score': 0.9426620602607727},\n",
       " {'word': 'usa_indoor_track_and_field_championships',\n",
       "  'score': 0.9426403641700745},\n",
       " {'word': 'noranda', 'score': 0.9426052570343018},\n",
       " {'word': 'corse', 'score': 0.9425861239433289},\n",
       " {'word': '538', 'score': 0.9425613284111023},\n",
       " {'word': 'votes', 'score': 0.9425483345985413},\n",
       " {'word': '2001', 'score': 0.942543089389801},\n",
       " {'word': 'hove', 'score': 0.9425011873245239},\n",
       " {'word': 'british_home_championship', 'score': 0.9423840045928955},\n",
       " {'word': 'centennials', 'score': 0.942352294921875},\n",
       " {'word': 'bruneck', 'score': 0.9423295855522156},\n",
       " {'word': '1996', 'score': 0.9423278570175171},\n",
       " {'word': '920', 'score': 0.9422697424888611},\n",
       " {'word': 't42', 'score': 0.9422504901885986},\n",
       " {'word': 'serbian_cyrillic', 'score': 0.9422489404678345},\n",
       " {'word': 'srie', 'score': 0.9422258138656616},\n",
       " {'word': '490', 'score': 0.9421858787536621},\n",
       " {'word': '152', 'score': 0.9421725869178772},\n",
       " {'word': 'dsseldorfer', 'score': 0.9420902729034424},\n",
       " {'word': '151', 'score': 0.9419835805892944},\n",
       " {'word': 'vtkovice', 'score': 0.9419494867324829},\n",
       " {'word': '067', 'score': 0.9419295787811279},\n",
       " {'word': 'friendly', 'score': 0.9418197274208069},\n",
       " {'word': 'jyp', 'score': 0.9418036341667175},\n",
       " {'word': '1987', 'score': 0.9417722821235657},\n",
       " {'word': 'categora', 'score': 0.9417667984962463},\n",
       " {'word': 'racecourse_ground', 'score': 0.9416924118995667},\n",
       " {'word': '356', 'score': 0.9416241645812988},\n",
       " {'word': 'points_per_game', 'score': 0.9415737986564636},\n",
       " {'word': 'wrt', 'score': 0.9415448307991028},\n",
       " {'word': 'lbs', 'score': 0.9415319561958313},\n",
       " {'word': 'belgian_first_division_a', 'score': 0.9415236711502075},\n",
       " {'word': 'augie', 'score': 0.9414951205253601},\n",
       " {'word': '675', 'score': 0.9414920210838318},\n",
       " {'word': 'la_liga', 'score': 0.9414880275726318},\n",
       " {'word': 'bienne', 'score': 0.9414762854576111},\n",
       " {'word': 'kos', 'score': 0.9414012432098389},\n",
       " {'word': '1991', 'score': 0.9413995742797852},\n",
       " {'word': 'komatsubara', 'score': 0.9413728713989258},\n",
       " {'word': 'triple_jump', 'score': 0.9412869215011597},\n",
       " {'word': 'petronas', 'score': 0.9412704706192017},\n",
       " {'word': 'dlouh', 'score': 0.9412441253662109},\n",
       " {'word': 'wcha', 'score': 0.9412338733673096},\n",
       " {'word': '316', 'score': 0.9412175416946411},\n",
       " {'word': '40', 'score': 0.941136360168457},\n",
       " {'word': 'the_brookings_institution', 'score': 0.941123366355896},\n",
       " {'word': 'pts', 'score': 0.9410461187362671},\n",
       " {'word': 'timeline', 'score': 0.9409768581390381},\n",
       " {'word': '294', 'score': 0.9408872723579407},\n",
       " {'word': 'ligue', 'score': 0.9407854676246643},\n",
       " {'word': '431', 'score': 0.9407543540000916},\n",
       " {'word': 'aboomoslem', 'score': 0.9405978322029114},\n",
       " {'word': 'qmjhl', 'score': 0.9405391216278076},\n",
       " {'word': 'teniente', 'score': 0.9405205845832825},\n",
       " {'word': 'tippeligaen', 'score': 0.9405164122581482},\n",
       " {'word': 'mantilla', 'score': 0.9404566287994385},\n",
       " {'word': '177', 'score': 0.9402951002120972},\n",
       " {'word': 'karvelas', 'score': 0.940284788608551},\n",
       " {'word': 'linkpings', 'score': 0.940265953540802},\n",
       " {'word': 'guy_forget', 'score': 0.9402448534965515},\n",
       " {'word': 'realgm', 'score': 0.9402294158935547},\n",
       " {'word': '532', 'score': 0.9402063488960266},\n",
       " {'word': 'bcjhl', 'score': 0.9401637315750122},\n",
       " {'word': 'tobol', 'score': 0.9400971531867981},\n",
       " {'word': 'doha', 'score': 0.9400652647018433},\n",
       " {'word': 'buick', 'score': 0.9400317072868347},\n",
       " {'word': 'colorado_eagles', 'score': 0.9399864673614502},\n",
       " {'word': 'soldier_field', 'score': 0.9398365616798401},\n",
       " {'word': 'bvl', 'score': 0.939812183380127},\n",
       " {'word': 'oman_professional_league', 'score': 0.9397839307785034},\n",
       " {'word': 'runs_batted_in', 'score': 0.9395410418510437},\n",
       " {'word': 'cadbury', 'score': 0.939526379108429},\n",
       " {'word': '282', 'score': 0.9394761919975281},\n",
       " {'word': 'cubic', 'score': 0.9393659234046936},\n",
       " {'word': 'hartford_wolf_pack', 'score': 0.9393203854560852},\n",
       " {'word': 'lig', 'score': 0.9393032789230347},\n",
       " {'word': 'me', 'score': 0.9392998218536377},\n",
       " {'word': '199', 'score': 0.9392786622047424},\n",
       " {'word': '204', 'score': 0.9392409920692444},\n",
       " {'word': '271', 'score': 0.9391897916793823},\n",
       " {'word': 'hom12', 'score': 0.9391121864318848},\n",
       " {'word': 'loss', 'score': 0.9391059279441833},\n",
       " {'word': '314', 'score': 0.9390684366226196},\n",
       " {'word': '402', 'score': 0.9390554428100586},\n",
       " {'word': '636', 'score': 0.9390310049057007},\n",
       " {'word': '085', 'score': 0.9390082955360413},\n",
       " {'word': 'liga_mx_femenil', 'score': 0.9388834238052368},\n",
       " {'word': 'rahal', 'score': 0.9388782978057861},\n",
       " {'word': 'hv71', 'score': 0.9388467073440552},\n",
       " {'word': '2004', 'score': 0.9388003349304199},\n",
       " {'word': 'tercera', 'score': 0.9387992024421692},\n",
       " {'word': 'nashville_predators', 'score': 0.9387838244438171},\n",
       " {'word': 'aviva_stadium', 'score': 0.9386768341064453},\n",
       " {'word': 'fc_krasnodar', 'score': 0.9386627674102783},\n",
       " {'word': 'thistle', 'score': 0.9385907053947449},\n",
       " {'word': 'cija', 'score': 0.9385429620742798},\n",
       " {'word': '728', 'score': 0.938512921333313},\n",
       " {'word': 'kristall', 'score': 0.9384734034538269},\n",
       " {'word': '231', 'score': 0.9384217858314514},\n",
       " {'word': '796', 'score': 0.9383960366249084},\n",
       " {'word': 'dampierre', 'score': 0.9382572770118713},\n",
       " {'word': '393', 'score': 0.9382243752479553},\n",
       " {'word': 'ahal', 'score': 0.9381924867630005},\n",
       " {'word': '335', 'score': 0.938149094581604},\n",
       " {'word': 'tb', 'score': 0.9381433725357056},\n",
       " {'word': '827', 'score': 0.9381241798400879},\n",
       " {'word': '225', 'score': 0.9381036162376404},\n",
       " {'word': 'batted', 'score': 0.9380624890327454},\n",
       " {'word': 'era', 'score': 0.9380453824996948},\n",
       " {'word': 'vstra', 'score': 0.9380135536193848},\n",
       " {'word': '716', 'score': 0.9380037188529968},\n",
       " {'word': 'peoria', 'score': 0.9379991292953491},\n",
       " {'word': 'gostaresh', 'score': 0.9379963874816895},\n",
       " {'word': 'saipa', 'score': 0.9379321336746216},\n",
       " {'word': '735', 'score': 0.9379187226295471},\n",
       " {'word': 'echl', 'score': 0.9379072785377502},\n",
       " {'word': 'csa', 'score': 0.9378899335861206},\n",
       " {'word': '130', 'score': 0.9378451108932495},\n",
       " {'word': 'porsche_supercup', 'score': 0.937830924987793},\n",
       " {'word': 'druga_hnl', 'score': 0.937821626663208},\n",
       " {'word': 'concacaf_gold_cup', 'score': 0.9377788305282593},\n",
       " {'word': 'brian_may', 'score': 0.9377428293228149},\n",
       " {'word': '288', 'score': 0.9376749992370605},\n",
       " {'word': 'mccrory', 'score': 0.9376614689826965},\n",
       " {'word': 'hoc1', 'score': 0.9376503229141235},\n",
       " {'word': '1941', 'score': 0.9376412630081177},\n",
       " {'word': '516', 'score': 0.9376345872879028},\n",
       " {'word': '296', 'score': 0.9376336336135864},\n",
       " {'word': 'defensed', 'score': 0.937570333480835},\n",
       " {'word': '2002', 'score': 0.9375636577606201},\n",
       " {'word': 'conference_premier', 'score': 0.9375336766242981},\n",
       " {'word': 'litre', 'score': 0.9375299215316772},\n",
       " {'word': 'blocks', 'score': 0.9375056028366089},\n",
       " {'word': '222', 'score': 0.9374274611473083},\n",
       " {'word': '448', 'score': 0.9374265670776367},\n",
       " {'word': '172', 'score': 0.9374215602874756},\n",
       " {'word': 'vodovac', 'score': 0.9373765587806702},\n",
       " {'word': '508', 'score': 0.9373093247413635},\n",
       " {'word': 'yeong', 'score': 0.9372438788414001},\n",
       " {'word': '2015', 'score': 0.9372003674507141},\n",
       " {'word': '2014', 'score': 0.9371175169944763},\n",
       " {'word': '623', 'score': 0.9369965195655823},\n",
       " {'word': '2016', 'score': 0.9369267225265503},\n",
       " {'word': 'yannick_noah', 'score': 0.9369252920150757},\n",
       " {'word': '485', 'score': 0.9368916153907776},\n",
       " {'word': 'pp', 'score': 0.9368669390678406},\n",
       " {'word': '649', 'score': 0.9368289709091187},\n",
       " {'word': '1942', 'score': 0.9368258118629456},\n",
       " {'word': 'rch12', 'score': 0.9367949962615967},\n",
       " {'word': 'florida_everblades', 'score': 0.936741292476654},\n",
       " {'word': '718', 'score': 0.93673175573349},\n",
       " {'word': '2003', 'score': 0.9367145299911499},\n",
       " {'word': '2005', 'score': 0.9365409016609192},\n",
       " {'word': 'gmt', 'score': 0.9364957809448242},\n",
       " {'word': '237', 'score': 0.9364914894104004},\n",
       " {'word': '277', 'score': 0.9364489316940308},\n",
       " {'word': 'torpedo_nizhny_novgorod', 'score': 0.9364380240440369},\n",
       " {'word': '1990', 'score': 0.9363833665847778},\n",
       " {'word': 'blr', 'score': 0.9363338947296143},\n",
       " {'word': 'belfast_giants', 'score': 0.9363332390785217},\n",
       " {'word': '2012', 'score': 0.9362757802009583},\n",
       " {'word': '240', 'score': 0.9362695813179016},\n",
       " {'word': '286', 'score': 0.9361458420753479},\n",
       " {'word': 'hc_dinamo_minsk', 'score': 0.9361015558242798},\n",
       " {'word': '322', 'score': 0.9360833168029785},\n",
       " {'word': '223', 'score': 0.9360661506652832},\n",
       " {'word': 'idaho_steelheads', 'score': 0.9360204935073853},\n",
       " {'word': 'elena_likhovtseva', 'score': 0.9360098838806152},\n",
       " {'word': '1985', 'score': 0.9359989166259766},\n",
       " {'word': '346', 'score': 0.9359955787658691},\n",
       " {'word': 'tampines_rovers', 'score': 0.9359877705574036},\n",
       " {'word': '442', 'score': 0.9359452724456787},\n",
       " {'word': 'dwars_door_vlaanderen', 'score': 0.935920774936676},\n",
       " {'word': 'jg', 'score': 0.9358955025672913},\n",
       " {'word': '678', 'score': 0.9357891082763672},\n",
       " {'word': 'oldsmobile', 'score': 0.9357644319534302},\n",
       " {'word': 'epl', 'score': 0.9356976747512817},\n",
       " {'word': '50', 'score': 0.9355691075325012},\n",
       " {'word': '455', 'score': 0.9355380535125732},\n",
       " {'word': '673', 'score': 0.9355298280715942},\n",
       " {'word': '037', 'score': 0.9355263710021973},\n",
       " {'word': 'apps', 'score': 0.9354549050331116},\n",
       " {'word': 'brasileiro', 'score': 0.9353655576705933},\n",
       " {'word': 'leiston', 'score': 0.9353500604629517},\n",
       " {'word': 'km', 'score': 0.9353470206260681},\n",
       " {'word': '367', 'score': 0.9353324770927429},\n",
       " {'word': 'lakshmanan', 'score': 0.935314416885376},\n",
       " {'word': 'altima', 'score': 0.9353022575378418},\n",
       " {'word': 'wednesday', 'score': 0.9352828860282898},\n",
       " {'word': 'albrechtsen', 'score': 0.93524169921875},\n",
       " {'word': '486', 'score': 0.9352349638938904},\n",
       " {'word': 'ht', 'score': 0.9351761341094971},\n",
       " {'word': '612', 'score': 0.9351426362991333},\n",
       " {'word': 'apr', 'score': 0.9351222515106201},\n",
       " {'word': 'kg', 'score': 0.9350840449333191},\n",
       " {'word': 'registering', 'score': 0.9350662231445312},\n",
       " {'word': 'lmgte', 'score': 0.9349504709243774},\n",
       " {'word': '411', 'score': 0.9349160194396973},\n",
       " {'word': '1992', 'score': 0.9348893165588379},\n",
       " {'word': 'vantaa', 'score': 0.9348384141921997},\n",
       " {'word': 'formula', 'score': 0.9348341226577759},\n",
       " {'word': 'nottingham_panthers', 'score': 0.9346262216567993},\n",
       " {'word': 'chia', 'score': 0.9346188306808472},\n",
       " {'word': '343', 'score': 0.9345653057098389},\n",
       " {'word': '688', 'score': 0.9345623850822449},\n",
       " {'word': 'yards', 'score': 0.9345337748527527},\n",
       " {'word': 'porsche', 'score': 0.934514582157135},\n",
       " {'word': 'lipetsk', 'score': 0.9344823956489563}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_neighbors(model, corpus.word_to_index, \"january\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your model!\n",
    "\n",
    "Once you have a fully trained model, save it using the code below. Note that we only save the `target_embeddings` from the model, but you could modify the code if you want to save the context vectors--or even try doing fancier things like saving the concatenation of the two or the average of the two!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(model, corpus, filename):\n",
    "    '''\n",
    "    Saves the model to the specified filename as a gensim KeyedVectors in the\n",
    "    text format so you can load it separately.\n",
    "    '''\n",
    "\n",
    "    # Creates an empty KeyedVectors with our embedding size\n",
    "#     kv = KeyedVectors(vector_size=model.embedding_size) \n",
    "    kv=KeyedVectors(vector_size=50)\n",
    "    vectors = []\n",
    "    words = []\n",
    "    # Get the list of words/vectors in a consistent order\n",
    "    for index in trange(model.target_embeddings.num_embeddings):\n",
    "        word = corpus.index_to_word[index]\n",
    "        vectors.append(model.target_embeddings(torch.LongTensor([index])).detach().numpy()[0])\n",
    "        words.append(word)\n",
    "\n",
    "    # Fills the KV object with our data in the right order\n",
    "    kv.add_vectors(words, vectors) \n",
    "    kv.save_word2vec_format(filename, binary=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.target_embeddings.num_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7160d2eca6ea40689cea8d62384bc619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/110331 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "save(model,corpus,'word2vec_batch32_debias_med_2.kv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL PART: DO THIS LAST AND READ CAREFULLY\n",
    "\n",
    "Before you start this part, you need to have a fully working solution and completed the exploratory part of the assignment.\n",
    "\n",
    "**Once you are ready, create a copy of your working notebook and call it `Debiased Word2Vec.ipynb`. Do not do this part in your working code for the assignment!!!**\n",
    "\n",
    "## Seriously, save your code in a new file and then start reading the rest of these instructions there.\n",
    "\n",
    "Ok, hopefully you're reading these in a new file... For this last part of the assignment, we're going to _change_ how word2vec learns at a fundamental level. \n",
    "\n",
    "As you might have noticed in your exploratory analysis, the word2vec model learns to weird and sometimes biased associations between words. In particular, your word2vec model has likely learned some unfortunate gender biases, e.g., that the vector for \"nurse\" is closer to \"woman\" than \"man\". The algorithm itself isn't to blame since it is learning these from a corpus (here, Wikipedia biographies) that contain biases already based on how people write. Wikipedia [is](http://markusstrohmaier.info/documents/2015_icwsm2015_wikipedia_gender.pdf) [well](http://dcs.gla.ac.uk/~mounia/Papers/wiki_gender_bias.pdf) [known](https://www.academia.edu/download/64856696/genderanalysisofWikipediabiostext_self_archived.pdf) for having gender biases in how it writes about men and women.\n",
    "\n",
    "\n",
    "**Side note**: Some times this bias-learning behavior is useful: We can use word2vec to uncover these biases and analyze their trends, like this PNAS paper did for [looking at bias in news writing along multiple dimensions of identity](https://www.pnas.org/content/pnas/115/16/E3635.full.pdf)\n",
    "\n",
    "In this last part of the homework, we'll ask how we might try to _prevent_ these biases by modifying the training. You won't need to solve this problem by any means, but the act of trying to reduce the biases will open up a whole new toolbox for how you (the experimenter/practioner) can change how and what models learn.\n",
    "\n",
    "There are many potential ways to _debias_ word embeddings so that their representations are not skewed along one \"latent dimension\" like gender. In this homework, you'll be trying one of a few different ideas for how to do it. **You are not expected to solve gender bias! This part of the assignment is to have to start grappling with a hard challenge but there is no penalty for doing less-well!** \n",
    "\n",
    "One common technique to have models avoid learning bias is similar to another one you already&mdash;**regularization**. In Logistic Regression, we could use L2 regularization to have our model avoid learning $\\beta$ weights that are overfit to specific or low-frequency features by adding a regularizer penalty where the larger the weight, the more penalty the model paid. Recall that this forces the model to only pick the most useful (generalizable) weights, since it has to pay a penalty for any non-zero weight. \n",
    "\n",
    "In word2vec, we can adapt the idea to think about whether our model's embeddings are closer or farther to different gender dimensions. For example, if we consider the embedding for \"president\", ideally, we'd want it to be equally similar to the embeddings for \"man\" and \"woman\". One idea then is to penalize the model based on how uneven the similarity is. We can do this by directly modifying the loss:\n",
    "```\n",
    "loss = loss_criteron(preds, actual_vals) + some_bias_measuring_function(model)\n",
    "```\n",
    "Here, the `some_bias_measuring_function` function takes in your model as input and returns how much bias you found. Continuing our example, we might implement it in pseudocode as\n",
    "```\n",
    "def some_bias_measuring_function(model):\n",
    "    pres_woman_sim = cosine_similarity(model, \"president\", \"woman\")\n",
    "    pres_man_sim = cosine_similarity(model, \"president\", \"man\")\n",
    "    return abs(pres_woman_sim - pres_man_sim)\n",
    "```\n",
    "This simple example would penalize the model for learning a representation of \"president\" that is more simular to one of the two words. Of course, this example is overly simple. Why just \"president\"? Why just \"man\" and \"woman\"? Why not other words or other gender-related words or other gender identities?? \n",
    "\n",
    "Another idea might be to just make the vectors for \"man\" and \"woman\" be as similar as possible:\n",
    "```\n",
    "def some_bias_measuring_function(model):\n",
    "    # cosine similarity is in [-1,1] but we mostly expect it in [0,1]\n",
    "    man_woman_sim = cosine_similarity(model, \"man\", \"woman\")\n",
    "    # penalize vectors that are not maximally similar, and avoid the edge case \n",
    "    # of negative cosine similarity\n",
    "    return 1 - max(man_woman_sim, 0)\n",
    "```\n",
    "\n",
    "All of this works in practice because PyTorch is fantastic about tracking the gradient with respect to the loss. This ability lets us easily define a loss function so that our word2vec model (1) learns to predict the right context words while (2) avoids learning biases. If we compare this code to the numpy part of Homework 1, it's easy to see how powerful PyTorch can be as an experimenter for helping you control what and how your models learn!\n",
    "\n",
    "Your task is to expand this general approach by coming up with an extension to word2vec that adds some new term to the `loss` value that penalizes bias in the gender dimension. There is no right way to do this and even some right-looking approaches may not work&mdash;or might word but simultaneously destroy the information in the word vectors (all-zero vectors are unbiased but also uninformative!). \n",
    "\n",
    "**Suggestion:** You may need to weight your bias term in the loss function (remember that $\\lambda_1 x_1 + \\lambda_2 x_2$ interpolation? This is sort of similar) so that your debiasing regularizer doesn't overly penalize your model.\n",
    "\n",
    "Once you have generated your model, record word vector similarities for the pairs listed on canvas in `word-pair-similarity-predictions.csv` where your file writes a result like\n",
    "```\n",
    "word1,word2,sim\n",
    "dog,puppy,0.91234123\n",
    "woman,preseident,0.81234\n",
    "```\n",
    "You'll record the similarity for each pair of words in the file and upload it to CodaLab, which is kind of like Kaggle but lets use a custom scoring program. We'll evaluate your embeddings based on how unbiased they are and how much information they still capture after debiasing. **Your grade does not depend on how well you do in CodaLab, just that you tried something and submitted.** However, the CodaLab leaderboard will hopefully provide a fun and insightful way of comparing just how much bias we can remove from our embeddings.\n",
    "\n",
    "The CodaLab link will be posted to Piazza"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
