{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SI630 Homework 2: Word2vec Vector Analysis\n",
    "\n",
    "*Important Note:* Start this notebook only after you've gotten your word2vec model up and running!\n",
    "\n",
    "Many NLP packages support working with word embeddings. In this notebook you can work through the various problems assigned in Task 3. We've provided the basic functionality for loading word vectors using [Gensim](https://radimrehurek.com/gensim/models/keyedvectors.html), a good library for learning and using word vectors, and for working with the vectors. \n",
    "\n",
    "One of the fun parts of word vectors is getting a sense of what they learned. Feel free to explore the vectors here! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = KeyedVectors.load_word2vec_format('model/word2vec_batch32_debias_med_2.kv', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.1471436 , -0.28772095, -2.2766628 ,  0.05391294, -1.4119608 ,\n",
       "       -0.29097795, -1.6112773 , -1.3050717 ,  1.815922  , -1.1087487 ,\n",
       "        0.9353696 , -0.8766251 , -0.82061183,  1.6959955 , -0.10369266,\n",
       "       -0.20159033, -1.0378739 ,  0.7054374 , -1.3304659 , -0.37106448,\n",
       "       -2.2896414 , -0.04715284, -0.5156441 ,  0.95758235, -0.01403815,\n",
       "        0.97477394, -1.4969469 , -3.3509867 , -0.42322063, -0.8213046 ,\n",
       "       -0.9351953 ,  1.2764132 , -0.34828973, -0.0136232 ,  0.47956136,\n",
       "        0.9739854 ,  1.2430013 , -0.52861917,  1.42831   , -0.8615355 ,\n",
       "        2.0485601 , -0.00539556, -0.7826375 ,  0.20810084,  0.01271399,\n",
       "        1.938198  ,  1.3718727 ,  0.14091182, -2.4371796 ,  0.89249504],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('articles', 0.9980189204216003),\n",
       " ('paintings', 0.9980115294456482),\n",
       " ('words', 0.9962870478630066),\n",
       " ('novels', 0.9962369799613953),\n",
       " ('portraits', 0.9962106943130493),\n",
       " ('material', 0.9956844449043274),\n",
       " ('drawings', 0.9956687688827515),\n",
       " ('told', 0.9954758882522583),\n",
       " ('characters', 0.9951522946357727),\n",
       " ('names', 0.995005190372467)]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_word(\"books\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crib ('oxford_brookes_university', 0.9986768960952759)\n",
      "gin ('bulger', 0.9968262314796448)\n",
      "stupid ('remark', 0.9966140985488892)\n",
      "motocross ('super_bowl', 0.9963929057121277)\n",
      "england ('australia', 0.9930800795555115)\n",
      "victory ('loss', 0.9942400455474854)\n",
      "wonderful ('enormous', 0.9994308948516846)\n",
      "teacher ('lab', 0.9973334074020386)\n",
      "april ('march', 0.9998651146888733)\n",
      "physics ('economics', 0.9949340224266052)\n"
     ]
    }
   ],
   "source": [
    "words=['crib','gin','stupid','motocross','england','victory','wonderful','teacher','april','physics']\n",
    "\n",
    "for word in words:\n",
    "    print(word,word_vectors.similar_by_word(word)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> I picked 10 words of different frequencies. From the result, it could be seen that the result of prediction is not very well. For words like country name, months or subjects, the result is similar in category to the original word/ However, for words with less frequency, the result of prediction is worse </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_analogy(a, b, c):\n",
    "    return word_vectors.most_similar(positive=[b, c], negative=[a])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "italian\n",
      "arts\n",
      "staff\n",
      "arts\n",
      "biographer\n"
     ]
    }
   ],
   "source": [
    "print(get_analogy('sushi','japanese','pizza'))\n",
    "print(get_analogy('math','physics','research'))\n",
    "print(get_analogy('teacher','student','superior'))\n",
    "print(get_analogy('literature', 'art', 'physics'))\n",
    "print(get_analogy('man', 'woman', 'physician'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>The equations I got are:<br>\n",
    "japanese-sushi+piazza=italian<br>\n",
    "physics-math+research=arts <br>\n",
    "student-teacher+superior=staff<br>\n",
    "woman-man+physician=biographer<br>\n",
    "art-literature+physics=arts<br>\n",
    "<br>\n",
    "I found that word analogies on words with the same part of speech are more likely to work, and analogies across different part of speech words are less effective. Since words of subjects, food, country and jobs have the best prediction results, the analogies I made are from these categories.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('word_pair_similarity_predictions.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liuzihui/miniconda3/envs/python37/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for idx,row in df.iterrows():\n",
    "    word1=row[0]\n",
    "    word2=row[1]\n",
    "#     print(word1,word2)\n",
    "    similarity=word_vectors.similarity(word1,word2)\n",
    "    df['sim'][idx]=similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>old</td>\n",
       "      <td>new</td>\n",
       "      <td>0.475980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>smart</td>\n",
       "      <td>intelligent</td>\n",
       "      <td>0.970990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hard</td>\n",
       "      <td>difficult</td>\n",
       "      <td>0.850950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>happy</td>\n",
       "      <td>cheerful</td>\n",
       "      <td>0.941494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hard</td>\n",
       "      <td>easy</td>\n",
       "      <td>0.884791</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word1        word2       sim\n",
       "0    old          new  0.475980\n",
       "1  smart  intelligent  0.970990\n",
       "2   hard    difficult  0.850950\n",
       "3  happy     cheerful  0.941494\n",
       "4   hard         easy  0.884791"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('word_pair_similarity_predictions.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
